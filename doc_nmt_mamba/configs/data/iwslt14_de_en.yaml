# IWSLT 2014 German-English configuration

name: iwslt14_de_en
dataset_name: iwslt14

# Tokenizer settings
# CRITICAL: Use "custom" for thesis work (32K vocab = proper parameter allocation)
# mBART has 250K vocab which makes model 95% embedding table - NOT recommended
tokenizer_type: custom  # "custom" (32K, RECOMMENDED) or "mbart" (250K, NOT recommended)
tokenizer_path: data/tokenizer/tokenizer.json

# Language settings (mBART format - used only if tokenizer_type=mbart)
src_lang: de_DE
tgt_lang: en_XX

# Sequence lengths
max_src_length: 4096
max_tgt_length: 4096

# CAT-N augmentation
cat_n: 5  # CAT-5 strategy
p_concat: 0.5  # 50% single, 50% concatenated

# Collation mode: "packed" (recommended) or "padded"
# Packed mode eliminates padding waste and is 20-30% faster on H100
collator_mode: packed

# Data loading - Optimized for 52-core Xeon with 2x H100
# More workers = faster data loading, but diminishing returns past ~24
num_workers: 16  # Increased from 8 (52 CPU cores available)
prefetch_factor: 4  # Prefetch 4 batches per worker
persistent_workers: true  # Keep workers alive between epochs
pin_memory: true  # Pin memory for faster GPU transfer
drop_last: false  # Keep all samples
