Proceedings of Machine Learning Research 288:1–13, 2025 2nd International Conference on Neuro-symbolic Systems (NeuS)

## **Expansion Span: Combining Fading Memory and Retrieval in Hybrid** **State Space Models**


**Elvis Nunez** ELVISNUN@AMAZON.COM
**Luca Zancato** ZANCATO@AMAZON.COM
**Benjamin Bowman** BOWMABEN@AMAZON.COM
**Aditya Golatkar** AGOLATKA@AMAZON.COM
**Wei Xia** WXIA@AMAZON.COM
**Stefano Soatto** SOATTOS@AMAZON.COM

_AWS AI Labs_


**Editors:** G. Pappas, P. Ravikumar, S. A. Seshia


**Keywords:** Hybrid State Space Models, SSMs, Attention, Sparse Attention, LoRA.
**Abstract**


The “state” of State Space Models (SSMs) represents their memory, which fades exponentially
over an unbounded span. By contrast, Attention-based models have “eidetic” (i.e., verbatim, or
photographic) memory over a finite span (context size). Hybrid architectures combine State Space
layers with Attention, but still cannot recall the distant past and can access only the most recent
tokens eidetically. Unlike current methods of combining SSM and Attention layers, we allow the
state to be allocated based on relevancy rather than recency. In this way, for every new set of query
tokens, our models can “eidetically” access tokens from beyond the Attention span of current Hybrid
SSMs without requiring extra hardware resources. We introduce a method to expand the memory
span of the hybrid state by “reserving” a fraction of the Attention context for tokens retrieved from
arbitrarily distant in the past, thus expanding the eidetic memory span of the overall state. We call
this reserved fraction of tokens the “expansion span,” and the mechanism to retrieve and aggregate
it “Span-Expanded Attention” (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose
a novel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows efficient
adaptation on long spans of tokens. We show that SE-Attn enables us to efficiently adapt pre-trained
Hybrid models on sequences of tokens up to 8 times longer than the ones used for pre-training. We
show that HyLoRA with SE-Attn is cheaper and more performant than alternatives like LongLoRA
when applied to Hybrid models on natural language benchmarks with long-range dependencies,
such as PG-19, RULER, and other common natural language downstream tasks.


**1. Introduction**


State Space Models are able to process sequences with an unbounded number of tokens by maintaining a fixed-size state. However, this state is lossy and information about early tokens “fades” as more
inputs are processed. In contrast, Transformer models have a state determined by the number of
tokens in their input sequence and are able to access information from all past tokens in their context
“eidetically.” However, they do so at the cost of extra compute and memory.
Recent Hybrid models Zancato et al. (2024); Glorioso et al. (2024b); De et al. (2024) augment
SSMs with Attention layers in an effort to counteract SSMs’ “fading” memory. However, Attention
layers only aggregate information by recency (i.e., they process the keys and values of the most
recent tokens up to hardware limitations). Hence, any token that lies beyond the Attention’s limited
span can only be approximately recalled through the SSMs’ state. This limits the effectiveness of


© 2025 E. Nunez, L. Zancato, B. Bowman, A. Golatkar, W. Xia & S. Soatto.


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


Hybrid SSMs when applied to long sequences of tokens, especially when compute and memory
resources are limited. In fact, while recent pre-trained Hybrid models have demonstrated strong
performance on common downstream and recall-intensive language tasks, they are trained following
Transformers’ pre-training practice: they are either trained on a fixed, relatively short window of
tokens when compute is limited—which is often set to 2048 as in LLaMA1 Touvron et al. (2023a),
or 4096 as in LLaMA2 Touvron et al. (2023b)—or are progressively fine-tuned on longer sequences
after the pre-training stage Lieber et al. (2024); Glorioso et al. (2024b); Dubey et al. (2024).


In this work, we modify Hybrid SSMs’ Attention layers to allow their state to be allocated by
_relevancy_ rather than _recency_, allowing our models to retrieve information that would otherwise
fall outside their Attention span. To do so, we propose Span-Expanded Attention (SE-Attn), a
selection mechanism that enables eidetic retrieval of tokens from the past. While SE-Attn can be
applied to both Hybrid SSMs and Transformers, we focus on hybrid architectures where this retrieval
capability naturally complements the fading memory of SSMs: while SSMs efficiently process long
sequences with their state-based memory, SE-Attn selectively preserves important information that
would otherwise fade in the SSM state.


Our method allows long context fine-tuning of any pre-trained Hybrid model, such as Mamba2-Hybrid Dao and Gu (2024), Jamba Lieber et al. (2024), and Zamba Glorioso et al. (2024a), on
sequences longer than the one used for pre-training without significant compute/memory overhead.
While Transformer-based fine-tuning strategies for processing long sequences can also be applied
to Hybrid models, they typically assume that information in the Attention layers is aggregated by
recency and are thus more expensive as sequences get longer. For example, extending a LLaMA
model from a 2k to 8k context size can require up to 32 A100 GPUs Chen et al. (2023). While there
exist efficient LoRA-based methods Hu et al. (2022) that lower this cost, we show that Transformerbased methods tend to not work as well for Hybrid models as they do for Transformers. We show
that this is mostly due to the fact that combining LoRA (on Attention layers) with SSM layers is not
expressive enough to model long-range dependencies beyond the pre-training context size.


To overcome such limitations, we propose to modify the standard LoRA recipe to fit the Hybrid
model structure better. Namely, we employ a fine-tuning strategy similar to LoRA+ Chen et al.
(2024) on Attention layers, while allowing the SSMs’ recurrent parameters to be adapted as well. In
particular, building upon previous observations Zancato et al. (2024); Yang et al. (2024b), we also
adapt the 1D convolutional layers; we empirically demonstrate that this adaptation yields the best
results at a reduced cost.


We make the following contributions: (1) We propose SE-Attn, a novel selection mechanism that
“reserves” a fraction of the context of Hybrid SSMs’ Attention layers for tokens that are retrieved from
the past based on their relevance to the current query. (2) We empirically show that Transformer-based
methods for long-context fine-tuning are sub-optimal when applied to Hybrid SSMs. Therefore, we
propose HyLoRA, an extension of LoRA+ Chen et al. (2024) for efficient and effective fine-tuning
of Hybrid SSMs. (3) We show that combining SE-Attn and HyLoRA on Hybrid models reliably
extends their context size up to 8 _×_ their pre-training size. Further, we show improved performance
over existing adaptation methods on common natural language tasks in the LM Harness Evaluation
suite Gao et al. (2024) and long context tasks in RULER Hsieh et al. (2024).


2


SPAN-EXPANDED ATTENTION


**2. Background and Related Work**


**Attention on long contexts.** Since the introduction of Self-Attention Vaswani et al. (2017), a
significant amount of research has been made to reduce its quadratic cost in the sequence length at
training time. To achieve this, state of the art methods usually approximate the Attention mechanism
with sparse/linearized versions. For example, Reformer Kitaev et al. (2020) uses locality-sensitivehashing to group tokens with similar embeddings, allowing the model to only attend to a subset
of tokens rather than the entire sequence. Other works have proposed to endow Transformers
models with “compressed” memory tokens that are updated dynamically and causally over sliding
windows on entire sequence chunks. For example, Transformer-XL Dai (2019) and Infini-Attention
Munkhdalai et al. (2024) segment an input sequence into chunks and process them sequentially while
maintaining a complementary set of tokens whose purpose is to summarize the older ones. In contrast
to these works, our SE-Attn retrieves relevant tokens “eidetically,” i.e., we retrieve tokens rather
than maintain a compressed representation of the past. Most similar to our method is Landmark
Attention Mohtashami and Jaggi (2023), which inserts landmark tokens into the input at fixed block
intervals and trains these tokens to act as summaries of their corresponding blocks via a grouped
softmax attention; these summary tokens are then used to index and retrieve relevant input tokens
when processing future segments. Our SE-Attn aims to integrate retrieval natively, without the need
for external landmark tokens or complex Softmax procedures.

**State Space Models.** While a great effort has been made to improve the efficiency of Transformer
models, a recent line of work has explored efficient alternative “linear” architectures. In particular,
State Space Models (SSMs) Gu and Dao (2023); Gu et al. (2021, 2022); Yang et al. (2024a); Sun et al.
(2023) have emerged as promising competitors to Transformer models due to their efficient scaling
and strong empirical performance. Numerous variations of SSMs have been proposed, some closely
resembling Linear Attention Sun et al. (2023) or Linear-Time Invariant dynamical systems Gu et al.
(2021), while others introduce novel adaptive/gated state updates Yang et al. (2024a); Gu and Dao
(2023); Dao and Gu (2024); Orvieto et al. (2023). Despite their differences, all follow the same basic
working principle that is inspired by classical state space models Kalman (1960): they process the
input sequence by maintaining a _fixed-size_ state which acts as a compressed (lossy) representation of
all the processed tokens. When implemented in hardware, the state must have finite precision and
therefore “fades” as more samples are processed. To overcome this limitation, the most successful
SSMs are typically hardware-aware implementations that efficiently utilize modern GPUs/TPUs.
These implementations use highly parallelizable and scalable primitives, such as associative scans
Gu and Dao (2023); De et al. (2024), chunking mechanisms Dao and Gu (2024); Yang et al. (2024a),
and techniques that avoid the materialization of the entire state in slow high bandwidth memory Gu
and Dao (2023).

**Hybrid State Space Models.** While extending the recurrent state in SSMs layers has led to
performant models, they typically fall short on tasks that require recalling information from the
distant past Waleffe et al. (2024); Jelassi et al. (2024). Hybrid State Space Models have been
introduced to solve this limitation and are typically designed to complement the SSMs’ “fading”
state with Attention layers Dao and Gu (2024); De et al. (2024); Lieber et al. (2024); Glorioso
et al. (2024b). While early architectures simply stack SSMs and Attention layers with different
blending ratios Waleffe et al. (2024); Gu and Dao (2023); Dao and Gu (2024) or replace full Attention
layers with Sliding Window Attention De et al. (2024), more performant and sophisticated hybrid
architectures have been recently proposed Glorioso et al. (2024b); Zancato et al. (2024). In particular,


3


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO



**Split into**



**(2)**
**Split into**









|Col1|11|12|Input<br>tokens|
|---|---|---|---|
|**1**<br>**2**<br>**3**<br>**4**<br>**9**<br>**10**<br>**11**<br>**12**<br>**3**<br>**4**<br>**5**<br>**6**<br>**7**<br>**8**<br>**1**<br>**2**<br><br>** ks**<br>**(3)**<br>**Retrieve**<br>**mory blocks**<br>**Retrieved**<br>**blocks**<br>**Block**<br>**candidates**<br>**1**<br>**2**<br>**3**<br>**4**<br>**5**<br>**6**<br>**7**<br>**8**<br>**9**<br>**10**<br>**11**<br>**12**|**1**<br>**2**<br>**3**<br>**4**<br>**9**<br>**10**<br>**11**<br>**12**<br>**3**<br>**4**<br>**5**<br>**6**<br>**7**<br>**8**<br>**1**<br>**2**<br><br>** ks**<br>**(3)**<br>**Retrieve**<br>**mory blocks**<br>**Retrieved**<br>**blocks**<br>**Block**<br>**candidates**<br>**1**<br>**2**<br>**3**<br>**4**<br>**5**<br>**6**<br>**7**<br>**8**<br>**9**<br>**10**<br>**11**<br>**12**|**ieved**<br>**cks**<br>**ock**<br><br>**11**<br>**12**|**ieved**<br>**cks**<br>**ock**<br><br>**11**<br>**12**|


Figure 1: **Span-Expanded Attention (SE-Attn) overview.** SE-Attn is a Sparse Attention mechanism
used to expand the memory span of Hybrid SSMs. _Left_ : SE-Attn works by reserving a fraction of
the Attention context for tokens retrieved arbitrarily far back in the past. We call this reserve the
“expansion span,” and we populate it with blocks of previous tokens (“memory blocks”). When new
tokens arrive, a similarity-based search compares the queries with past memory blocks—represented
as summary tokens—to retrieve relevant memory blocks. Then, these retrieved memory blocks
are jointly processed with the queries via Attention. While the final Attention mechanism always
processes a fixed number of tokens, it can have a longer span since it retrieves tokens from arbitrarily
far back in the past. _Right_ : Retrieving tokens from the past yields a sparse Attention pattern.


B’MOJO Zancato et al. (2024) complements SSMs’ fading state with a form of “eidetic” memory
whereby SSM layers are mixed with Sliding Window Attention and tokens in the window can attend
to a set of tokens from the past that are deemed difficult to predict using an asynchronous causal
selection mechanism. Similarly, in our work, we mix SSM layers with an Attention layer that
operates over chunks of tokens, but we enable these chunks to dynamically retrieve tokens from the
past that are most relevant to the chunk’s tokens, rather than tokens selected a priori as in B’MOJO.


**3. Span-Expanded Attention**


Our goal is to enable pre-trained Hybrid SSMs to accurately process sequences longer than the ones
seen during pre-training. For a sequence of _L_ tokens with model dimension _d_ model, the computational
complexity of standard Self-Attention is _O_ ( _d_ model _L_ [2] ). For very large _L_, computing Self-Attention
can be prohibitively expensive. Training models with a large _L_ is particularly challenging, as training
has the additional cost of computing gradients, further limiting memory resources. To address this,
we propose Span-Expanded Attention (SE-Attn), a drop-in replacement for standard Self-Attention
in Hybrid SSMs. To train SE-Attn, we propose HyLoRA, a variant of LoRA+ Chen et al. (2024)
specifically tailored to Hybrid models. A schematic of our SE-Attn is provided in Figure 1.


**3.1. Attention**


At the heart of modern LLMs is the Attention mechanism, whose role is to construct a contextualized
representation of its input. The input to Attention, _x ∈_ R _[L][×][d]_, is a tensor of _L_ tokens, each with
dimension _d_ . Attention is parameterized by _WQ, WK, WV ∈_ R _[d][×][d]_ [model] and _Wo ∈_ R _[d]_ [model] _[×][d]_, which
are used to construct linear projections of the input: _Q_ = _xWQ_, _K_ = _xWK_, _V_ = _xWV ∈_ R _[L][×][d]_ [model] .
After adding positional information to the keys and queries, the output of the Attention layer is [1]




          -           - _QK_ _[T]_
Attention( _Q, K, V_ ) = softmax ~~_√_~~
_d_ model




- _V_ _Wo ∈_ R _[L][×][d]_ . The realization of the attention score



1. For simplicity, we consider Single-Head Attention in this exposition.


4


SPAN-EXPANDED ATTENTION


matrix, _QK_ _[T]_ _∈_ R _[L][×][L]_, grows quadratically in the sequence length, _L_ . Moreover, the output of
Attention is typically followed by a feed-forward layer (FFN), which expands and contracts the
dimension of the input. While FFNs are generally regarded as being the primary computational
bottlenecks of Attention-based models for short contexts, when the sequence length exceeds the
expanded dimension, Attention becomes the primary bottleneck. Since LLMs consist of many layers
of Attention, this computation is particularly expensive during training, where activations are cached.
To make the Attention mechanism computationally amenable to longer contexts during training, we
draw inspiration from RAG and Sparse Attention methods.


**3.2. Amnesic Attention**


The crux of our method is in the chunking of the _L_ tokens in the input sequence _x_, into chunks
of size _M_ . Namely, we begin by computing projections _Q_ = _xWQ_, _K_ = _xWK_, _V_ = _xWV_ and
adding positional information as in standard Attention. Next, each of the _Q, K, V ∈_ R _[L][×][d]_ [model]
projections are split into _T_ = _M_ _[L]_ [chunks of] _[ M]_ [ tokens in each along the sequence dimension, yielding]

_Qi, Ki, Vi ∈_ R _[M]_ _[×][d]_ [model] for _i_ = 1 _, . . ., T_ . A naive way to reduce the quadratic cost of Attention is to
apply it independently on each of these chunks, _Ai_ = Attention( _Qi, Ki, Vi_ ) and then concatenate
and project them to get the final output, Concatenate( _A_ 1 _, A_ 2 _, . . ., AT_ ). However, since the chunks
are processed independently, there is no information exchanged between them. Hence, this naive—
though efficient—Attention mechanism, which we refer to as “SE-Attn-NoMem”, cannot model time
dependencies on contexts larger than the chunk size.


**3.3. Eidetic Retrieval Attention**



In this section, we improve upon SE-Attn-NoMem by allowing different chunks to exchange information while minimizing compute; we call this Attention mechanism SE-Attn. To this end, we
augment the processing of each chunk with a mechanism to retrieve tokens from previous chunks.
In particular, we allow chunk _i_ to retrieve tokens from chunks 1 _,_ 2 _, . . ., i −_ 1 and, when the most
relevant chunks are selected, append their tokens to its context (its “expansion span”). SE-Attn
takes as input a sequence, _x ∈_ R _[L][×][d]_ and computes projections _Q_ = _xWQ_, _K_ = _xWK_, _V_ = _xWV_
followed by the addition of RoPE Su et al. (2024) embeddings as in standard Attention. As described in Section 3.2, _Q, K, V_ are split into _T_ chunks with _M_ tokens in each, yielding tuples
( _Qi, Ki, Vi_ ), _i_ = 1 _, . . ., T_ . Additionally, _Q, K, V_ are split into a second set of _U_ chunks with _S_
tokens in each, yielding tuples ( _Q_ [Mem] _j_ _, Kj_ [Mem] _, Vj_ [Mem] ) where _Q_ [Mem] _j_ _, Kj_ [Mem] _, Vj_ [Mem] _∈_ R _[S][×][d]_ [model] for
_j_ = 1 _, . . ., U_ . We refer to these tuples as “memory blocks”; in SE-Attn, the query from each chunk,
_Qi_, attends not only to _Ki_, but also to a set of retrieved memory blocks which populate SE-Attn’s
expansion span. In particular, each _Qi_ retrieves _k_ (top- _k_ ) key/value memory blocks from the past [2] :

- - - _Kϕ_ [Mem] _i_ ( _U_ )1 _[, V]_ _ϕ_ [ Mem] _i_ ( _U_ )1 _, . . .,_ _Kϕ_ [Mem] _i_ ( _U_ ) _k_ _[, V]_ _ϕ_ [ Mem] _i_ ( _U_ ) _k_ where _ϕi_ ( _U_ ) _j_ denotes the index of the _j_ -th memory




- - - _Kϕ_ [Mem] _i_ ( _U_ )1 _[, V]_ _ϕ_ [ Mem] _i_ ( _U_ )1 _, . . .,_ _Kϕ_ [Mem] _i_ ( _U_ ) _k_ _[, V]_ _ϕ_ [ Mem] _i_ ( _U_ ) _k_ where _ϕi_ ( _U_ ) _j_ denotes the index of the _j_ -th memory

block selected by the _i_ -th chunk. Retrieved blocks are appended to the chunks’ keys and values, and
SE-Attn computes the Attention output for the _i_ th chunk as in Equation (3).




- _, . . .,_ _K_ [Mem]
_ϕi_ ( _U_ ) _k_ _[, V]_ _ϕ_ [ Mem] _i_ ( _U_ ) _k_



_K_ ˜ = Concatenate( _Kϕ_ [Mem] _i_ ( _U_ )1 _[, . . ., K]_ _ϕ_ [Mem] _i_ ( _U_ ) _k_ _[, K][i]_ [)] (1)

_V_ ˜ = Concatenate( _Vϕ_ [Mem] _i_ ( _U_ )1 _[, . . ., V]_ _ϕ_ [ Mem] _i_ ( _U_ ) _k_ _[, V][i]_ [)] (2)

_A_ [SE-Attn] _i_ = Attention( _Qi,_ _K_ [˜] _i,_ _V_ [˜] _i_ ) (3)


2. By “the past,” we mean tokens in the sequence that came before tokens in the chunk.


5


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


Afterwards, each chunk’s Attention outputs are concatenated and projected:


_o_ [SE-Attn] = Concatenate( _A_ [SE-Attn] 1 _, A_ [SE-Attn] 2 _, . . ., A_ [SE-Attn] _T_ ) _._


Note that Equation (3) is a form of Cross-Attention since we are not concatenating memory query
tokens to the chunk’s query. This is to preserve causality, as the retrieved tokens cannot attend to the
chunk’s tokens.
**Memory Retrieval.** Each chunk _xi_ must judiciously select which memory blocks to retrieve from
the past. To do this efficiently, we associate a 1-dimensional tensor, _cj ∈_ R _[d]_ [model], to each of the
_j_ = 1 _, . . ., U_ memory blocks which act as a compressed representation of each memory block. To
determine which memory blocks _Qi_ should attend to, we compute a “relevancy score” between
_Qi_ and each _cj_, which measures how relevant memory block _j_ is to chunk _i_ . This relevancy
score is implemented with a Cross-Attention score between chunks and compressed memory block
representations. Recalling that _Qi ∈_ R _[M]_ _[×][d]_ [model], we compute relevancy score _Rij ∈_ R between
chunk _i ∈{_ 1 _,_ 2 _, . . ., T_ _}_ and memory block _j ∈{_ 1 _,_ 2 _, . . ., U_ _}_ as follows: _Rij_ = [�] _t_ _[M]_ =1 [(] _[Q][i][c][j]_ [)] _[t][.]_
_Ri ∈_ R _[U]_ represents the relevancy score between chunk _i_ and all memory blocks. However, since
chunk _i_ should only retrieve memory blocks that came before it temporally, we add a mask to _Ri_ and
then apply Softmax to obtain the final scores, _R_ [˜] _i_, between chunk _i_ and all memory blocks as follows:


            - 1            _R_ ˜ _i_ = softmax ~~_√_~~ ( _Ri_ + _Mi_ ) _∈_ R _[U]_ (4)
_d_ model


where _Mi_ is a mask of 0 and _−∞_ constructed to set the scores of future memory blocks (relative
to chunk _i_ ) to _−∞_ . Once all relevancy scores are computed, chunk _i_ simply retrieves the top _k_
memory blocks with the highest _R_ [˜] _ij_ scores and concatenates them with the keys and values as in
Equations (1) and (2) before computing Attention as in Equation (3).
**Compressed Memory Blocks.** Next, we discuss how we construct the compressed memory block
representations, _cj_ . Recently, Landmark Attention Mohtashami and Jaggi (2023) considered using
“landmark” tokens to obtain compressed representations of memory blocks. We consider using
landmark tokens to construct _cj_ in Appendix F. In SE-Attn, we consider a simpler approach which
we found to work well. For each memory block, ( _Q_ [Mem] _j_ _, Kj_ [Mem] _, Vj_ [Mem] ), we perform standard non



          causal Attention, _A_ [Mem] _j_ = softmax _Q_ [Mem] _j_ ~~_√_~~ ( _dK_ model _j_ [Mem] ) _[T]_




_Vj_ [Mem] _∈_ R _[S][×][d]_ [model] ; we consider non-causal



Attention as we are interested in a global representation where all tokens within the memory block
can attend to each other. We compute the mean of these weighted memory tokens as the compressed
representation: _cj_ = _S_ [1] - _St_ =1 [(] _[A]_ _j_ [Mem] ) _t ∈_ R _[d]_ [model], where ( _A_ [Mem] _j_ ) _t_ denotes the _t_ -th row of _A_ [Mem] _j_ .


**3.4. Training SE-Attn with LoRA**


In this paper, we fine-tune models pre-trained with standard Attention; however, we fine-tune
them with SE-Attn, which modifies the pre-trained Attention mechanism by introducing a retrieval
mechanism. SE-Attn repurposes the model’s Attention parameters ( _WQ, WK, WV, Wo_ ) to perform
retrieval. In order to efficiently train the model to learn to use SE-Attn, we use a variant of LoRA
Hu et al. (2022). Recently, Chen et al. (2024) introduced LoRA+ designed to fine-tune Transformer
models on long contexts. LoRA fine-tunes Attention parameters with low rank adapter matrices.
LoRA+ differs from LoRA by also training embedding and normalization layers.


6


SPAN-EXPANDED ATTENTION


Recently, Galim et al. (2024) found that pure SSMs can be fine-tuned by training the SSM
projection layers with LoRA. Since we fine-tune on long contexts, we prioritize efficient training,
and consequently apply LoRA only to the Attention layers of our hybrid models. However, it is
common for SSM layers to include a 1D convolution layer after their initial projection in order to
perform sequence mixing Zancato et al. (2024); Glorioso et al. (2024b); Lieber et al. (2024); Dao
and Gu (2024); De et al. (2024). The 1D convolution parameters constitute a very small portion of
the model parameters ( _∼_ 0.7% for Mamba-2-Hybrid 2.7B), but as discussed further in Section 4.3,
we found that training these 1D convolution layers in conjunction with LoRA+ improved our models’
performance on long-context tasks; we refer to this augmented LoRA+ variation as HyLoRA.

**4. Experiments**


**4.1. Experimental Setup**


**Models.** We enhance the recall capabilities of pre-trained hybrid SSM models by fine-tuning them
with different Attention layers on spans of tokens longer than the ones used for pre-training. We
consider Mamba-2-Hybrid 2.7B Waleffe et al. (2024) as our representative SSM hybrid model and
provide results for Zamba2-Hybrid Glorioso et al. (2024a) in Appendix D. We also explore expanding
the span of Transformer models in Appendix C. These models, obtained from Hugging Face, were
pre-trained with a context size of 2048. In our experiments, “Non-fine-tuned” refers to the pre-trained
model with no fine-tuning. Similar to Chen et al. (2024), we utilize SE-Attn for efficient fine-tuning
and revert to using Full-Attn during evaluation; this is discussed further in Appendix A.
**Datasets.** We fine-tune models using a common language dataset and provide additional results when
fine-tuning on PG-19 Rae et al. (2019a) and a mixture of language and code dataset in Appendix E.
**Baselines.** We compare SE-Attn to three Attention variants: standard full Attention (“Full-Attn”)
Vaswani et al. (2017), Sliding Window Attention (“SW-Attn”) Beltagy et al. (2020), and Shifted
Sparse Attention ( _S_ [2] -Attn) Chen et al. (2024). Full-Attn serves as the paragon, as the other methods
aim to approximate it. All Attention implementations use FlashAttention-2 Dao (2024).
**Training Procedure.** We adopt the training recipe used in Chen et al. (2024) to fine-tune our models,
with the exception of using HyLoRA instead of LoRA+. See Appendix G for more details.
**Evaluation Metrics.** We assess the performance of our models across a range of common benchmarks. To assess the predictive capabilities of our fine-tuned models on long sequence lengths, we
measure perplexity (PPL) on the PG-19 validation set; however, as discussed further in Appendix B,
we do not find PPL to be a faithful measure of a model’s long context abilities. To assess performance
on more real-world language tasks in the short and long-context settings, we evaluate our models
on various LM Evaluation Harness tasks Gao et al. (2024). To measure our models’ performance
on long-context tasks, we evaluate on the RULER Hsieh et al. (2024) benchmark. We consider
eleven RULER tasks, encompassing needle-in-a-haystack, variable-hopping, and aggregation tasks;
we aggregate these metrics into five groups as explained in Appendix I.4. We provide additional
long-context benchmarks in Appendix E.3.


**4.2. Results**


All of our Mamba-2-Hybrid models are fine-tuned with a context size of 8192. For SE-Attn, we use
a block size of 32, and a top- _k_ of 8. For SW-Attn, we use a window size of 4096. _S_ [2] -Attn uses the
parameters from Chen et al. (2024). When fine-tuning Mamba-2-Hybrid using SE-Attn, we found that
applying the same chunk sizes at each layer leads to suboptimal downstream performance. Therefore,
we segment each sample into chunks of variable sizes (picked randomly from _{_ 2048 _,_ 4096 _}_ ). We


7


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO

|Attention|Eval Context Size (PG-19 PPL ↓) Short Context Tasks (↑) Long Context Tasks (↑)<br>2048 8192 16384 32768 ARC-E ARC-C Hella. LAMB. PIQA WG Avg. SWDE SQA SNQA Avg.|Col3|Col4|
|---|---|---|---|
|Non-fne-tuned|10.72<br>14.99<br>19.35<br>26.37|69.91<br>37.97<br>67.62<br>69.84<br>76.06<br>65.04<br>64.41|85.60<br>15.18<br>3.65<br>34.81|
|Full-Attn|10.99<br>10.28<br>10.39<br>11.14|69.53<br>38.48<br>67.30<br>68.93<br>75.08<br>64.40<br>63.95|85.24<br>26.99<br>19.75<br>43.99|
|SW-Attn|10.98<br>10.80<br>11.82<br>13.45|69.82<br>38.23<br>67.35<br>69.18<br>75.30<br>63.85<br>63.95|84.61<br>24.85<br>15.41<br>41.63|
|_S_~~2~~-Attn|10.87<br>12.89<br>14.67<br>16.37|70.12<br>38.05<br>67.39<br>69.84<br>75.95<br>64.56<br>64.32|86.41<br>17.44<br>8.53<br>37.46|
|SE-Attn|10.99<br>10.45<br>11.14<br>12.64|70.20<br>38.65<br>67.15<br>69.11<br>75.57<br>63.93<br>64.10|85.96<br>26.70<br>18.04<br>43.57|



Table 1: **Fine-tuning Mamba-2-Hybrid with SE-Attn outperforms fine-tuning with** _S_ [2] **-Attn**
**and SW-Attn on long-context natural language tasks.** We evaluate PG-19 validation perplexity
(PPL) and observe that fine-tuning with SE-Attn preserves performance at longer contexts better
than _S_ [2] -Attn and SW-Attn. On long context tasks from the LM Harness suite, SE-Attn outperforms
_S_ [2] -Attn and SW-Attn. See Appendix J for task abbreviation definitions.


found this prevents the model from overfitting to a fixed chunk size, which is a hyperparameter
chosen a priori, independent of the actual data content. An ablation on SE-Attn with different chunk
sizes is provided in Section 4.3.
**Perplexity.** We provide PG-19 PPL results in Table 1. All fine-tuned models outperform the non-finetuned model. SE-Attn yields the closest performance to the paragon model fine-tuned with Full-Attn,
outperforming _S_ [2] -Attn and SW-Attn across all context sizes at or above the fine-tuning size.
**LM Harness.** Next, we evaluate Mamba-2-Hybrid models across short and long-context tasks in
the LM Evaluation Harness suite. Our results are provided in Table 1, where we observe that all
models perform similarly on short-context tasks—including the non-fine-tuned model—suggesting
there is no performance regression when fine-tuning on larger contexts. Furthermore, we observe
that fine-tuning with SE-Attn gives the closest performance to fine-tuning with Full-Attn.



|70<br>60<br>50<br>40<br>30<br>20<br>10<br>0<br>2048 4096 6144 8192 10240|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br> <br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>||||||12288<br>14336<br>163<br>|12288<br>14336<br>163<br>|


( _a_ ) Mamba-2-Hybrid Attention Variants


|70<br>60<br>50<br>40<br>30<br>20<br>2048 4096 6144 8192 10240|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>||||||||||
|2048<br>4096<br>6144<br>8192<br>10240<br><br> <br>20<br>30<br>40<br>50<br>60<br>70<br>|||||||12288<br>14336<br>16384<br>|12288<br>14336<br>16384<br>|12288<br>14336<br>16384<br>|



( _b_ ) Mamba-2-Hybrid LoRA Variants









Figure 2: **Fine-tuning with SE-Attn outperforms SW-Attn and** _S_ [2] **-Attn on the RULER bench-**
**mark. HyLoRA outperforms LoRA and LoRA+ on Hybrid models.** (a): We fine-tune Mamba-2Hybrid with a context size of 8192 and evaluate on eleven RULER tasks, as explained in Appendix I.4.
Fine-tuning with SE-Attn consistently outperforms SW-Attn and _S_ [2] -Attn even when evaluating on
context sizes beyond the fine-tuning size. (b): We fine-tune Mamba-2-Hybrid with SE-Attn using
LoRA, LoRA+, and HyLoRA. LoRA and LoRA+ perform sub-optimally. Our HyLoRA additionally
trains the 1D convolution layers and yields strong performance.


**RULER.** Next, we assess the performance of our models on long-context tasks using the RULER
benchmark. Figure 2 shows the average accuracy across eleven RULER tasks. We observe that


8


SPAN-EXPANDED ATTENTION


fine-tuning with SE-Attn performs similar to fine-tuning with Full-Attn, and outperforms _S_ [2] -Attn
and SW-Attn. We provide more detailed RULER results in Figure 8, where we see a substantial
improvement on the variable tracking (VT) task, which may be attributed to SE-Attn’s retrieval
during fine-tuning.





|70<br>60<br>50<br>40<br>30<br>20<br>102048 4096 614R4U|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>RU<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>|||||||
|2048<br>4096<br>6144<br>RU<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>|||||||
|2048<br>4096<br>6144<br>RU<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>|||8192<br>102<br>LER Sequence|40<br>1<br> Lengt|2288<br>14336<br>163<br> h|2288<br>14336<br>163<br> h|


( _a_ )

























|70<br>60<br>50<br>40<br>30<br>20<br>2048 4096 614R4U|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>RU<br>20<br>30<br>40<br>50<br>60<br>70<br>|||||||
|2048<br>4096<br>6144<br>RU<br>20<br>30<br>40<br>50<br>60<br>70<br>|||||||
|2048<br>4096<br>6144<br>RU<br>20<br>30<br>40<br>50<br>60<br>70<br>|||8192<br>102<br>LER Sequenc|40<br>1<br> Lengt|2288<br>14336<br>163<br> h|2288<br>14336<br>163<br> h|


( _b_ )



|Col1|Col2|Col3|Blo<br>88SS//81k6|ck Size (S) / Top-<br>k 1166SS//81k6k|
|---|---|---|---|---|
||||8S/32|k<br><br>16S/32k|
||||||
|2048<br>4|096<br>6144<br>|8192<br>102<br>|40<br>12<br>|288<br>14336<br>|


( _c_ )



|Col1|Col2|Col3|Blo<br>88SS//81k6|ck Size (S) / Top-<br>k 1166SS//81k6k|
|---|---|---|---|---|
||||8S/32|k<br><br>16S/32k|
||||||
|2048<br>4|096<br>6144<br>|8192<br>102<br>|40<br>12<br>|288<br>14336<br>|


( _d_ )



Figure 3: **SE-Attn ablations on Mamba-2-Hybrid.** (a): Attention-based memory retrieval (SE-Attn)
improves upon no retrieval and random retrieval. (b): Using SE-Attn with a chunk size chosen
randomly from _{_ 2048 _,_ 4096 _}_ acts as a regularizer and outperforms SE-Attn with fixed chunk sizes of
2048 and 4096. (c): SE-Attn with larger memory blocks (i.e., more tokens per block) with a smaller
top- _k_ tends to do better than smaller blocks with a larger top- _k_ . (d): An expansion span consisting of
256 total tokens (8 memory blocks with 32 tokens in each) gives the strongest performance.


**4.3. Ablations**


In this section, we ablate over some of the design choices of SE-Attn on Mamba-2-Hybrid. For
this analysis, we consider the RULER benchmark, as it is a strong indicator of performance on
long-context tasks. We use the average of the eleven RULER tasks defined in Appendix I.4.
**Does retrieval during training help?** In Section 3.2, we introduced SE-Attn-NoMem, a variant
of SE-Attn where we do not do any retrieval and process chunks independently. Naturally, we
do not expect this to do well due to the lack of shared information across chunks. We confirm
this in Figure 3(a), where we observe that SE-Attn-NoMem achieves a much lower performance
than SE-Attn with retrieval. Furthermore, we also fine-tune with SE-Attn-Random, a variant of
SE-Attn that populates its expansion span by retrieving random memory blocks for each chunk. This
improves upon SE-Attn-NoMem, indicating that retrieving some information from the past improves
performance; however, it does not do as well as SE-Attn, which retrieves blocks based on relevancy.
**Chunk size.** We found that using a random chunk size during each forward pass improved upon
having a fixed chunk size. In Figure 3(b), we compare the performance of SE-Attn, which chooses a
random chunk size in _{_ 2048 _,_ 4096 _}_ during each layer’s forward call, to SE-Attn with fixed chunk
sizes of 2048 and 4096. Random chunk sizes outperform fixed ones, suggesting a regularizing effect
that makes the model more robust to different context lengths.
**Block size and top-** _k_ **.** For a fixed expansion span size, we might expect that retrieving memory
blocks at a finer granularity should perform better than retrieving a smaller number of large blocks;
this is because retrieving a larger number of small blocks is as flexible as retrieving a small number
of large blocks. However, as illustrated in Figure 3(c), we found that fine-tuning SE-Attn with larger
block sizes and a smaller top-k gave the best performance. This is possibly due to the increased
complexity of the retrieval task, which gets more difficult as the number of possible blocks to retrieve
increases. As we are fine-tuning with LoRA, we hypothesize that the model’s capacity may not be


9


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO







|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||92<br>16384<br>|32768<br>65536<br>13<br>|32768<br>65536<br>13<br>|1072|


( _b_ )



|Col1|Col2|Col3|Col4|
|---|---|---|---|
|100<br>200||||
|8192<br>~~0~~|16384<br>32|768||
|8192<br>~~0~~|<br>32768<br>65536<br>131<br>|<br>32768<br>65536<br>131<br>|072|


( _a_ )





|More Memory Efficient<br>ter|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|**More Memory Eficient**<br>**ter**|**More Memory Eficient**<br>**ter**|** Memory Eficie**|**t**||||
|**More Memory Eficient**<br>**ter**|||||||
|**Fas**|||||||
|**Fas**|||||||


( _c_ )



Figure 4: **SE-Attn offers a greater runtime-memory trade-off than other Attention variants.**
We profile various attention layers during a single training step. SW-Attn uses a window size of 4096
and SE-Attn alternates between a chunk size of 2048 and 4096. (a): Runtime of a single training
step. (b): Peak GPU memory used during the training step. (c): Runtime vs. memory used. Points
on the lower left of the plot exhibit a stronger runtime-memory trade-off.


sufficient to learn to retrieve effectively. As illustrated in Figure 3(d), we found that an expansion
span populated with 256 retrieved tokens (32 memory blocks with 8 tokens in each) works best.
**HyLoRA for Hybrid Models.** We fine-tune our hybrid models using HyLoRA, which builds upon
LoRA+ Chen et al. (2024) by also training 1D convolution layers. In Figure 2(b) we show that,
when evaluated on RULER tasks, fine-tuning SE-Attn models with HyLoRA gives the strongest
performance. In Appendix B, we study the effect of the LoRA rank and apply HyLoRA to Full-Attn.


**4.4. Empirical Runtime Analysis**


While hardware-efficient Attention implementations, such as FlashAttention Dao (2024), enable
linear memory scaling, computational complexity remains quadratic in sequence length. In Figure 4(a), we profile different Attention layers on various context sizes for one training step and see
that SE-Attn is much faster than _S_ [2] -Attn and Full-Attn. Moreover, the runtime of SE-Attn is similar
to that of SW-Attn, despite SE-Attn’s much longer Attention span. We also note that the runtime
increase of Full-Attn/FlashAttention compared to SE-Attn is 4-5 _×_ for long sequences, while the
memory peak increase is 17%, as shown in Figure 4(b). SE-Attn has a minor memory overhead
because it requires storing partial block summary statistics in order to retrieve tokens from the past.
However, this overhead is outweighed by its faster runtime. As depicted in Figure 4(c), our method
has a strong runtime-memory trade-off—similar to that of SW-Attn, but with the added benefit of
improved performance on language benchmarks. See Appendix H for a theoretical runtime analysis.


**5. Conclusion**


We close with the limitations of our method. Although we have conducted experiments at a relatively
large model scale (2.7B) and long contexts (up to 32k), testing our method for efficiently adapting
larger models on longer contexts is paramount. Furthermore, while we utilize datasets that require
modeling of long-range dependencies, we found that perplexity-based tasks do not faithfully measure
models’ capabilities to handle long contexts, and instead tasks like RULER provide better signals of
long-context capabilities. However, RULER is mostly a synthetic dataset and does not cover more
nuanced tasks that require reasoning over long documents. Validating our method on more complex
long-range benchmarks is a promising area for future work.


10


SPAN-EXPANDED ATTENTION


**References**


Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao,
Ashish Rao, Atri Rudra, and Christopher Re. Just read twice: closing the recall gap for recurrent´
language models. _arXiv preprint arXiv:2407.05483_, 2024.


Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and
Vivek Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Com-_
_putational Linguistics (Volume 1: Long Papers)_, pages 3119–3137, Bangkok, Thailand, August
2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL
[https://aclanthology.org/2024.acl-long.172/.](https://aclanthology.org/2024.acl-long.172/)


Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
_arXiv preprint arXiv:2004.05150_, 2020.


Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_, 2023.


Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongloRA: Efficient fine-tuning of long-context large language models. In _The Twelfth International_
_Conference on Learning Representations_ [, 2024. URL https://openreview.net/forum?](https://openreview.net/forum?id=6PmJoRfdaK)
[id=6PmJoRfdaK.](https://openreview.net/forum?id=6PmJoRfdaK)


Zihang Dai. Transformer-xl: Attentive language models beyond a fixed-length context. _arXiv_
_preprint arXiv:1901.02860_, 2019.


Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In
_The Twelfth International Conference on Learning Representations_ [, 2024. URL https://](https://openreview.net/forum?id=mZn2Xyh9Ec)
[openreview.net/forum?id=mZn2Xyh9Ec.](https://openreview.net/forum?id=mZn2Xyh9Ec)


Tri Dao and Albert Gu. Transformers are ssms: generalized models and efficient algorithms through
structured state space duality. In _Proceedings of the 41st International Conference on Machine_
_Learning_, ICML’24. JMLR.org, 2024.


Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert
Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. _arXiv preprint_
_arXiv:2402.19427_, 2024.


Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
_arXiv preprint arXiv:2407.21783_, 2024.


Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, and Kangwook Lee. Parameter-efficient
fine-tuning of state space models. _arXiv preprint arXiv:2410.09016_, 2024.


11


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot
[language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.](https://zenodo.org/records/12608602)


Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, and Beren Millidge. The zamba2 suite: Technical report. _arXiv preprint_
_arXiv:2411.15242_, 2024a.


Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam
Ibrahim, and Beren Millidge. Zamba: A compact 7b ssm hybrid model. _arXiv preprint_
_arXiv:2405.16712_, 2024b.


Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv_
_preprint arXiv:2312.00752_, 2023.


Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re.´
Combining recurrent, convolutional, and continuous-time models with linear state space layers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,
_Advances in Neural Information Processing Systems_, volume 34, pages 572–585. Curran Asso[ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/](https://proceedings.neurips.cc/paper_files/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf)
[2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf)


Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured
state spaces. In _International Conference on Learning Representations_ [, 2022. URL https:](https://openreview.net/forum?id=uYLFoz1vlAC)
[//openreview.net/forum?id=uYLFoz1vlAC.](https://openreview.net/forum?id=uYLFoz1vlAC)


Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang
Zhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language
models? _arXiv preprint arXiv:2404.06654_, 2024.


Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International_
_Conference on Learning Representations_ [, 2022. URL https://openreview.net/forum?](https://openreview.net/forum?id=nZeVKeeFYf9)
[id=nZeVKeeFYf9.](https://openreview.net/forum?id=nZeVKeeFYf9)


Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: transformers are better than state space models at copying. In _Proceedings of the 41st International_
_Conference on Machine Learning_, ICML’24. JMLR.org, 2024.


Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.


[Gregory Kamradt. Needle in a haystack - pressure testing llms., 2023. URL https://github.](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main)
[com/gkamradt/LLMTest_NeedleInAHaystack/tree/main.](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main)


Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _The_
_International Conference on Learning Representations (ICLR)_, 2020.


12


SPAN-EXPANDED ATTENTION


Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,
Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformermamba language model. _arXiv preprint arXiv:2403.19887_, 2024.


Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for transformers.
In _Proceedings of the 37th International Conference on Neural Information Processing Systems_,
NIPS ’23, Red Hook, NY, USA, 2023. Curran Associates Inc.


Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient
infinite context transformers with infini-attention. _arXiv preprint arXiv:2404.07143_, 2024.


Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu,
and Soham De. Resurrecting recurrent neural networks for long sequences. In _International_
_Conference on Machine Learning_, pages 26670–26698. PMLR, 2023.


Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
Compressive transformers for long-range sequence modelling. _arXiv preprint_, 2019a. URL
[https://arxiv.org/abs/1911.05507.](https://arxiv.org/abs/1911.05507)


Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. _arXiv preprint arXiv:1911.05507_, 2019b.


Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the_
_26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages
3505–3506, 2020.


Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.


Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and
Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv_
_preprint arXiv:2307.08621_, 2023.


Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee´
Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and`
efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.


Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
_Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc.,
2017. [URL https://proceedings.neurips.cc/paper_files/paper/2017/](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
[file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)


13


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert
Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. _arXiv preprint arXiv:2406.07887_, 2024.


Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention
transformers with hardware-efficient training. In _Proceedings of the 41st International Conference_
_on Machine Learning_, ICML’24. JMLR.org, 2024a.


Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Advances in Neural In-_
_formation Processing Systems_, volume 37, pages 115491–115522. Curran Associates, Inc.,
2024b. [URL https://proceedings.neurips.cc/paper_files/paper/2024/](https://proceedings.neurips.cc/paper_files/paper/2024/file/d13a3eae72366e61dfdc7eea82eeb685-Paper-Conference.pdf)
[file/d13a3eae72366e61dfdc7eea82eeb685-Paper-Conference.pdf.](https://proceedings.neurips.cc/paper_files/paper/2024/file/d13a3eae72366e61dfdc7eea82eeb685-Paper-Conference.pdf)


Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with
delta rule. In _The Thirteenth International Conference on Learning Representations_, 2025. URL
[https://openreview.net/forum?id=r8H7xhYPwz.](https://openreview.net/forum?id=r8H7xhYPwz)


Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, Alessandro Achille, and Stefano Soatto. B'mojo: Hybrid state
space realizations of foundation models with eidetic and fading memory. In A. Globerson,
L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Advances_
_in Neural Information Processing Systems_, volume 37, pages 130433–130462. Curran Asso[ciates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/](https://proceedings.neurips.cc/paper_files/paper/2024/file/eb6281d3f2cfe2eb8b1307227d3677b3-Paper-Conference.pdf)
[2024/file/eb6281d3f2cfe2eb8b1307227d3677b3-Paper-Conference.pdf.](https://proceedings.neurips.cc/paper_files/paper/2024/file/eb6281d3f2cfe2eb8b1307227d3677b3-Paper-Conference.pdf)


[Vladislav Zavadskyy. Lots of code, 2017. URL https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/zavadskyy/lots-of-code)
[zavadskyy/lots-of-code. Accessed: 2024-10-27.](https://www.kaggle.com/datasets/zavadskyy/lots-of-code)


14


SPAN-EXPANDED ATTENTION

|Attention|Evaluation Context Size (PPL ↓)<br>8192 16384 32768 65536|
|---|---|
|Non-fne-tuned|14.99<br>19.35<br>26.37<br>34.51|
|Full-Attn|10.28<br>10.39<br>11.14<br>12.38|
|SW-Attn|10.33<br>10.22<br>10.16<br>10.16|
|_S_~~2~~-Attn|10.73<br>10.76<br>11.85<br>13.72|
|SE-Attn|10.47<br>10.70<br>10.91<br>10.96|



Table 2: **Mamba-2-Hybrid fine-tuned with SE-Attn or SW-Attn preserves perplexity up to** 32 _×_
**the pre-training context size.** We fine-tune a Mamba-2-Hyrbid model (pre-trained on a context size
of 2048) on a context size of 8192 using various Attention variants. We test on longer context sizes
using the same Attention used during adaptation.


**Appendix A. Evaluating with Full Attention**


Our models are efficiently fine-tuned using SE-Attn and use Full-Attn during evaluation, as in Chen
et al. (2024). Due to KV-caching, the complexity of Full-Attn scales linearly during evaluation,
so an efficient Attention layer in this setting is not as crucial as in training. Nevertheless, for the
sake of completeness, we begin by evaluating our models with the same Attention layer used during
fine-tuning (i.e., we fine-tune and deploy our Hybrid model with SE-Attn). In Table 2, we experiment
with models that use Full-Attn, SW-Attn, _S_ [2] -Attn, and SE-Attn; then, we evaluate perplexity on the
PG-19 dataset. All models were fine-tuned with a context size of 8192. We observe that SW-Attn is
best at preserving the perplexity on context sizes up to 32 _×_ larger than the one used for pre-training.
SE-Attn is also able to maintain a lower perplexity across longer context sizes, while _S_ [2] -Attn
deteriorates much more quickly. As discussed further in the next section, perplexity may not be the
most faithful metric for assessing models’ long-context capabilities.


**Appendix B. LoRA for Hybrid Models and the Pitfalls of Perplexity**


In this section, we conduct a more thorough ablation study on the effect of training Mamba-2-Hybrid
with different variants of LoRA, and provide further evidence that perplexity is not a reliable indictor
of long context performance.
**Fine-tuning with Full-Attn.** We first expand upon Figure 2(b), where we showed that fine-tuning
Mamba-2-Hybrid with SE-Attn using HyLoRA produced a stronger model than training with LoRA
and LoRA+. In Figure 5, we provide a similar plot, but using Full-Attn during fine-tuning rather than
SE-Attn. We observe a similar trend, where HyLoRA produces a stronger model than LoRA and
LoRA+. In the remaining analyses in this section, we provide results for fine-tuning with SE-Attn, as
we observed similar trends when fine-tuning with Full-Attn.
**Effect of LoRA rank.** We next explore how the rank used for LoRA fine-tuning affects downstream
performance on the RULER task. Given its stronger performance, we focus on our HyLoRA. In our
previous Mamba-2-Hybrid experiments, we used a LoRA rank ( _r_ ) of 32 and an _α_ of 64 (for our
Transformer experiments, we used _r_ = 8 and _α_ = 16 as in Chen et al. (2024)). In Figure 6, we plot
the average RULER results for Mamba-2-Hybrid models fine-tuned with HyLoRA using different
ranks (we scale _α_ to maintain the ratio _α/r_ = 2). Here, we observe that training with a larger rank
improves downstream performance, and we start to see some saturation around _r_ = 64.


1


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


|60<br>(%)<br>Accuracy<br>50<br>RULER<br>40<br>Avg.<br>30<br>20|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|20<br>30<br>40<br>50<br>60<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>Avg. RULER Accuracy (%)||48<br>4096<br>61|48<br>4096<br>61|4<br>81|92<br>102|40<br>1|2288<br>1|336<br>16384|336<br>16384|





Figure 5: **HyLoRA outperforms LoRA and LoRA+ on Hybrid models.** We fine-tune Mamba-2Hybrid with Full-Attn using LoRA, LoRA+, and HyLoRA. We find that LoRA and LoRA+ perform
sub-optimally compared to HyLoRA which also trains 1D convolution layers.


In Table 3, we provide PG-19 perplexity results for Mamba-2-Hybrid trained with different LoRA
variants. All models are fine-tuned with a context size of 8192 and evaluated with multiple context
sizes. For a given context size, we do not see a substantial difference in the perplexity. This is similar
to the observation in Chen et al. (2024). However, as discussed above, the rank can have a significant
effect on downstream long-context tasks that require strong recall capabilities, as in RULER. Hence,
while perplexity results may be promising, they are not necessarily indicative of performance on
more complex long-context tasks.
Based on the analyses in this section, we conclude the following:


  - Fine-tuning the 1D convolution layers, as we do in our HyLoRA, significantly improves
performance on downstream long-context tasks that require retrieval, such as RULER.


  - Fine-tuning with larger LoRA ranks improves performance up to a certain point—64 for our
experiments.


  - Perplexity may not be the most faithful metric for assessing performance on long-context
downstream tasks. Instead, researchers should consider evaluating on more complex tasks,
such as those in the RULER benchmark.


**Appendix C. Applying SE-Attention to Transformers**


In this section, we show that SE-Attn can also be applied to Transformer models and use Llama 1 7B
Touvron et al. (2023a) as our representative Transformer model. When fine-tuning Llama 7B models,
we found that using a fixed chunk size of _M_ = 4096 gave better downstream performance. All of


2


SPAN-EXPANDED ATTENTION


|70<br>(%)<br>60<br>Accuracy<br>50<br>RULER<br>40<br>Avg.<br>30<br>204|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|204<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||8<br>4096<br>61|8<br>4096<br>61|4<br>81|92<br>102|40<br>1|2288<br>1|336<br>16384|336<br>16384|



Figure 6: **Fine-tuning with a larger LoRA rank using HyLoRA improves performance on the**
**RULER benchmark.** We fine-tune Mamba-2-Hybrid using HyLoRA with different LoRA ranks (we
maintain a LoRA rank to alpha ratio of 2). We observe that fine-tuning with a larger rank produces
stronger downstream results on RULER, with some saturation when using a rank of 64.

|Sequence Length|LoRA Method (Rank=32) LoRA Rank (Method=HyLoRA)<br>LoRA LoRA+ HyLoRA 8 16 32 64|Col3|
|---|---|---|
|2048|10.77<br>10.98<br>10.99|11.00<br>11.00<br>10.99<br>10.99|
|8192|10.29<br>10.49<br>10.45|10.45<br>10.46<br>10.45<br>10.44|
|16384|10.91<br>11.18<br>11.14|11.03<br>11.10<br>11.14<br>11.14|
|32768|12.34<br>12.66<br>12.64|12.37<br>12.53<br>12.64<br>12.64|
|65536|13.99<br>14.36<br>14.26|13.84<br>14.08<br>14.26<br>14.28|



Table 3: **Mamba-2-Hybrid LoRA Ablations.** We fine-tune Mamba-2-Hybrid with a context size
of 8192 with SE-Attn using different LoRA variants. We consider LoRA, LoRA+, and HyLoRA
(ours) and evaluate perplexity on the PG-19 dataset. We observe that all LoRA variants yield similar
perplexity results. However, as depicted in Figure 2 and Figure 6, different LoRA variants yield
substantially different performances on more complex tasks.


our Llama models are fine-tuned with a context size of 16384. For SE-Attn, we use a block size
of 32, and a top- _k_ of 8. For SW-Attn, we use a window size of 4096, and _S_ [2] -Attn uses the default
parameters in Chen et al. (2024). Due to computational constraints, we do not consider fine-tuning
with Full-Attn. All Attention variants use the same RoPE Su et al. (2024) scaling as in Chen et al.
(2024).

**Perplexity.** Fine-tuning Llama with SE-Attn leads to better generalization on context sizes smaller
and larger than the one used for fine-tuning. In Table 4, we observe that fine-tuning with SE-Attn
preserves the performance of the non-fine-tuned model at smaller context sizes, and offers greater
generalization to larger context sizes, as measured on PG-19 validation perplexity.


3


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO

|Attention|Eval Context Size (PG-19 PPL ↓) Short Context Tasks (↑) Long Context Tasks (↑)<br>2048 8192 16384 32768 ARC-E ARC-C Hella. LAMB. PIQA WG Avg. SWDE SQA SNQA Avg.|Col3|Col4|
|---|---|---|---|
|Non-fne-tuned|8.63<br>17.62<br>105.78<br>244.32|73.19<br>41.38<br>66.64<br>54.38<br>76.28<br>62.51<br>62.40|38.52<br>20.75<br>12.36<br>23.88|
|SW-Attn<br>_S_2-Attn|8.80<br>8.17<br>8.35<br>10.32<br>9.32<br>8.64<br>8.58<br>10.42|74.20<br>43.09<br>75.00<br>71.96<br>77.42<br>67.48<br>68.19<br>74.20<br>42.58<br>73.84<br>69.94<br>77.80<br>67.72<br>67.68|82.81<br>24.41<br>21.06<br>42.76<br>80.56<br>23.36<br>19.65<br>41.19|
|SE-Attn|8.76<br>8.13<br>8.01<br>9.28|75.04<br>44.37<br>75.02<br>71.03<br>78.02<br>67.17<br>68.44|84.79<br>24.59<br>21.36<br>43.58|



Table 4: **Fine-tuning Llama1 with SE-Attn outperforms fine-tuning with** _S_ [2] **-Attn and SW-**
**Attn on natural language tasks.** We fine-tune Llama1 with a context size of 16384 using various
Attention variants. Similar to applying SE-Attn to Mamba-2-Hybrid, here we again observe that
fine-tuning Llama with SE-Attn improves upon SW-Attn and _S_ [2] -Attn.

|80<br>70<br>(%)<br>60<br>Accuracy<br>50<br>40<br>RULER<br>30<br>20 Avg.<br>10<br>0<br>20|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)|||||||||||
|20<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>Avg. RULER Accuracy (%)||48<br>4|96<br>61|44<br>81|92<br>102|0<br>12|288<br>1|336<br>16|384<br>18|32|



Figure 7: **Fine-tuning with SE-Attn outperforms SW-Attn and** _S_ [2] **-Attn on the RULER bench-**
**mark when applied to Llama1.** We fine-tune Llama1 with a context size of 16384 using various
Attention variants. We average over eleven RULER tasks, as explained in Appendix I.4. Fine-tuning
with SE-Attn consistently outperforms SW-Attn and _S_ [2] -Attn even when evaluating on context sizes
beyond the fine-tuning size.


**LM Harness.** Fine-tuning Llama with SE-Attn yields a stronger performance on long-context tasks
on the LM Evaluation Harness benchmark. As shown in Table 4, fine-tuning with SW-Attn, _S_ [2] Attn, and SE-Attn all improve upon the non-fine-tuned model on shorter context tasks and perform
similarly. However, SE-Attn gives a greater performance on long context tasks.


**RULER.** Fine-tuning Llama with SE-Attn produces a model with stronger performance on RULER
tasks than fine-tuning with SW-Attn and _S_ [2] -Attn, as illustrated in Figure 7. On average, SW-Attn
and _S_ [2] -Attn perform similarly, however, fine-tuning with SE-Attn improves performance by _∼_ 5%.
Despite all models having a similar PPL up to a context size of 16k as shown in Table 4, we observe
a substantial difference between the RULER performance of models fine-tuned with our SE-Attn and
those fine-tuned with the SW-Attn and _S_ [2] -Attn, again suggesting that PPL is not the most accurate
assessment of long-context performance, as discussed in Appendix B.


4


SPAN-EXPANDED ATTENTION


**Appendix D. Beyond Mamba-2-Hybrid**


In this section, we apply SE-Attn to the Zamba2 1.2B model Glorioso et al. (2024a). Zamba2 uses
shared Attention blocks with different LoRA adapters for the shared blocks. Despite its intricate
architecture, we can replace these layers with SE-Attn and fine-tune it to perform on larger context
sizes. The pre-trained model was trained with a context size of 4096 and we are able to efficiently
fine-tune it with a context size of 12,288. We benchmark our model on NIAH tasks from RULER and
provide results in Table 5. We compare to Yang et al. (2025) and Yang et al. (2024b) and demonstrate
competitive performance to SOTA models.

|S-NIAH-1 S-NIAH-2 S-NIAH-3<br>Model 1K 2K 4K 8K 1K 2K 4K 8K 1K 2K 4K 8K|S-NIAH-1|S-NIAH-2|S-NIAH-3|
|---|---|---|---|
|S-NIAH-1<br>S-NIAH-2<br>S-NIAH-3<br>Model<br>1K<br>2K<br>4K<br>8K<br>1K<br>2K<br>4K<br>8K<br>1K<br>2K<br>4K<br>8K|1K<br>2K<br>4K<br>8K|1K<br>2K<br>4K<br>8K|1K<br>2K<br>4K<br>8K|
|DeltaNet<br>Mamba2<br>Gated DeltaNet|97.4<br>96.8<br>99.0<br>98.8<br>99.2<br>98.8<br>65.4<br>30.4<br>98.4<br>88.4<br>91.4<br>91.8|98.4<br>45.6<br>18.6<br>14.4<br>99.4<br>98.8<br>56.2<br>17.0<br>100.0<br>99.8<br>92.2<br>29.6|85.2<br>47.0<br>22.4<br>-<br>64.4<br>47.6<br>4.6<br>-<br>86.6<br>84.2<br>27.6<br>-|
|Zamba2-Hybrid<br>**Zamba2-Hybrid + SE-Attn**|100.0<br>99.2<br>99.4<br>0.0<br>100.0<br>100.0<br>100.0<br>41.0|100.0<br>100.0<br>75.0<br>0.0<br>100.0<br>100.0<br>100.0<br>42.0|94.4<br>75.4<br>37.6<br>0.0<br>100.0<br>100.0<br>94.0<br>34.0|



Table 5: **SE-Attn can efficiently extend the context length of Zamba2-Hybrid** . We fine-tune
Zamab2-Hybrid using SE-Attn and evaluate on RULER NIAH tasks.


**Appendix E. Additional Fine-Tuning Results on Mamba-2-Hybrid**


In this section, we provide additional benchmarks on the Mamba-2-Hybrid model. We begin by
expanding upon Figure 2(a) with additional RULER tasks in Figure 8. Here, we observe that Mamba2-Hybrid fine-tuned with SE-Attn consistently outperforms SW-Attn and _S_ [2] -Attn on all RULER
tasks across a broad range of context sizes—even beyond the 8192 fine-tuning context size.


**E.1. Fine-Tuning on Natural Language + Code**


Next, we consider fine-tuning on a dataset that augments natural language with a code dataset. In
particular, we construct a dataset consisting of 70% natural language, and 30% C++ code extracted
from the Lots of Code Zavadskyy (2017) dataset. We provide PG-19 validation PPL results, as
well as results on tasks from the LM Evaluation Harness suite in Table 6. Similar to fine-tuning on
natural language only (as in Table 1), we observe that fine-tuning with SE-Attn yields the strongest
performance on downstream tasks. We provide performance on RULER in Figure 9. We again
observe that fine-tuning with SE-Attn yields the strongest perfromance compared to other efficient
Attention layers. Moreover, compared to fine-tuning only on natural language, here we observe a
greater improvement on the Variable Tracking task. As explained in Appendix I.4, this task requires
the model to keep track of the values of variables that are defined and overridden throughout the
input context, and must then return the variables equal to some value. This requires strong recall
capabilities, which fine-tuning with SE-Attn enables, and is amplified by the use of code data in the
fine-tuning data mix.


**E.2. Fine-Tuning on PG-19**


Next, we consider fine-tuning on the PG-19 Rae et al. (2019b) dataset. We provide perplexity results
on the validation set, along with LM Harness task results in Table 7. Compared to fine-tuning on a
different natural language dataset, and a natural language + code dataset as in Table 1 and Table 6,


5


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO














|100<br>80<br>60<br>40<br>20<br>0|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|0<br>20<br>40<br>60<br>80<br>100||||||
|0<br>20<br>40<br>60<br>80<br>100||||||
|0<br>20<br>40<br>60<br>80<br>100||||||


|0<br>0<br>0<br>0|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|0<br>0<br>0<br>0|||||
|0<br>0<br>0<br>0|||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||


|80<br>60<br>40<br>20<br>0<br>2048 4096 6144 8192 10240 1228<br>RULER Sequence Lengt|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
||||||14336 163<br>|



Figure 8: **Mamba-2-Hybrid RULER benchmark fine-tuned on natural language.** We fine-tune
Mamba-2-Hybrid with a context size of 8192 using multiple Attention variants and evaluate on the
RULER benchmark (see Appendix I.4 for definitions of each metric). On average, fine-tuning with
SE-Attn yields a performance close to the paragon model fine-tuned with Full-Attn . We observe the
biggest gain in needle-in-a-haystack (NIAH) tasks, as well as variable tracking (VT), all of which
require strong recall capabilities enabled by our SE-Attn.

|Attention|Eval Context Size (PG-19 PPL ↓) Short Context Tasks (↑) Long Context Tasks (↑)<br>2048 8192 16384 32768 ARC-E ARC-C Hella. LAMB. PIQA WG Avg. SWDE SQA SNQA Avg.|Col3|Col4|
|---|---|---|---|
|Non-fne-tuned<br>|10.72<br>14.99<br>19.35<br>26.37|69.91<br>37.97<br>67.62<br>69.84<br>76.06<br>65.04<br>64.41|85.60<br>15.18<br>3.65<br>34.81|
|Full-Attn<br>|10.94<br>10.23<br>10.36<br>11.09|70.12<br>38.14<br>67.40<br>69.34<br>75.08<br>64.56<br>64.11|84.88<br>25.99<br>19.61<br>43.49|
|SW-Attn<br>|10.94<br>10.76<br>11.81<br>13.43|69.70<br>38.82<br>67.51<br>69.38<br>75.41<br>64.88<br>64.28|84.52<br>24.44<br>15.30<br>41.42|
|_S_~~2~~-Attn<br>|10.86<br>12.89<br>14.67<br>16.36|69.95<br>37.88<br>67.45<br>69.94<br>76.01<br>64.72<br>64.32|86.50<br>16.65<br>8.61<br>37.25|
|SE-Attn<br>|10.95<br>10.41<br>11.07<br>12.50|70.45<br>38.91<br>67.39<br>69.07<br>75.08<br>64.96<br>64.31|85.24<br>26.14<br>18.08<br>43.15|



Table 6: **Fine-tuning Mamba-2-Hybrid with SE-Attn on a natural language + code dataset**
**outperforms fine-tuning with** _S_ [2] **-Attn and SW-Attn on natural language tasks.** We fine-tune
Mamba-2-Hybrid with a context size of 8192 using various Attention variants on a dataset that
consists of 70% natural language and 30% code. We evaluate PG-19 validation perplexity (PPL) and
observe that fine-tuning with SE-Attn yields better perplexity scores than _S_ [2] -Attn and SW-Attn. On
short-context tasks from the LM Harness suite, all models perform similarly. On long context tasks
from the LM Harness suite, SE-Attn outperforms _S_ [2] -Attn and SW-Attn.


here we obtain lower perplexity scores due to the lack of distribution shift. Interestingly, compared
to fine-tuning on the previously mentioned datasets, we observe a slight degradation on LM Harness
tasks. Moreover, in Figure 10 we plot RULER performance when fine-tuning on PG-19, and here we
also see a decay in performance compared to fine-tuning on the previous datasets. This suggests that
the PG-19 dataset may be too far out of distribution for these long-context retrieval task. Nevertheless,
we see that fine-tuning with SE-Attn yields the best performance across all of these tasks.


6


SPAN-EXPANDED ATTENTION














|100<br>80<br>60<br>40<br>20<br>0|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|0<br>20<br>40<br>60<br>80<br>100||||||
|0<br>20<br>40<br>60<br>80<br>100||||||
|0<br>20<br>40<br>60<br>80<br>100||||||


|0<br>0<br>0<br>0|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|0<br>0<br>0<br>0|||||
|0<br>0<br>0<br>0|||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||


|80<br>60<br>40<br>20<br>0<br>2048 4096 6144 8192 10240 1228<br>RULER Sequence Lengt|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 1228<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br> <br>||||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
||||||14336 163<br>  h|



Figure 9: **Mamba-2-Hybrid RULER benchmark fine-tuned on natural language + code.** We
fine-tune Mamba-2-Hybrid with different Attention layers on a dataset that consists of 70% natural
language and 30% code and evaluate on RULER. Compared to fine-tuning on only natural language
(as in Figure 8), we see a substantial improvement on tasks like variable tracking (VT), and needlein-a-haystack tasks (NIAH), both of which require strong recall capabilities enabled by fine-tuning
with SE-Attn.

|Attention|Eval Context Size (PG-19 PPL ↓) Short Context Tasks (↑) Long Context Tasks (↑)<br>2048 8192 16384 32768 ARC-E ARC-C Hella. LAMB. PIQA WG Avg. SWDE SQA SNQA Avg.|Col3|Col4|
|---|---|---|---|
|Non-fne-tuned<br>|10.72<br>14.99<br>19.35<br>26.37|69.91<br>37.97<br>67.62<br>69.84<br>76.06<br>65.04<br>64.41|85.60<br>15.18<br>3.65<br>34.81|
|Full-Attn<br>|10.73<br>10.04<br>10.14<br>10.91|67.34<br>37.12<br>66.80<br>68.62<br>74.32<br>62.67<br>62.81|84.88<br>25.35<br>18.46<br>42.90|
|SW-Attn<br>|10.72<br>10.59<br>11.64<br>13.25|66.96<br>37.54<br>66.83<br>68.79<br>74.54<br>63.30<br>62.99|84.79<br>22.54<br>14.09<br>40.48|
|_S_~~2~~-Attn<br>|10.78<br>12.74<br>14.42<br>16.11|69.36<br>38.14<br>67.45<br>69.90<br>76.12<br>64.72<br>64.28|86.50<br>17.00<br>7.74<br>37.08|
|SE-Attn<br>|10.73<br>10.22<br>10.84<br>12.28|67.42<br>37.88<br>66.48<br>69.22<br>73.88<br>61.88<br>62.80|85.15<br>23.06<br>16.46<br>41.55|



Table 7: **Fine-tuning Mamba-2-Hybrid with SE-Attn on PG-19 outperforms fine-tuning with**
_S_ [2] **-Attn and SW-Attn on long-context natural language tasks.** We fine-tune Mamba-2-Hybrid
with a context size of 8192 using various Attention variants on PG-19. We evaluate PG-19 validation
perplexity (PPL) and observe that fine-tuning with SE-Attn yields better perplexity scores than
_S_ [2] -Attn and SW-Attn. On short-context tasks from the LM Harness suite, _S_ [2] -Attn has the strongest
performance. On long context tasks from the LM Harness suite, SE-Attn outperforms _S_ [2] -Attn and
SW-Attn.


**E.3. Additional Long-context Benchmarks**


In this section, we evaluate our Mamba-2-Hybrid model fine-tuned on only language data on additional long-context tasks. First, we test our models’ long-context understanding on the LongBench
benchmark and report results in Table 8. Here, we observe that our fine-tuned models improve
over the pre-trained baseline models, suggesting that these models’ long-context understanding has
improved. Moreover, we see that models fine-tuned with our SE-Attn match the performance of the
model fine-tuned with Full-Attn .


7


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO














|100<br>80<br>60<br>40<br>20|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|20<br>40<br>60<br>80<br>100||||||
|20<br>40<br>60<br>80<br>100||||||
|20<br>40<br>60<br>80<br>100||||||


|0<br>0<br>0<br>0<br>0|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|0<br>0<br>0<br>0<br>0|||||
|0<br>0<br>0<br>0<br>0|||||
|0<br>0<br>0<br>0<br>0|||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||


|80<br>60<br>40<br>20<br>0<br>2048 4096 6144 8192 10240 12288<br>RULER Sequence Lengt|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|2048<br>4096<br>6144<br>8192<br>10240 12288<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br>||||||
|2048<br>4096<br>6144<br>8192<br>10240 12288<br>RULER Sequence Lengt<br>0<br>20<br>40<br>60<br>80<br>||||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
||||||14336 16<br>  h|



Figure 10: **Mamba-2-Hybrid RULER benchmark fine-tuned on PG-19.** We fine-tune Mamba2-Hybrid with different Attention layers on the PG-19 Rae et al. (2019b) dataset and then evaluate
on RULER. Compared to fine-tuning on other natural language datasets with a greater variety of
text as in Figure 8, here we observe a degradation in performance across all models, likely due to a
distribution shift in the PG-19 data and the RULER tasks.

|Attention|Single-Doc QA (↑) Multi-Doc QA (↑) Summarization (↑) Few-shot (↑) Code (↑)<br>NQA QQA MFQ HQA 2WM Mus GvR QMS MNs TRC TQA LCC RBP Avg.|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|Non-fne-tuned|0.92<br>3.78<br>13.92|4.72<br>8.26<br>1.54|9.65<br>3.71<br>23.23|59.0<br>63.22|60.55<br>31.99<br>|21.88|
|Full-Attn|3.25<br>7.15<br>16.35|8.03<br>9.51<br>3.69|25.4<br>17.64<br>26.05|64.5<br>82.09|56.67<br>55.58<br>|28.92|
|SW-Attn<br>_S_2-Attn|2.29<br>7.52<br>17.07<br>1.39<br>5.58<br>16.43|7.72<br>9.4<br>3.41<br>5.31<br>7.88<br>1.83|18.44<br>11.87<br>23.94<br>13.64<br>5.64<br>26.86|64.5<br>81.48<br>57.0<br>72.97|58.33<br>53.06<br><br>56.62<br>36.87<br>|27.62<br>23.69|
|SE-Attn|3.19<br>9.37<br>16.0|8.31<br>9.31<br>3.94|24.65<br>14.39<br>26.86|66.5<br>80.82|58.71<br>53.65<br>|28.90|



Table 8: **SE-Attn matches the performance of Full-Attn on long-context understanding tasks** .
We fine-tune Mamba-2-Hybrid models using various attention mechanisms on 14 LongBench Bai
et al. (2024) tasks.


We previously considered RULER as a means of assessing our models’ in-context recall capabilities. While RULER can assess our models’ performance on synthetic in-context recall tasks, we
also want to evaluate our their performance on “real-world” in-context tasks. As such, in Table 9, we
evaluate on the tasks considered in Arora et al. (2024). We observe that fine-tuning with our SE-Attn
is able to match the performance of fine-tuning with Full-Attn on these tasks.


**E.4. Evaluating with Efficient Attention**


In Appendix A, we explained how we fine-tune our models using an efficient Attention mechanism,
and then evaluate with full attention. In Table 2, we showed that evaluating our models with efficient
Attention mechanisms produced strong results on perplexity benchmarks. However, we did not
observe similarly positive results on more complex tasks, such as those in RULER. As illustrated in
Figure 11, using the same efficient Attention layer used during fine-tuning does not perform as well
as evaluating with Full-Attn (we omit results for models fine-tuned with _S_ [2] -Attn and evaluated with


8


SPAN-EXPANDED ATTENTION

|Attention|SWDE SQUADv2 FDA TriviaQA NQ Dro|p Avg|
|---|---|---|
|on-fne-tun|ed<br>85.60<br>50.11<br>76.41<br>22.60<br>6.62<br>3.2|0<br>40.76|
|Full-Attn|85.24<br>50.09<br>80.22<br>25.75<br>6.79<br>3.5|5<br>41.94|
|SW-Attn<br>_S_2-Attn|84.61<br>50.10<br>78.31<br>25.51<br>6.76<br>3.6<br>86.41<br>50.11<br>81.49<br>23.63<br>6.84<br>3.3|2<br>41.48<br>2<br>41.97|
|SE-Attn|85.96<br>50.09<br>82.12<br>26.39<br>7.17<br>3.7|6<br>42.58|



Table 9: **SE-Attn matches the performance of Full-Attn on real-world in-context tasks** . We
fine-tune Mamba-2-Hybrid models with different attention layers. We do not truncate inputs to 2K
tokens when evaluating as was done in Arora et al. (2024).


_S_ [2] -Attn as the implementation of _S_ [2] -Attn does not support evaluating with it at arbitrary sequence
lengths). Hence, our training pipeline involves training a model with an efficient Attention layer,
and reverting to Full-Attn during inference. Moreover, we again note that perplexity is a misleading
metric in regard to measuring a model’s performance on long context tasks. Though we observed
strong performance on perplexity when evaluating with these efficient Attention layers, this did not
translate to stronger performance on tasks that require a strong recall capability, such as those in
RULER.










|100<br>90<br>80<br>70<br>60<br>50<br>40<br>30<br>20<br>2048 4|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||||||||
|2048<br>4<br>20<br>30<br>40<br>50<br>60<br>70<br>80<br>90<br>100<br> <br>||48<br>4|096<br>6144<br>|8192<br>10240<br>|12288<br>14<br>|336<br>16384|336<br>16384|


|100<br>80<br>60<br>40<br>20<br>2048 4|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>|||||||||
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>|||||||||
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>|||||||||
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>|||||||||
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>|||||||||
|2048<br>4<br>20<br>40<br>60<br>80<br>100<br> <br>||48<br>4|096<br>6144<br>|8192<br>10240<br>|12288<br>14<br>|12288<br>14<br>|336<br>16384|336<br>16384|







( _a_ )



( _b_ )



Figure 11: **Evaluating with efficient Attention mechanisms on RULER does not do as well as**
**evaluating with standard full Attention.** We fine-tune Mamba-2-Hybrid with SW-Attn (a) and
SE-Attn (b) and then evaluate on the NIAH-Single-1 RULER task using either the same Attention
layer used for fine-tuning, or Full-Attn. We observe that fine-tuning with efficient Attention layers
(SE-Attn and SW-Attn) and then using Full-Attn during evaluation yields better results. Lines
with a circle marker denote models fine-tuned with an efficient Attention mechanism, and then
evaluated with the same Attention mechanism; lines with a diamond marker were fine-tuned with an
efficient Attention mechanism, but evaluated with Full-Attn. We omit results for evaluating models
fine-tuned with _S_ [2] -Attn as this Attention mechanism does not support arbitrary sequence lengths
during inference.


9


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


**Appendix F. Retrieval with Landmark Tokens**


Landmark Attention Mohtashami and Jaggi (2023) was recently introduced as a way for Transformer
models to process long sequences by inserting “landmark” tokens into the sequence whose representations would then be used as summaries of the blocks of tokens that came before them. At a high
level, our approach in SE-Attn is similar. However, we simplify the process of compressing blocks
of tokens by forgoing the use of landmark tokens and instead using Attention to summarize them, as
described in Section 3.3. Moreover, to learn which blocks to retrieve, we do not rely on a complex
Grouped Softmax function, and instead use a simple Cross-Attention score to ascertain relevance. In
this way, we implement retrieval natively into the model’s architecture.
In this section, we consider a variant of SE-Attn, which we refer to as SE-Attn-LM, that is
inspired by Landmark Attention. Namely, instead of using our Attention-based compression to
construct summaries of memory blocks, we insert a non-learnable “landmark” token into each
memory block, and use the cross-attention between this token and the memory tokens as the summary
of the memory block. We compare the performance of this variant to our summaries computed
using average pooling of Attention scores (see Section 3.3) in Figure 12. Here, we see that SE-Attn
yields better performance. We suspect this is because using a non-learnable landmark token to
summarize memory blocks is too challenging of a task to accomplish using standard LoRA. While
full fine-tuning (without LoRA) may improve the performance of SE-Attn-LM, this is beyond the
scope of our work as we prioritize efficiency.


|70<br>(%)<br>60<br>Accuracy<br>50<br>RULER<br>40<br>Avg.<br>30<br>20|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||||||||||
|20<br>30<br>40<br>50<br>60<br>70<br>Avg. RULER Accuracy (%)||48<br>4096<br>61|48<br>4096<br>61|4<br>819|102|40<br>12288<br>1|40<br>12288<br>1|336<br>16384|336<br>16384|





Figure 12: **Summarizing memory blocks via average pooling of attention yields stronger**
**performance than summarizing them using “landmark” tokens.** We fine-tune Mamba-2-Hybrid
using our SE-Attn, and SE-Attn-LM, which summarizes memory blocks with a landmark token
(similar to Mohtashami and Jaggi (2023)) instead of using the average of the Self Attention output
of memory blocks, as in SE-Attn. We find that our simpler SE-Attn produces a stronger model,
likely due to the easier training task, which does not require adapting the model to leverage landmark
tokens for compression.


10


SPAN-EXPANDED ATTENTION


**Appendix G. Training Details**


Our fine-tuning recipe largely follows Chen et al. (2024), with the exception of using a larger learning
rate for SE-Attn, SW-Attn, and Full-Attn models (2 _×_ 10 _[−]_ [4] vs. the default 2 _×_ 10 _[−]_ [5] for _S_ [2] ). We
found that using a larger learning rate for _S_ [2] did not improve its performance, as shown in Figure 13,
so we used the default 2 _×_ 10 _[−]_ [5] learning rate for all _S_ [2] fine-tuning experiments.
All of our experiments are conducted on a single 8xA100 node. We fine-tune for 1000 steps on
a total of 0.5B tokens. We fine-tune Mamba-2-H with a context size of 8192 with 8 accumulation
steps and a per-device batch size of 1. We fine-tune Llama1 with with a context size of 16384 tokens
with 4 accumulation steps and a per-device batch size of 1. We use FlashAttention-2 Dao (2024) and
DeepSpeed Stage 2 Rasley et al. (2020).




|60<br>50 (%)<br>Accuracy<br>40<br>30 Avg.<br>20<br>10<br>20|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||||||||||
|20<br>10<br>20<br>30<br>40<br>50<br>60<br>Avg. Accuracy (%)||48<br>4096<br>61|48<br>4096<br>61|4<br>81|92<br>102|40<br>1|2288<br>1|336<br>16384|336<br>16384|



Figure 13: **A larger learning rate does not improve the performance of** _S_ [2] **.** Here we fine-tune a
Mamba-2-Hybrid model using _S_ [2] -Attn with two different learning rates: 2 _×_ 10 _[−]_ [4] and 2 _×_ 10 _[−]_ [5] .
The learning rate used in Chen et al. (2024) is 2 _×_ 10 _[−]_ [5], which we found to work well. For all other
Attention layers, we found 2 _×_ 10 _[−]_ [4] to offer a slight improvement over 2 _×_ 10 _[−]_ [5] .


**Appendix H. Runtime and Memory Analysis**


The runtime of SE-Attn is lower than Full-Attn and _S_ [2] -Attn on large contexts. For an input
with sequence length _L_, SE-Attn first constructs _U_ = _LS_ [memory blocks of size] _[ S]_ [. Attention]
is applied on each. Thus, the complexity of this operation is _O_ ( _d_ model _US_ [2] ) = _O_ ( _d_ model _LS_ ).
Next, the input sequence is split into _T_ = _ML_ [chunks of size] _[ M]_ [. Cross-Attention is then applied]
between each chunk’s query and each memory block’s compressed representation; the cost of
this is _O_ ( _d_ model _MTU_ ) = _O_ ( _d_ model _[L]_ _S_ [2] [)][. Cross-Attention is then applied between each chunk’s]

query tokens and the concatenations of the chunk’s key tokens and memory tokens. Since SEAttn retrieves _K_ blocks with _S_ tokens in each, the cost of this Cross-Attention for each chunk is
_O_ ( _d_ model _M_ ( _SK_ + _M_ )) = _O_ ( _d_ model _M_ [2] ) since _SK < M_ . The cost for all chunks is therefore


11


NUNEZ ZANCATO BOWMAN GOLATKAR XIA SOATTO


_O_ ( _d_ model _TM_ [2] ) = _O_ ( _d_ model _LM_ ). The total cost of SE-Attn is therefore _O_ ( _d_ model _LS_ + _d_ model _LM_ +
_d_ model _[L]_ _S_ [2] [)][. For sufficiently large] _[ S]_ [, the runtime of SE-Attn is faster than Full-Attn and] _[ S]_ [2][-Attn,]

especially on large contexts, and is similar to that SW-Attn.


**Appendix I. RULER Task Definitions**


The RULER benchmark Hsieh et al. (2024) consists of four different task categories: retrieval,
multi-hop tracing, aggregation, and question answering. In this paper, we focus only on the retrieval,
multi-hop tracing, and aggregation tasks (we provide question answering results on the LM Evaluation
Harness benchmark). These three categories span eleven different tasks as explained below.


**I.1. Needle-in-a-Haystack (NIAH) Tasks**


RULER consists of 8 different NIAH tasks. These tasks embed “needles” in a string of noise. These
needles are typically key-value pairs, and the goal is to return the value of a key. These tasks are
characterized by six parameters:


  - type ~~h~~ aystack (TH): This specifies the type of noise to embed the key in. The choices are
“repeat” which constructs noise as in Mohtashami and Jaggi (2023), “essay” which will use
sentences from the Paul Graham essays Kamradt (2023), or “needle” in which case each
sentence will define a new key-value pair.


  - type ~~n~~ eedle ~~k~~ (TK): This specifies the type of the needle’s key. The options are “words”, in
which case the key is a word (in the form of adjective-noun, e.g., spiritual-oven), or “uuids” in
which case the key is a UUID.


  - type ~~n~~ eedle ~~v~~ (TV): This specifies the type of the needle’s value. It can either be “numbers”
in which case the value is a 7-digit number, or it can be “uuids” in which case the value is a
UUID.


  - num ~~n~~ eedle ~~k~~ (NK): This specifies the number of key-value pairs to embed in the haystack.


  - num ~~n~~ eedle v (NV): This specifies how many different values a key is assigned. If greater
than 1, the goal is output all the values of they key.


  - num ~~n~~ eedle q (NQ): This specifies the number of different keys the model must return the
value for.


**I.2. Multi-hop Tracing Tasks**


RULER considers a “variable tracking” task that is a form of coreference resolution. In this task, a
sequence of variables are defined throughout noisy text as in Mohtashami and Jaggi (2023). New
variables are defined as previous ones, and a final value is assigned to a particular variable. The goal is
to be able to trace back which variables have also been assigned the final value, i.e., determine which
variables refer to the final value. We use the default num ~~c~~ hains=1 and num ~~h~~ ops=4 parameters


12


SPAN-EXPANDED ATTENTION

|NIAH Task|TH|TK|TV|NK|NV|NQ|
|---|---|---|---|---|---|---|
|Single 1|repeat|words|numbers|1|1|1|
|Single 2|essay|words|numbers|1|1|1|
|Single 3|essay|words|uuids|1|1|1|
|Multikey 1|essay|words|numbers|4|1|1|
|Multikey 2|needle|words|numbers|1|1|1|
|Multikey 3|needle|uuids|uuids|1|1|1|
|Multivalue|essay|words|numbers|1|4|1|
|Multiquery|essay|words|numbers|1|1|4|



Table 10: **RULER NIAH definitions.** The “Needle-in-a-Haystack” (NIAH) tasks in the RULER
benchmarks are defined by 6 parameters which modulate the difficulty of the tasks. We consider 8
different NIAH tasks as defined above (these are the default NIAH tasks in the RULER library).


**I.3. Aggregation Tasks**


RULER considers two aggregation tasks, common words extraction (CWE), and frequent words
extraction (FWE). In CWE, the context consists of list of words, and the goal is to return the most
common words. We use the default parameters freq ~~c~~ w=30, freq ~~u~~ cw=3, and num ~~c~~ w=10. In FWE,
the context consists of random word strings, and the goal is to return the ones that appear the most
frequently. We use the default alpha=2 parameter for this.


**I.4. Aggregating RULER Tasks**


We aggregate the eleven RULER tasks above into six groups:


1. _NIAH-S_ : NIAH Single 1, NIAH Single 2, NIAH Single 3.


2. _NIAH-M_ : NIAH Multikey 1, NIAH Multikey 2, NIAH Multikey 3.


3. _NIAH-M-QV_ : NIAH Multivalue, NIAH Multiquery.


4. _VT_ : Variable Tracking.


5. _CF-WE_ : Common Words Extraction (CWE) and Frequent Words Extraction (FWE).


6. _Average_ : The average of all eleven tasks above.


**Appendix J. Task Abbreviations**


**LM Harness** : Hella=HellaSwag, LAMB=LAMBADA, WG=WinoGrande, SQA=ScrollsQAsper,
SNQA=ScorllsNarrativeQA.
**LongBench** : NQA=NarrativeQA, QQA=QasperQA, MFQ=MultiFieldQA, HQA=HotpotQA,
2WM=2WikiMultihopQA, Mus=Musiqe, GvR=GovReport, QMS=QMSum, MNs=MultiNews,
TRC=TREC, TQA=TriviaQA, SSM=SamSum, RBP=RepoBench-P.


13


