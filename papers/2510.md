Under review as a conference paper at ICLR 2026

## MEMMAMBA: RETHINKING MEMORY PATTERNS IN STATE SPACE MODEL



**Youjin Wang** _[†]_
School of Statistics
Renmin University of China
Beijing, China


**Jiahao Yan** _[‡]_
Gao Ling Institute of Artificial Intelligence
Renmin University of China
Beijing, China


**Xiao Sun** _[∗]_
Shanghai Artificial Intelligence Laboratory
Shanghai, China



**Yangjingyi Chen** _[‡]_
Shanghai University of Finance and Economics
Shanghai, China


**Jiaxuan Lu** _[∗]_
Shanghai Artificial Intelligence Laboratory
Shanghai, China



ABSTRACT


With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics.
However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion,
making them hard to scale. Transformers can model global dependencies but are
constrained by quadratic complexity. Recently, selective state-space models such
as Mamba have demonstrated high efficiency with _O_ ( _n_ ) time and _O_ (1) recurrent inference, yet their long-range memory decays exponentially. In this work,
we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba’s long-range memory and how
does it retain information? To quantify key information loss, we further introduce
horizontal–vertical memory fidelity metrics that capture degradation both within
and across layers. Inspired by how humans distill and retain salient information
when reading long documents, we propose MemMamba, a novel architectural
framework that integrates state summarization mechanism together with crosslayer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over
existing Mamba variants and Transformers on long-sequence benchmarks such as
PG19-PPL and Passkey Retrieval, while delivering a 48% speedup in inference
efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity–memory trade-off, offering a
new paradigm for ultra-long sequence modeling.


1 INTRODUCTION


Long-sequence data typically refers to continuous sequences spanning thousands to millions of time
steps or tokens, which pervade modern machine learning applications, from modeling book-length
documents in NLP, to analyzing DNA sequences in bioinformatics, to processing complex multimodal medical records. A central challenge in sequence modeling is how to capture ultra-long-range


_†_ Independent first author: Youjin Wang (wangyoujin@ruc.edu.cn)

_‡_ These authors contributed equally as the second authors.
_∗_ Corresponding authors: Jiaxuan Lu (lujiaxuan@pjlab.org.cn); Xiao Sun.


1


Under review as a conference paper at ICLR 2026


dependencies while maintaining efficiency. Traditional architectures exhibit significant limitations
when dealing with such data. Recurrent neural networks (RNNs) and their variants (LSTM, GRU)
are inherently sequential and suffer from vanishing or exploding gradients, making them unstable
for long dependencies (Pascanu et al., 2013) (Hochreiter & Schmidhuber, 1997). Transformers introduced a paradigm shift with self-attention and global context modeling (Vaswani et al., 2017), but
their quadratic complexity in sequence length renders them inefficient for truly long contexts (Brown
et al., 2020). The trade-off between expressiveness and scalability has created an architectural impasse.


Recent advances in selective state-space models (SSMs), notably the Mamba architecture (Gu &
Dao, 2023), offer a compelling alternative. By decoupling sequence length from computation,
Mamba achieves linear-time complexity _O_ ( _n_ ) and constant-time recurrent inference _O_ (1), positioning itself as a promising foundation for long-sequence modeling. However, despite this computational leap, its memory fidelity degrades rapidly at scale. As sequence length grows, Mamba and
its successors ( _e.g._, Mamba-2) exhibit sharp declines in tasks demanding strong memory retention,
such as 5-shot MMLU or long-range key-value retrieval (Waleffe et al., 2024). This leads to a fundamental question: How does Mamba’s memory pattern evolve with distance and depth, and what
underlies its degradation?


This paper introduces a new lens for understanding and advancing long-sequence models. We
present the first systematic analysis of Mamba’s memory mechanism. Through mathematical derivation and information-theoretic analysis, we characterize its memory decay behavior and introduce
the _horizontal–vertical memory fidelity_ framework, which quantifies critical information loss from
two perspectives: token-level semantic transmission and cross-layer information coupling. Our analysis reveals that, although Mamba’s state update ensures computational stability, the contribution of
early information decays exponentially during both intra-layer recursion and inter-layer propagation,
fundamentally constraining its long-range memory capacity.


Building on these insights, we propose **MemMamba**, a novel architecture that reimagines statespace modeling as a structured memory system. Inspired by how humans take notes while reading
long texts, MemMamba integrates lightweight state summarization with cross-layer and cross-token
attention to dynamically preserve and reuse salient information, all while maintaining linear computational complexity. This “note-taking” mechanism alleviates long-range forgetting, breaking the
classical trade-off in SSMs. Empirically, MemMamba achieves breakthrough improvements across
multiple long-sequence benchmarks. On the PG19 language modeling task, it maintains stable perplexity (17.35) even at 60k tokens, where Mamba and DeciMamba (Ben-Kish et al., 2025) of similar
parameter scale collapse completely. On the Passkey Retrieval task, MemMamba preserves 90% retrieval accuracy at 400k tokens. On the cross-document retrieval task under noisy conditions, it
significantly outperforms both Transformers and existing state-space variants.


The main contributions of this work are summarized as follows:


    - **Memory-theoretic insight.** We formalize Mamba’s information bottlenecks through
the horizontal–vertical memory fidelity framework, offering a new perspective on longsequence degradation.

    - **Architectural innovation.** MemMamba introduces state summarization and cross-layer /
cross-token attention to simulate note-taking and bridge across-time and across-layer memory decay, without compromising efficiency.

    - **Empirical breakthroughs.** On language modeling, sparse retrieval, and cross-document
reasoning tasks, MemMamba consistently outperforms Mamba variants and Transformer
baselines, achieving 48% inference speedup, setting a new bar for memory retention in
efficient sequence models.


2 RELATED WORK


2.1 STATE SPACE MODELS


State space models (SSMs) have become strong candidates for long-sequence modeling due to their
linear-time complexity and recursive inference. Since S4 (Gu et al., 2021), SSMs have made con

2


Under review as a conference paper at ICLR 2026


tinuous progress in language, speech, and time-series modeling. Mamba (Gu & Dao, 2023) stands
out by leveraging a selective SSM mechanism that enhances expressiveness, achieving performance
comparable to or surpassing Transformers in language modeling, genomics, and reasoning tasks.


Building on Mamba’s success, follow-up works focus on three main directions: 1) Architectural
optimization: BiMamba (Liang et al., 2024) improves long-range dependency modeling with bidirectional state updates, and Vision Mamba (Zhu et al., 2024) adapts Mamba for vision tasks. 2)
Computational efficiency: FastMamba (Wang et al., 2025) improves training and inference speed
via parallelization and caching, enabling scalability to longer sequences. 3) Application extension:
Mamba has been applied to molecular dynamics (Hu et al., 2025), speech recognition (Zhang et al.,
2025), EEG signal understanding (Liu et al., 2025), image recognition (Lin et al., 2025; Lu et al.,
2025), event analysis (Lin et al., 2024), and long-text understanding, showcasing its cross-modal
generalization capabilities.


Nevertheless, most of these efforts primarily target architectural design or efficiency improvements,
which further highlights the central challenge of long-sequence modeling: _how to continuously_
_enhance a model’s ability to capture long dependencies while preserving computational efficiency_ ?


2.2 LONG-SEQUENCE MODELING


Long-sequence modeling is a critical issue in AI and cognitive science. Early models like LSTMs
and GRUs introduced gating for long-term dependencies, while NTMs and DNCs added external
memory. Memory Networks proposed slot-based storage, and Hopfield networks improved associative memory. Neuroscience-inspired models, such as spiking neural networks and HTM, have also
emerged.


The Transformer has become the standard for modeling long-range dependencies. The Compressive Transformer improves efficiency with compressed memory, though at the cost of information
loss (Rae et al., 2019). Megalodon supports million-token contexts but excels in extreme-length
tasks (Ma et al., 2024). Sparse-attention models like Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) reduce complexity, but struggle with ultra-long sequences.


These limitations have led to SSM-based approaches like Mamba. DeciMamba extends context
length 25× through dynamic pooling, boosting performance by 220% on TriviaQA (Ben-Kish et al.,
2025), but risks losing fine-grained information. mmMamba integrates multimodal distillation for
a 20.6× speedup (Li et al., 2024), but requires costly distillation data. DocMamba reduces memory
overhead by 88.3% for document processing (Hu et al., 2024), though gains are task-specific. LongMamba improves long-context extrapolation, but faces stability issues (Ye et al., 2025). Mamba-2
refines architecture and stability, outperforming the original Mamba, but still lags behind Transformers in tasks requiring strong copying or in-context learning (Gu & Dao, 2024).


These advances highlight Mamba’s potential in long-sequence tasks. However, most prior work
focuses on structural or context extension. The question of how models remember and forget critical
information remains largely unexplored. Our work focuses on memory patterns in Mamba to address
long-range forgetting and expand the design space for memory-augmented SSMs.


3 INVESTIGATION OF MEMORY PATTERNS


3.1 MEMORY DECAY IN MAMBA


The Mamba model builds on the mechanism of selective state space models (SSMs), achieving efficient sequence modeling through dynamic state compression, with explicit state update equations
forming its computational core. While Mamba has significant advantages in computational efficiency, it tends to suffer from memory decay when modeling long-range dependencies, and it is also
limited in capturing fine-grained local information.


The state update of Mamba is defined as:


_ht_ = _A · ht−_ 1 + _B · xt, yt_ = _C · ht,_ (1)


3


Under review as a conference paper at ICLR 2026


where _A ∈_ R _[d][s][×][d][s]_ is the state transition matrix satisfying _|A| <_ 1 to guarantee BIBO stability, _ht_
denotes the hidden state at step _t_, and _xt_ is the input at step _t_ .


To measure how the contribution of important information decays over long distances, we define the
notion of _information contribution_, _i.e._, the degree to which an input affects subsequent states of the
model. For an input _xt−k_ occurring _k_ steps earlier, its contribution to the current state _ht_ can be
expressed as:


Contribution( _xt−k →_ _ht_ ) = _|A_ _[k]_ _· B · xt−k| ≤|A_ _[k]_ _| · |B| · |xt−k|._ (2)


As _k_ increases ( _i.e._, as the input becomes further in the past), _A_ _[k]_ decays exponentially ( _e.g._, _A_ _[k]_ _≈_
_e_ _[−][αk]_ with _α >_ 0), causing early inputs to be almost completely forgotten.


3.2 MEMMAMBA NETWORK ARCHITECTURE


3.3 MEMORY DECAY IN TRANSFORMER


The Transformer model relies on the self-attention mechanism, where queries (Q), keys (K), and values (V) interact through Softmax normalization to perform weighted aggregation, thereby enabling
global dependency modeling. However, this mechanism incurs quadratic complexity with respect to
sequence length, which constitutes a major bottleneck for long-sequence processing. While Transformers retain long-range dependencies more effectively than Mamba, their high computational cost
also induces memory truncation effects in practice.


The time complexity of Transformer self-attention (TC) is:


TC = _O_ ( _L · n_ [2] _· d_ ) _,_ (3)


where _L_ is the number of layers, _n_ is the sequence length, and _d_ is the feature dimension.


For ultra-long sequences ( _e.g._, _n_ = 10 [5] ), the quadratic _n_ [2] term results in _∼_ 10 [10] operations, which
far exceeds the capacity of current hardware. In practice, approximations such as sliding-window
attention (with window size _w_ = 512) or sparse attention are employed, but these truncations inevitably discard information outside the window. We therefore define the notion of _effective mod-_
_eling length (EML)_, the maximum sequence length within which dependencies can be effectively
captured:
EML _≤_ _w ≪_ _n_ = _⇒_ inability to capture long-range dependencies _._ (4)

A detailed mathematical derivation of this truncation-induced information loss is provided in Appendix A.1. where we further elaborate the intermediate steps and theoretical implications.


3.4 HORIZONTAL AND VERTICAL MEMORY FIDELITY


Our theoretical derivations and preliminary experiments reveal that key information loss can be
decomposed into two complementary aspects: _horizontal_ information loss among tokens within a
layer, and _vertical_ information loss across layers (see Appendix A.1 for details). To capture both
dimensions of degradation, we propose the **Horizontal–Vertical Memory Fidelity Framework**,
which provides a principled lens for analyzing memory retention in long-sequence models.


**Definitions.** We define the _Expected Token Memory Fidelity (ETMF)_ as the degree to which semantic information of tokens is preserved during horizontal propagation across time steps, and the
_Expected Cross-Layer Memory Fidelity (ECLMF)_ as the degree to which information is preserved
during vertical transmission across layers. ETMF focuses on token-level semantic fidelity, while
ECLMF focuses on layer-wise propagation fidelity.


**Significance.** These two metrics provide complementary perspectives: ETMF reflects whether longrange token semantics remain faithful after recursive propagation, while ECLMF quantifies the
degradation of information across layers. Together, they highlight the dual challenges of _memory_
_decay_ and _extrapolation limits_ in Mamba, offering principled tools for evaluating and interpreting
memory behavior. Moreover, ETMF and ECLMF can guide architectural enhancements such as
cross-layer attention or redundant encoding strategies.


4


Under review as a conference paper at ICLR 2026


Figure 1: Overall workflow of MemMamba. The framework is composed of _n_ stacked MemMamba
Block Layers, where each layer preserves critical context via the Note Block and enables long-range
interaction through sparse cross-layer attention.


Figure 2: Workflow of a MemMamba Block Layer. Each block integrates three components: state
space model (SSM) updates, cross-token attention, and periodically triggered cross-layer attention.


The full mathematical definitions, derivations, and implementation details of ETMF and ECLMF
are provided in the Appendix. where we also include clarifying remarks and algorithmic procedures
to facilitate understanding and practical application.


4 METHOD


Existing state space models demonstrate superior linear complexity in long-sequence modeling, yet
their recursive update mechanisms lead to gradual decay of distant dependencies. This phenomenon
resembles the forgetting curve observed in cognitive science: when humans read long documents
without taking notes, early key information is often overwritten or lost. Inspired by this analogy,
we propose the _MemMamba Network_, which preserves critical context within limited representation space and provides indexing for long-range interactions across layers and tokens, ensuring that
essential signals are not diluted as sequences grow longer (Baevski et al., 2020).


5


Under review as a conference paper at ICLR 2026


MemMamba is composed of _n_ stacked MemMamba Block Layers. Each layer integrates three
components: state space model (SSM) updates, cross-token attention, and periodically triggered
cross-layer attention. To avoid redundant computation, the cross-token mechanism is executed at
every layer, whereas cross-layer attention is only activated every _p_ layers.

At layer _l_ and time step _t_, the input _x_ _[l]_ _t_ [first undergoes a threshold-based evaluation: if the token is]
likely to be forgotten, it is compressed and stored in the Note Block, which updates the state pool _St_ _[l]_ [.]
The state pool is then compared against the current SSM state. If forgetting is detected, cross-token
attention is performed between the state pool and the current input to restore forgotten information.
Specifically, a summary ˜ _st −_ 1 _[l]_ is retrieved from _St −_ 1 _[l]_ and fused with _x_ _[l]_ _t_ [through cross-token]
attention:
if _I_ token( _x_ _[l]_ _t_ [)] _[ > τ]_ [1] _[⇒]_ _[s][l]_ _t_ [=] _[ N][ l]_ [(] _[x]_ _t_ _[l]_ [)] _[, S]_ _t_ _[l]_ [=][ Insert][(] _[St][ −]_ [1] _[l][, s]_ _t_ _[l]_ [)] _[,]_ (5)

if _I_ state( _zt −_ 1 _[l]_ ) _> τ_ 2 _⇒_ _c_ [token] _t_ _[,l]_ = Attention( _Q_ = _x_ _[l]_ _t_ _[, K]_ [ = ˜] _[st][ −]_ [1] _[l][, V]_ [ = ˜] _[st][ −]_ [1] _[l]_ [)] _[.]_ (6)


Cross-layer attention is triggered every _p_ layers. When _l_ mod _p_ = 0, state pools from previous
layers are aggregated at corresponding token positions, and cross-layer attention is applied. For
each token in the current layer, summaries from the last _g_ layers are collected and aggregated into a
cross-layer context _s_ _[R]_ [(] _[l]_ [)] :


_c_ [layer] _t_ _[,l]_ = Attention( _Q_ = _x_ _[l]_ _t_ _[, K]_ [ =] _[ s][R]_ [(] _[l]_ [)] _[, V]_ [ =] _[ s][R]_ [(] _[l]_ [)][)] _[.]_ (7)


This dual-threshold and sparse cross-layer mechanism ensures that cross-token supplementation
occurs at every layer, while cross-layer memory interaction is sparsely activated, striking a balance
between memory retention and computational efficiency.


4.1 NOTE BLOCK


The Note Block dynamically identifies and extracts key information during sequence processing,
mimicking the human note-taking process while reading. It compresses and stores important tokens
into a state pool. For an input _x_ _[l]_ _t_ [, its importance is measured by the scoring function] _[ I]_ [token. If the]
score exceeds a threshold, the “Take note” operation is executed and the compressed summary is
inserted into the state pool; otherwise, the token is skipped:


_I_ token( _x_ _[l]_ _t_ [)] _[ > τ]_ [1][; ;] _[ ⇒]_ [; ;] _[ s]_ _t_ _[l]_ [=] _[ N][ l]_ [(] _[x]_ _t_ _[l]_ [)] _[,]_ (8)

where _N_ _[l]_ ( _·_ ) denotes a dimensionality reduction operator ( _e.g._, linear projection or pooling). The
summary _s_ _[l]_ _t_ [is then inserted into the state pool:]

_St_ _[l]_ [=][ Insert][(] _[S]_ _t_ _[l]_ _−_ 1 _[, s]_ _t_ _[l]_ [)] _[.]_ (9)


The state pool has limited capacity and adopts FIFO or priority-based replacement strategies, ensuring that only high-information summaries are retained.


4.2 MEMMAMBA BLOCK



The MemMamba Block is the core intra-layer computation unit, combining SSM updates, thresholdtriggered cross-token attention, and periodically triggered cross-layer attention. The update rules
are:



_c_ [token] _t_ _[,l]_ =



�Attention� _Q_ = _ht_ _[l][, K]_ [ = ˜] _[s]_ _t_ _[ l]_ _−_ 1 _[, V]_ [ = ˜] _[s]_ _t_ _[ l]_ _−_ 1� _,_ if _I_ state� _zt_ _[l]_ _−_ 1� _> τ_ 2 _,_



(10)
0 _,_ otherwise.



_c_ [layer] _t_ _[,l]_ =



�Attention� _Q_ = _xt_ _[l][, K]_ [ =] _[ s][R]_ [(] _[l]_ [)] _[, V]_ [ =] _[ s][R]_ [(] _[l]_ [)][�] _,_ if _l_ mod _p_ = 0 _,_

(11)
0 _,_ otherwise.



_x_ ¯ _t_ _[l]_ [+1] = _xt_ _[l]_ [+1] + _F_ tok _[l]_          - _c_ [token] _t_ _[,l]_          - + _F_ lay _[l]_          - _c_ [layer] _t_ _[,l]_          - _._ (12)

where _F_ tok _[l]_ and _F_ lay _[l]_ [are fusion functions (] _[e.g.]_ [, gating or residual mapping). The fused result is]
then passed into the SSM update as input.


6


Under review as a conference paper at ICLR 2026


Table 1: Perplexity (PPL) comparison across models under different context lengths. Best results
are highlighted in red, second-best in blue. Lower PPL is better. Results _>_ 100 are denoted as INF.
All experiments are conducted under the same hardware and software environment. PPL variance
across multiple runs is within _±_ 0 _._ 7.


Model Parm 1K 2K 4K 10K 20K 30K 40K 50K 60K


Mamba 130M 21.00 19.60 18.77 19.29 31.63 INF INF INF INF
DeciMamba (Ben-Kish et al., 2025) 150M 21.90 20.06 18.55 21.98 23.15 27.05 40.48 INF INF
Com-Transformer (Rae et al., 2019) 400M 33.09 NA NA NA NA NA NA NA NA
Megalodon (Ma et al., 2024) 200M 66.14 66.55 66.43 65.02 64.81 64.3 64.21 64.02 63.92
**MemMamba** **200M 19.35 18.23 17.52 17.71 18.25 17.33 17.54 17.97 17.35**


Table 2: Passkey retrieval accuracy across different context lengths.At 400k tokens, due to GPU
memory limits, only a subset of samples are evaluated. Longer sequences require larger memory
(evaluated on 20-core 80GB H800).


Model 1K 2K 4K 8K 16K 32K 64K 128K 256K 400K


Pythia-160M (Ben-Kish et al., 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
2025)

Mamba-130M (Ben-Kish et al., 1.0 1.0 1.0 1.0 0.8 0.0 0.0 0.0 0.0 0.0
2025)

DeciMamba-130M (Ben-Kish 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.6
et al., 2025)

**MemMamba** **1.0** **1.0** **1.0** **1.0** **1.0** **1.0** **1.0** **1.0** **1.0** **0.9**


5 EXPERIMENTAL ANALYSIS


**Datasets** . We evaluate the proposed method on three long-sequence benchmarks. The main experiment is conducted on the PG19-PPL dataset (around 100M tokens), which consists of English novels
from Project Gutenberg published around 1919. The average length is 69k tokens, and the task is
language modeling, evaluated by perplexity (PPL) to measure semantic consistency and narrative
coherence.


Beyond language modeling, we further evaluate on two synthetic retrieval benchmarks to characterize long-range memory and retrieval ability at a finer granularity. In the Passkey Retrieval task, a
target token is randomly inserted into an extremely long input sequence, and the model is required
to precisely retrieve this information at prediction time. Since the location of key information is uncertain, this task particularly tests whether the model can maintain long-term memory under sparse
cues. The Document Retrieval task covers multi-domain documents and supports both simple and
detailed retrieval modes, providing a comprehensive evaluation of memory and reasoning across
documents and domains.


**Settings** . All experiments are implemented in PyTorch 2.1.2 and Python 3.10 on Ubuntu 22.04
with CUDA 11.8. Training is performed on a single NVIDIA RTX 4090 GPU (24GB) and 25
vCPUs (Intel Xeon Platinum 8481C). Our MemMamba is a 24-layer SSM-based model. Each state
summary vector is compressed to 64 dimensions, and the state pool size is fixed at 50. The training
sequence length is 8,000 tokens, and the model is trained for 100k steps using the AdamW optimizer
(learning rate = 1e-4, weight decay = 0.1). We adopt constant learning rate scheduling, gradient
accumulation (4 steps), and gradient clipping (max norm = 1). The random seed is fixed to 123 to
ensure reproducibility. (Detailed model and hardware configurations are provided in Appendix A.6.)


7


Under review as a conference paper at ICLR 2026


Table 3: Performance of different models under varying numbers of noisy documents.Higher scores
indicate better performance. Best results at each noise level are highlighted in red.


Model 10 20 120 160 200


Mamba (Ben-Kish et al., 2025) 0.68 0.71 0.01 0 0
DeciMamba (Ben-Kish et al., 2025) 0.72 **0.74** 0.48 0.19 0.12
**MemMamba** **0.8** 0.66 **0.52** **0.44** **0.24**


5.1 COMPARISON WITH BASELINES


**Language Modeling.** We compare MemMamba and its mechanisms against several state-of-theart long-sequence models on PG19: DeciMamba (an efficient Mamba variant optimized for reduced
computation), Megalodon (enhanced sequence representations tailored for extremely long contexts),
Compressive Transformer (memory compression for efficiency), and Pythia (a modular LLM with
high flexibility). Results are reported in Table 1.


MemMamba outperforms all baselines at most context lengths. Notably, in ultra-long sequences of
30k–60k tokens, although performance degrades for all models, MemMamba shows much stronger
robustness and stability. This indicates that state summarization and cross-layer attention effectively
mitigate Mamba’s memory decay in long-range dependencies.


We further analyze model performance under varying parameter scales (see Figure 5). We find
that MemMamba achieves performance comparable to 1–2B parameter models even at very small
parameter scales.


**Passkey Retrieval.** MemMamba maintains high retrieval accuracy even with input lengths of several hundred thousand tokens. When the target token is placed more than 200k tokens away from
the prediction point, MemMamba still retrieves the key information accurately, whereas Mamba and
Pythia completely fail at such lengths. Compared to DeciMamba, MemMamba achieves higher accuracy on extremely long sequences (400k tokens), demonstrating more robust long-range memory
retention. **Document Retrieval.** On the document retrieval benchmark, MemMamba achieves leading performance under both simple and detailed retrieval settings. As the number of noisy documents
increases, Mamba’s performance drops sharply, while DeciMamba shows partial improvement but
remains unstable. In contrast, MemMamba consistently maintains higher scores under high-noise
conditions, highlighting its advantage in cross-document and cross-domain reasoning tasks.


Overall, MemMamba achieves consistent improvements over state-of-the-art baselines across language modeling, sparse retrieval, and cross-document reasoning. Compared to Transformer-based
models, it shows stronger scalability on ultra-long sequences; compared to Mamba variants, it significantly enhances long-term memory retention while preserving linear complexity. We attribute its
advantages to effective compression of key information via state summarization, and alleviation of
deep-layer memory decay through cross-layer attention.


5.2 ABLATION STUDIES


**Core mechanisms.** Under identical parameter budgets and training settings, MemMamba maintains
low and stable PPL across contexts: at 1.5k tokens its PPL is 19.35, and as the context grows to 60k
tokens the PPL fluctuates only within 17.33–18.25. Removing both the state summarization and the
cross-layer / cross-token attention causes a stark contrast.


**Efficiency.** On the same hardware in a single-process setting, we benchmark the inference speed for
MemMamba, DeciMamba, and a Transformer across sequence lengths [1000 _,_ 2000 _,_ 4000 _,_ 10000 _,_
20000 _,_ 30000 _,_ 40000 _,_ 50000 _,_ 60000], using 100 samples per length. Despite the extra computations
introduced to enhance modeling capacity, MemMamba’s end-to-end latency is only **0** _._ **52** _×_ that of
the Transformer ( _i.e._, a **48** % speedup). Unlike conventional models whose recursive updates can
degrade efficiency, MemMamba leverages compact representations and cross-layer / cross-token at

8


Under review as a conference paper at ICLR 2026


Figure 3: Ablation results of the core mechanisms. The same hardware conditions and training
configurations are used.


Figure 4: Comparison of ETMF and ECLMF across different Mamba variants


tention to optimize information flow, thereby sustaining high computational efficiency on ultra-long
sequences. In addition, the sparse skip mechanism further reduces redundant computation, ensuring
stable linear complexity _O_ ( _n_ + _m_ ) (with _n_ the sequence length and _m_ the number of retrieved summaries/attention interactions). Together, these design choices underpin MemMamba’s efficiency
advantage on long-sequence tasks, delivering significantly faster inference while preserving modeling strength relative to baseline models.


In addition, we evaluate the efficiency of our model. Under the same hardware conditions, MemMamba achieves approximately a 50% improvement in efficiency compared with the Transformer.


We further test the ETMF and ECLMF scores of the original Mamba, DeciMamba, and MemMamba. For both metrics, higher scores indicate better retention of early token information.
The results show that MemMamba significantly outperforms both the original Mamba and DeciMamba.Although DeciMamba shows a slight advantage in extremely long-range cross-layer transmission, its instability poses a substantial drawback.


5.3 PROOF OF LINEAR COMPLEXITY


Despite the introduction of state summarization and cross-layer attention, MemMamba still preserves linear complexity in both time and space. Specifically, its computational cost scales with
sequence length _n_ and hidden dimension _d_ as _O_ ( _n · d_ ), in contrast to _O_ ( _n_ [2] _d_ ) for Transformers.
This is achieved by constraining the state dimension _ds_ and the attention pool size _k_ to be constants.
Detailed derivations and proofs are provided in Appendix A.4.


6 CONCLUSION


We introduce **MemMamba**, a memory-centric extension to state space models that bridges the
long-standing gap between scalability and long-range dependency modeling. By augmenting the
Mamba architecture with dynamic state summarization and lightweight cross-layer and cross-token
attention, MemMamba offers a principled solution to the memory decay problem that limits ex

9


Under review as a conference paper at ICLR 2026


isting SSMs. Our information-theoretic framework formalizes this degradation and motivates a
new architectural direction: integrating structured memory without sacrificing efficiency. Empirically, MemMamba achieves state-of-the-art results on a wide range of long-sequence benchmarks
including PG19, Passkey Retrieval, and multi-document reasoning. Meanwhile, complexity analysis
confirms that it retains linear time and space scaling, delivering a 48% speedup over baseline architectures. More broadly, MemMamba represents a step toward a new generation of memory-centric
neural architectures that treat retention and reasoning as first-class citizens. Future work will explore extensions to multimodal settings, integration with retrieval-augmented systems, and scaling
MemMamba as a foundation for efficient, high-fidelity memory across complex real-world tasks.


REFERENCES


Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In _Proceedings of Advances in Neural_
_Information Processing Systems 33 (NeurIPS 2020)_, volume 33, pp. 12449–12460, 2020.


Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_
_(EMNLP)_, pp. 7059–7071, 2020.


Amnon Ben-Kish, Itay Zimerman, Salman Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf,
and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. In _Pro-_
_ceedings of the International Conference on Learning Representations (ICLR)_, 2025.


Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In _Advances in Neural Information_
_Processing Systems_, 2020.


Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In
_Proceedings of the 1st Conference on Language Modeling (COLM)_, 2023.


Albert Gu and Tri Dao. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality. In _Proceedings of the International Conference on Machine Learn-_
_ing_, 2024.


Albert Gu, Karan Goel, and Christopher R´e. Efficiently modeling long sequences with structured
state spaces. _arXiv preprint arXiv:2111.00396_, 2021.


Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):
1735–1780, 1997. doi: 10.1162/neco.1997.9.8.1735.


Jingjing Hu, Dan Guo, Zhan Si, Deguang Liu, Yunfeng Diao, Jing Zhang, Jinxing Zhou, and Meng
Wang. Mol-mamba: Enhancing molecular representation with structural & electronic insights. In
_Proceedings of the AAAI Conference on Artificial Intelligence_, volume 39, pp. 317–325, 2025.


Pengfei Hu, Zhenrong Zhang, Jiefeng Ma, Shuhang Liu, Jun Du, and Jian Shu Zhang. Docmamba:
A state-space model for document processing. In _Proceedings of the AAAI Conference on Arti-_
_ficial Intelligence_, volume 39, pp. 24095–24103, 2024. doi: 10.1609/AAAI.v39i22.34584. Submitted to ACL/NeurIPS 2024.


Zongshu Li, Guibo Zhu, Dongyi Yi, and Jinqiao Wang. Multimodal mamba: A versatile multimodal
model for seamless integration into diverse downstream tasks. In _Proceedings of the 13th Inter-_
_national Conference on Computing and Pattern Recognition (ICCPR ’24)_, pp. 303–313, 2024.
doi: 10.1145/3704323.3704364.


Aobo Liang, Xingguo Jiang, Yan Sun, Xiaohou Shi, and Ke Li. Bi-mamba+: Bidirectional mamba
for time series forecasting. _arXiv preprint arXiv:2404.15772_, 2024.


10


Under review as a conference paper at ICLR 2026


Yuhui Lin, Jiahao Zhang, Siyuan Li, Jimin Xiao, Ding Xu, Wenjun Wu, and Jiaxuan Lu. Event uskt:
U-state space model in knowledge transfer for event cameras. _arXiv preprint arXiv:2411.15276_,
2024.


Yuhui Lin, Jiaxuan Lu, Yue Yong, and Jiahao Zhang. Mv-gmn: State space model for multi-view
action recognition. _arXiv preprint arXiv:2501.13829_, 2025.


Hanwen Liu, Yifeng Gong, Zuwei Yan, Zeheng Zhuang, and Jiaxuan Lu. Msgm: A multi-scale
spatiotemporal graph mamba for eeg emotion recognition. _arXiv preprint arXiv:2507.15914_,
2025.


Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, and Xiaosong Wang. Hypergraph mamba for efficient whole slide image understanding. _arXiv preprint arXiv:2505.17457_,
2025.


Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May,
Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and
inference with unlimited context length. In _Proceedings of the Neural Information Processing_
_Systems (NeurIPS)_, 2024.


Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In _Proceedings of the 30th International Conference on Machine Learning_, pp. 2347–
2355, 2013.


Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. _arXiv preprint arXiv:1911.05507_, 2019.


Claude E. Shannon. A mathematical theory of communication. _The Bell System Technical Journal_,
27(3):379–423, 1948.


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Infor-_
_mation Processing Systems_, 2017.


Raphael Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao,
Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika
Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An empirical study
of mamba-based language models. _arXiv preprint arXiv:2406.07887_, 2024.


Aotao Wang, Haikuo Shao, Shaobo Ma, and Zhongfeng Wang. Fastmamba: A high-speed and
efficient mamba accelerator on fpga with accurate quantization. _arXiv preprint arXiv:2505.18975_,
2025.


Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Pavlo
Molchanov, Jan Kautz, and Yingyan Celine Lin. Longmamba: Enhancing mamba’s long context
capabilities via training-free receptive field enlargement. In _Proceedings of the International_
_Conference on Learning Representations (ICLR 2025)_, 2025.


Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers
for longer sequences. In _Advances in Neural Information Processing Systems_, volume 33, pp.
17283–17297, 2020.


Xiangyu Zhang, Qiquan Zhang, Hexin Liu, Tianyi Xiao, Xinyuan Qian, Beena Ahmed, Eliathamby
Ambikairajah, Haizhou Li, and Julien Epps. Mamba in speech: Towards an alternative to selfattention. _IEEE Transactions on Audio, Speech and Language Processing_, 2025.


Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. _arXiv_
_preprint arXiv:2401.09417_, 2024.


11


Under review as a conference paper at ICLR 2026


A APPENDIX


A.1 ON THE FORGETTING OF CRITICAL INFORMATION


The core of critical-information loss lies in the _irreversible_ loss introduced by compression/approximation (Shannon, 1948). For a generic model, by the information-theoretic compression limit (Shannon’s theorem), the per-layer entropy loss without cross-layer sharing satisfies


∆ _H ≥_ _H_ ( _sl_ ) _−_ _H_ ( _ht_ ) _._ (13)


Accumulated loss leads to the disappearance of key information. Importantly, Transformers and
Mamba exhibit _fundamentally different_ loss mechanisms:


A.1.1 CRITICAL-INFORMATION LOSS IN TRANSFORMERS


Transformers often rely on low-rank approximations ( _e.g._, Nystr¨om) or sparse attention to compress
information. If critical features do not lie within the projection subspace, the loss is irreversible,
which manifests as an unbounded reconstruction error:


_|x −_ _x_ ˆ _|_ 2 _→∞_ if _x ⊥_ projection subspace _,_ (14)


where _x_ denotes the original critical signal and ˆ _x_ its reconstruction; the “projection subspace” is the
low-rank subspace used by the approximation.


A.1.2 CRITICAL-INFORMATION LOSS IN MAMBA


In Mamba, state compression makes it easy to forget long-range critical information both _within_ and
_across_ layers:


1. **Intra-layer dependence.** Each layer’s state _h_ [(] _t_ _[l]_ [)] depends only on the previous layer’s
_h_ [(] _t_ _[l][−]_ [1)] . After _L_ rounds of state decay, early critical information essentially vanishes:



_h_ [(] _t_ _[L]_ [)] _≈_



_L_

- _A_ [(] _[l]_ [)] _[,τ]_ _· h_ [(1)] _t_ 0 ( _L ≥_ 10) _,_ (15)

_l_ =1



where _h_ [(] _t_ _[L]_ [)] is the hidden state of layer _L_ at step _t_, _h_ [(1)] _t_ 0 [denotes the early signal at layer 1]
and time _t_ 0, _τ_ = _t −_ _t_ 0 is the temporal gap, and _A_ [(] _[l]_ [)] _[,τ]_ is the _τ_ -step transition operator at
layer _l_ .


1. **Inter-layer dependence.** Without effective cross-layer coupling or cross-layer attention,
early-layer critical information is almost impossible to retain in deep layers.


A.1.3 THEORY OF CROSS-LAYER TRANSMISSION OF CRITICAL INFORMATION


Let _h_ [(] _t_ _[l]_ [)] _∈_ R _[d]_ denote the hidden state of layer _l_ at time _t_ . A linearized approximation of the intralayer recursion (keeping the dominant linear operators and treating nonlinearities as residual _ε_ ) is


_h_ [(] _t_ _[l]_ [)] = _A_ [(] _[l]_ [)] _h_ [(] _t−_ _[l]_ [)] 1 [+] _[ U]_ [ (] _[l]_ [)] _[z]_ _t_ [(] _[l][−]_ [1)] + _ε_ [(] _t_ _[l]_ [)] _[,]_ (16)


where _A_ [(] _[l]_ [)] is the temporal recursion operator (state update/compression matrix), _zt_ [(] _[l][−]_ [1)] is the input
from the previous layer, and _ε_ [(] _t_ _[l]_ [)] is a residual term.


In the “no cross-layer interaction” configuration of vanilla Mamba, the coupling term is negligible
( _U_ [(] _[l]_ [)] _≈_ 0). Fixing the time span _τ_ = _t −_ _t_ 0 and tracing the influence of an early input _xt_ 0 on a later
state _h_ [(] _t_ _[L]_ [)], we expand over time and depth (ignoring intermediate-input contributions):



_h_ [(] _t_ _[l]_ [)] _≈_ ( _A_ [(] _[l]_ [)] ) _[τ]_ _h_ [(] _t_ 0 _[l]_ [)] [+]



_τ_
�( _A_ [(] _[l]_ [)] ) _[τ]_ _[−][s]_ [�] _U_ [(] _[l]_ [)] _zt_ [(] 0 _[l][−]_ + [1)] _s_ [+] _[ ε]_ _t_ [(] 0 _[l]_ [)] + _s_ - _._ (17)

_s_ =1


12


Under review as a conference paper at ICLR 2026


When _U_ [(] _[l]_ [)] _≈_ 0, the contribution from early layers is only preserved by ( _A_ [(] _[l]_ [)] ) _[τ]_ _h_ [(] _t_ 0 _[l]_ [)][. Stacking to]
depth _L_, the contribution of _xt_ 0 is approximated as


                    _h_ [(] _t_ _[L]_ [)] _≈_ ( _A_ [(] _[L]_ [)] ) _[τ]_ [�] ( _A_ [(] _[L][−]_ [1)] ) _[τ]_ _· · ·_ ( _A_ [(1)] ) _[τ]_ _h_ [(1)] _t_ 0 _[· · ·]_ + (other inputs/residuals) _._ (18)


By submultiplicativity of matrix norms (let _|A|_ := max1 _≤l≤L |A_ [(] _[l]_ [)] _|_ ), the early-information contribution to _h_ [(] _t_ _[L]_ [)] is upper bounded by
���contrib( _xt_ 0! _→_ ! _h_ ( _tL_ ))��� _≤|A|Lτ · |h_ (1) _t_ 0 _[|][.]_ (19)


Hence, if _|A| <_ 1, information decays exponentially in both _τ_ and _L_ ; even if _|A|_ ! _≈_ !1 but each _A_ [(] _[l]_ [)]
is low-rank/projective, components orthogonal to the projection subspace are irretrievably discarded.


A.2 DETAILED DERIVATIONS FOR HORIZONTAL/VERTICAL MEMORY FIDELITY


A.2.1 EXPECTED TOKEN MEMORY FIDELITY (ETMF)


Focusing on information transmission across tokens within a layer, starting from the (single-layer)
Mamba update
_h_ [(] _t_ _[l]_ [)] = _A_ [(] _[l]_ [)] _h_ [(] _t−_ _[l]_ [)] 1 [+] _[ U]_ [ (] _[l]_ [)] _[x]_ _t_ [(] _[l]_ [)] + _ε_ [(] _t_ _[l]_ [)] _[,]_ (20)
we quantify token-to-token transmission via the expected cosine similarity:


ETMF := E _i,j_              - cos( _ti,_ _t_ [ˆ] _j_ )� _,_ (21)


where _ti_ is the original token representation at time _i_ ( _e.g._, embedding _E_ [ _xi_ ] or the first-layer state
_h_ [(1)] _i_ [), and][ ˆ] _[t][j]_ [ is the reconstructed/predicted representation at time] _[ j]_ [. Low ETMF indicates severe se-]
mantic distortion of long-range tokens due to exponential decay in the absence of explicit attention.


To reduce cost in practice, we adopt a _self-reconstruction_ approximation that computes cosine
similarity at the same position ( _i_ = _j_ ). Concretely: take _tj_ = _E_ [ _xj_ ] (with _E_ possibly tied
to the output head). From the last-layer state _h_ [(] _j_ _[L]_ [)], compute logits logits _j_ = _h_ [(] _j_ _[L]_ [)] _W_ out _[⊤]_,
_pj_ = softmax(logits _j/τ_ ) (temperature _τ_ = 1), and reconstruct _t_ [ˆ] _j_ = [�] _v_ _[p][j]_ [(] _[v]_ [)] _[E]_ [[] _[v]_ []][. The co-]

sine cos( _tj,_ _tj_ [ˆ] ) averaged over sequence/batch yields ETMF. To capture distance-sensitive effects,
one may compute ETMF∆= E _i_ [cos( _ti,_ _ti_ [ˆ] + ∆)] for ∆ _∈_ 8 _,_ 16 _,_ 32; due to higher cost, we include
this as a supplementary analysis (see Sec. 5). While self-reconstruction is effective for single-point
fidelity, it can underestimate cross-distance loss; future work can tune _τ_ or sample multiple ∆ to
improve fidelity estimation.


A.2.2 EXPECTED CROSS-LAYER MEMORY FIDELITY (ECLMF)


For cross-layer transmission, using the multi-layer recursion

_h_ [(] _t_ _[l]_ [)] = _A_ [(] _[l]_ [)] _h_ [(] _t−_ _[l]_ [)] 1 [+] _[ U]_ [ (] _[l]_ [)] _[z]_ _t_ [(] _[l][−]_ [1)] + _ε_ [(] _t_ _[l]_ [)] _[,]_ (22)

with _zt_ [(] _[l][−]_ [1)] the previous-layer output and _U_ [(] _[l]_ [)] the inter-layer projection, if _U_ [(] _[l]_ [)] ! _≈_ !0 and _|A_ [(] _[l]_ [)] _|_ ! _<_ !1,
the dependence on early layers decays exponentially with depth:

_h_ [(] _t_ _[L]_ [)] _≈_         -         - _[L]_ ( _A_ [(] _[i]_ [)] ) _[τ]_ [�] _h_ [(1)] _t_ 0 _[,]_ _|h_ [(] _t_ _[L]_ [)] _| ≤|A|_ _[Lτ]_ _, |h_ [(1)] _t_ 0 _[|][.]_ (23)

_i_ =1



We therefore define




           - _h_ [(] _[l]_ [)] ; _h_ [(] _[l]_ [+] _[G]_ [)][�]
ECLMF _l →_ _l_ + _G_ := _[I]_ [!] _,_ (24)

_H_ ! ~~�~~ _h_ [(] _[l]_ [)] ~~[�]~~



where _I_ ( _·_ ; _·_ ) is mutual information and _H_ ( _·_ ) is entropy. Because estimating high-dimensional MI is
expensive, we use a reconstruction surrogate:

ECLMF� _l →_ _l_ + _G_ = 1 _−_ _[|][h]_ [(] _[l]_ [+] _[G]_ [)] _[ −D]_ [(] _[h]_ [(] _[l]_ [)][)] _[|][F]_ _,_ (25)

_|h_ [(] _[l]_ [)] _|F_ + _ϵ_


13


Under review as a conference paper at ICLR 2026


with a lightweight decoder _D_ (ridge regression by default) and _ϵ_ = 10 _[−]_ [6] . In practice: choose gap
_G_ ! _∈_ !2 _,_ 5 _,_ 10; collect _Hl, Hl_ + _G_ ! _∈_ !R _[B][×][T][ ×][D]_ ; mask paddings and flatten to _X, Y_ ! _∈_ !R _[N]_ _[×][D]_ ; fit
_W_ = ( _X_ _[⊤]_ _X_ + _λI_ ) _[−]_ [1] _X_ _[⊤]_ _Y_ with _λ_ = 10 _[−]_ [4] ; compute _Y_ [ˆ] = _XW_ and _r_ = _|Y −_ _Y_ [ˆ] _|F /_ ( _|X|F_ + _ϵ_ );
the score is 1 _−_ _r_, averaged over _l_ and samples to yield ECLMF _G_ . Linear decoding assumes primarily linear cross-layer mappings with noise and correlates empirically with information preservation
( _e.g._, canonical correlations). One can replace it with small MLP decoders or Gaussian MI estimators (see Sec. A) to validate the surrogate; we adopt the linear version for efficiency.


ETMF and ECLMF are complementary: ETMF gauges token-level (horizontal) semantic fidelity,
whereas ECLMF measures layer-wise (vertical) memory integrity. Together they constitute our
“horizontal–vertical memory framework,” explaining Mamba’s memory decay and extrapolation
limits and offering actionable metrics for introducing cross-layer attention and redundancy. Empirical results (Fig. 5) confirm that state summarization and cross-layer attention in MemMamba
markedly improve both ETMF and ECLMF, mitigating long-range forgetting.


A.3 THEORETICAL DERIVATIONS FOR THE MODEL DESIGN


MemMamba breaks the “complexity–memory” trade-off by combining _bounded-error_ state summarization with _linear-complexity_ low-rank cross-layer attention. Theoretically, we show linear
time/space complexity _O_ ( _n_ ), long-range critical-information recall _≥_ 90%, BIBO stability, and
non-vanishing gradients; under equal budgets it outperforms Transformers and SOTA Mamba variants (DeciMamba/LongMamba). Empirical results match these derivations, supporting ultra-long
sequence modeling both theoretically and practically.


A.3.1 ERROR BOUND OF POOLING APPROXIMATION


MemMamba’s state summarization and cross-layer attention entail Frobenius-norm–optimal estimations. For sequence length _n_ and window size _w_ ( _m_ = _n/w_ ), define a state matrix **H** ! _∈_ !R _[n][×][d]_,
partitioned into blocks **H** _i_, and compute summaries by max pooling:


**s** [ _i,_ :] = max 1 _≤_ _j ≤_ _w_ **H** _i_ [ _j,_ :] _._ (26)


Reconstruction **H** _[′]_ ! _∈_ !R _[n][×][d]_ is obtained by broadcasting within each window:


**H** _[′]_ [( _i −_ 1) _w_ + 1 : _iw,_ :] = **s** [ _i,_ :] _·_ **1** _[⊤]_ _w_ _[,]_ (27)


with **1** _w_ the all-ones vector. The squared Frobenius error is

_|_ **H** _−_ **H** _[′]_ _|F_ [2] =                   - _i_ = 1 _[m]_ [��] **H** _i −_ **s** [ _i,_ :] _·_ **1** _[⊤]_ _w_ ��2 _F_ _[.]_ (28)


Since _s_ [ _i,_ :] is the columnwise maximum of **H** _i_, let ∆= max _i, j, k_ ( _s_ [ _i, k_ ] _−_ **H** _i_ [ _j, k_ ]) (local fluctuation upper bound, often small for text); then
�� **H** _i −_ **s** [ _i,_ :] _·_ **1** _⊤w_ ��2 _F_ _[≤]_ _[w][ ·][ d][ ·]_ [ ∆][2] _[,]_ (29)



and hence _√_
_|_ **H** _−_ **H** _[′]_ _|F ≤_



_√_
_mwd,_ ∆=



_|_ **H** _−_ **H** _|F ≤_ _mwd,_ ∆= _nd,_ ∆ _._ (30)

Although this scales with _[√]_ ~~_n_~~ ~~,~~ ∆ is typically very small; the reconstruction error is bounded and
controllable, while maxima (key signals) are preserved by design.



A.3.2 EQUAL-BUDGET COMPARISON: MEMMAMBA VS. TRANSFORMER/MAMBA


We compare under compute budget (total FLOPs) and memory budget _M_ .


**(1) Equal compute** ( _C_ MemMamba = _C_ Transformer = _C_ ). Transformer complexity:







_C_
_._ (31)
_LT dT_



_C_ = _O_ ( _LT n_ [2] _T_ _[d][T]_ [);] _[ ⇒]_ [;] _[ n][T]_ _[≈]_



MemMamba complexity:



_C_
_C_ = _O_ ( _Lonodo_ ); _⇒_ ; _no ≈_ _._ (32)
_Lodo_


14


Under review as a conference paper at ICLR 2026


Figure 5: Comparison of perplexity (PPL) across models at different context lengths.


Thus, for the same _C_, _no_ can be orders of magnitude larger than _nT_ ( _e.g._, _C_ =10 [12] yields _nT ∼_ 10 [4]
vs. _no∼_ 10 [6], _i.e._, _∼_ 100 _×_ longer contexts for MemMamba).


**(2) Equal memory** ( _M_ MemMamba = _M_ Transformer = _M_ ). Mamba / MemMamba memory scales
linearly:
_M_ Mamba = _O_ ( _Lmnmdm_ ) _,_ _M_ MemMamba = _O_ ( _Lonodo_ ) _,_ (33)
hence _no_ ! _≈_ ! _nm_ at fixed _M_, but MemMamba’s long-range recall is much higher ( _e.g._,
RecallMemMamba! _≥_ 1 _−_ _δ_, _δ<_ 0 _._ 05) than Mamba’s (which decays with _|A|_ ). Therefore, under equal
memory, MemMamba achieves higher accuracy.


A.3.3 BIBO STABILITY OF MEMMAMBA


Consider the update
**h** [(] _[t]_ [+1)] = **Ah** [(] _[t]_ [)] + **B**          - **x** [(] _[t]_ [+1)] + _α_ **c** [(] _t_ _[t]_ [+1)]          - _,_ (34)

where **c** _t_ [(] _[t]_ [+1)] = [�] _i_ = 1 _[l]_ _αi_ **W** _vsi_ with bounded _si_ ( _|si| ≤_ _S_ ), and inputs **x** [(] _[t]_ [)] are bounded. Then

_|_ **h** [(] _[t]_ [+1)] _| ≤|_ **A** _|, |_ **h** [(] _[t]_ [)] _|_ + _|_ **B** _|, |_ **x** + _αkS|._ (35)


If _|_ **A** _| <_ 1, as _t →∞_ we get

_|_ **h** [(] _[t]_ [+1)] _| ≤_ _[|]_ **[B]** _[|][,][ |]_ **[x]** [ +] _[ α]_ **[c]** _[s][|]_ _< ∞,_ (36)

1 _−|_ **A** _|_


establishing BIBO stability (no divergence or pathological decay).


A.3.4 CONVERGENCE OF GRADIENT PROPAGATION


For the fusion _x_ _[′]_ = _x_ + _αc_, the gradients are


_∇xL_ = _∇x′L,_ ( **I** + _α∇x_ **c** ) _,_ _∇sL_ = _∇x′L, α, ∇s_ **c** _._ (37)


Since _∇s_ **c** is bounded (Softmax derivative _≤_ 1), we have _|∇x′L|_ ! _≥_ ! _α|∇sL|_ ! _>_ !0, avoiding the
_∝|A|_ _[L]_ vanishing-gradient issue in vanilla Mamba and ensuring optimization convergence.


A.3.5 LONG-SEQUENCE RECALL: VANILLA MAMBA VS. MEMMAMBA


Let the key feature _f_ from _k_ steps in the past have strength _|f_ _|_ = _γ_ . **Vanilla Mamba:** the contribution to the current state is _≤|A|_ _[k]_ _|B|γ_, yielding a recall

RecallMamba _≤_ _[|]_ **[A]** _[|][k][∗][|]_ **[B]** _[|][γ]_ _,_ (38)

_θ_


15


Under review as a conference paper at ICLR 2026


with detection threshold _θ_ . For _k_ =100, RecallMamba _<_ 0 _._ 01, indicating severe memory decay.


**MemMamba:** the summarized state _si_ retains _⟨si, f_ _⟩≥_ _γ −_ ∆, cross-layer weights _αi ≥_ _ρ_ (correlation _ρ_ ! _≥_ !0 _._ 5), and the fused signal satisfies _|x_ _[′]_ ! _∩_ ! _f_ _| ≥_ _α_ ( _γ −_ ∆). Thus


RecallCSA _≥_ _[α]_ [(] _[γ][ −]_ [∆)] _≥_ 0 _._ 9 _,_ (39)

_θ_


_e.g._, with _α_ =0 _._ 8, ∆=0 _._ 1 _γ_, _θ_ =0 _._ 7 _γ_ . Hence RecallMemMamba! _≥_ !0 _._ 9, substantially exceeding
Mamba and Transformer.


A.4 THE “COMPLEXITY–MEMORY” TRADE-OFF IN SEQUENCE MODELING


A.4.1 LINEAR TIME COMPLEXITY


MemMamba combines state summarization with attention for long-sequence modeling. For length
_n_ and dimension _d_, let _H_ = [ **h** 1 _, . . .,_ **h** _n_ ] and define the summary by max pooling


**s** [ _j_ ] = max( **H** ) _,_ (40)


with cost _O_ ( _nd_ ). The Mamba block updates


**h** _t_ = **Ah** _t −_ 1 + **Bx** _t,_ (41)



with cost _O_ ( _nd_ ) assuming constant state dimension _ds_ ( _e.g._, 32). Cross-layer attention interacts
with the state pool via



_√_
score = softmax( **QK** _[⊤]_ _/_



_d_ ) _,_ (42)



where **Q** = **W** _q_ **x**, **K** = **W** _s_ **s**, **V** = **W** _v_ **x**, costing _O_ ( _nd_ ) with constant pool size _k_ ( _e.g._, 50).
Fusion then gives
**x** _[′]_ _t_ [=] **[ x]** _[t]_ [+] _[ α][ ·]_ [ score] _[ ·]_ **[ V]** _[,]_ (43)


still _O_ ( _nd_ ). Summing across _L_ layers yields _O_ ( _Lnd_ ); treating _L, d_ as constants, the complexity is
linear in _n_ .


Further, since **QK** _[⊤]_ has rank _≤_ _k ≪_ _n_, it is a low-rank matrix. By Nystr¨om theory, the approximation error obeys

�� **QK** _⊤_ _−_ **QK**                                           - _[⊤]_ [��] 2 _≤_ _σk_ + 1 _._ (44)


Because rank( **QK** _[⊤]_ )! _≤_ ! _k_, we have _σk_ +1 = 0, _i.e._, zero approximation error in the idealized
setting, while the computation _O_ ( _nkd_ ) is far smaller than Transformer’s _O_ ( _n_ [2] _d_ ).


A.4.2 LINEAR SPACE COMPLEXITY


We analyze memory usage asymptotically. Let the sequence length be _n_ and the model width be _C_ .


    - **Transformer:** memory grows as _O_ ( _n_ [2] ) with sequence length ( _e.g._, attention maps), creating a severe bottleneck.


    - **Mamba:** memory is linear, _O_ ( _nC_ ).


    - **MemMamba:** thanks to state summarization, active memory scales effectively as _O_ ( _nC_ )
with small constants; with a constant-size pool, the dominant terms remain linear.


Overall space is dominated by inputs _O_ ( _nd_ ), Mamba states _O_ ( _nds_ ), and the state pool _O_ ( _knds_ )
(constant _k_ ), yielding total _O_ ( _nd_ ). The attention’s extra memory is _O_ ( _nd_ ), preserving linearity.


In addition, we compare MemMamba with other SOTA Mamba variants and Transformers under
different parameter scales. The results show that MemMamba consistently outperforms baselines at
the same or even smaller scales, demonstrating superior parameter efficiency.


16


Under review as a conference paper at ICLR 2026


Figure 6: Effect of different pooling functions on modeling quality.


Figure 7: Impact of state-pool size and window size on PPL.


A.5 SENSITIVITY ANALYSIS


We assess robustness with respect to key hyperparameters, including window size _w_, choice/size of
the state pool ( _k_ ), and fusion methods, which directly affect memory fidelity, efficiency, and training
stability (cf. bounds in Sec. 3). We report results over broad ranges to identify optimal trade-offs.


The results indicate: (i) within wide ranges, window size and state-pool size have negligible effect
on performance, implying strong robustness; (ii) among pooling choices, the _simple max_ variant
consistently performs best, outperforming _mean_, T-Max-Avg, and S3Pool. Thus, MemMamba does
not require fine-tuning to remain stable, while stronger pooling can further improve local-fidelity if
needed.


We also compare five fusion methods—gated, residual, elementwise pro-duct, 1D convolution, and
weighted—across sequence lengths from 1k to 60k tokens.Residual and weighted fusion show lower
PPL at most lengths, indicating better long-range modeling, whereas 1D convolution degrades on
very long sequences, likely due to rising computational costs. Detailed results are in Table 4.


In summary, MemMamba is robust to most configuration choices. Window size _w_ and pool size
have little impact across broad ranges; among pooling functions, the _simple max_ choice offers the
best fidelity–efficiency balance. For fusion, differences are small on short sequences, but residual
and weighted fusion dominate on long contexts, while 1D convolution degrades due to complexity.
Overall, MemMamba maintains stable performance without heavy tuning; using _max_ pooling and
weighted fusion provides the most reliable accuracy–efficiency–stability trade-off.


A.6 IMPLEMENTATION DETAILS


All experiments are implemented in PyTorch 2.1.2 and Python 3.10 on Ubuntu 22.04 with CUDA
11.8. Training is conducted on a single NVIDIA RTX 4090 (24GB) and 25 vCPUs (Intel Xeon
Platinum 8481C).


17


Under review as a conference paper at ICLR 2026


Table 4: PPL comparison of fusion methods across context lengths (values adjusted to match means).


Fusion Method 1K 2K 4K 10K 20K 30K 40K 50K 60K


Gated 20.00 18.88 18.18 18.36 18.91 17.99 18.19 18.63 18.01
Residual 19.97 18.75 18.15 18.62 19.17 18.64 18.95 19.31 19.11
Elementwise Prod. 19.89 18.74 18.02 18.20 18.83 17.56 18.10 18.69 17.49
1D Convolution 19.86 18.72 18.04 18.29 18.80 17.45 18.01 18.94 17.41
Weighted **19.35** **18.23** **17.53** **17.71** **18.26** **17.33** **17.54** **17.98** **17.36**


Table 5: Data split statistics.


Split Train Valid Test


Books 28,602 50 100
Tokens 1,973,136,207 3,007,061 6,966,499


Our MemMamba is a 24-layer SSM-based model with cross-layer and cross-token attention modules
added to each layer, following prior observations that vanilla Mamba forgets rapidly outside this
range. Each state summary vector is compressed to 64 dimensions, and the state-pool size is fixed
at 50. The training sequence length is 8k tokens.


We train for 100k steps using AdamW (learning rate = 1e _−_ 4, weight decay = 0 _._ 1), with a constant
LR schedule, gradient accumulation (4 steps), and gradient clipping (max norm = 1). The random
seed is set to 123 for reproducibility.


For the language modeling experiments (PPL), we use the pg19 corpus with the data split in Table 5.


During evaluation, we benchmark nine context lengths (1k, 2k, 4k, 10k, 20k, 30k, 40k, 50k, 60k).
Each sample is divided into 10 windows with 50 labels per window. The train and validation set sizes
are 500 and 50, respectively, and the maximum training input length is 2k tokens. For documents
longer than the training length, we apply random truncation to maintain input compatibility.


18


