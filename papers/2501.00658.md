Published as a conference paper at ICLR 2025

## UNDERSTANDING AND MITIGATING BOTTLENECKS OF STATE SPACE MODELS THROUGH THE LENS OF RE### CENCY AND OVER-SMOOTHING


**Peihao Wang** **[1]** **, Ruisi Cai** **[1]** **, Yuehao Wang** **[1]** **, Jiajun Zhu** **[1,2]** **, Pragya Srivastava** **[3]** **,**
**Zhangyang Wang** **[1]** **, Pan Li** **[4]**

1University of Texas at Austin, 2Zhejiang University, 3Google DeepMind, 4Georgia Tech
{peihaowang,ruisi.cai,yuehao,atlaswang}@utexas.edu, junnian@zju.edu.cn,

pragya8srivastava@gmail.com, panli@gatech.edu


ABSTRACT


Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence
dependencies, we rigorously demonstrate that they are inherently limited by strong
recency bias. Our empirical studies also reveal that this bias impairs the models’ ability to recall distant information and introduces robustness issues. Our
scaling experiments then discovered that deeper structures in SSMs can facilitate
the learning of long contexts. However, subsequent theoretical analysis reveals
that as SSMs increase in depth, they exhibit another inevitable tendency toward
over-smoothing, e.g., token representations becoming increasingly indistinguishable. This _fundamental dilemma_ between recency and over-smoothing hinders
the scalability of existing SSMs. Inspired by our theoretical findings, we propose to _polarize_ two channels of the state transition matrices in SSMs, setting
them to zero and one, respectively, simultaneously addressing recency bias and
over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks
SSMs to benefit further from deeper architectures. All source codes are released at
[https://github.com/VITA-Group/SSM-Bottleneck.](https://github.com/VITA-Group/SSM-Bottleneck)


1 INTRODUCTION


Sequence processing architectures have evolved from RNNs (Hochreiter & Schmidhuber, 1997;
Sutskever et al., 2014; Cho et al., 2014; Cho, 2014), to transformers (Vaswani et al., 2017; Devlin
et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020), and more recently to State Space Models
(SSMs) (Gu et al., 2021a; Gu & Dao, 2023). In particular, SSMs (Gu et al., 2021a; Gu & Dao,
2023; Dao & Gu, 2024) have emerged as a compelling alternative to transformers, enabling more
efficient handling of long sequences. SSMs operate in two modes: convolution and recurrence (Gu
et al., 2021b). During convolutional mode, SSMs assume visibility of the entire sequence and utilize
hardware-optimized convolutions to propagate information across all tokens in parallel. This approach
avoids the calculation of pairwise correlations inherent in attention mechanisms, thereby accelerating
training. Mamba (Gu & Dao, 2023) enabled convolution with a parallel scanning algorithm, which
yielded more expressive sequence-level mixing without sacrificing efficiency. During recurrent mode,
SSMs process one token at a time while maintaining a compact recurrent hidden state encoding the
sequence history. The outputs are sequentially decoded from this hidden state, avoiding storing all
past key-value pairs (Dai et al., 2019) and thus reducing inference memory usage.


Furthermore, SSMs have been meticulously tailored to effectively capture long-range dependencies
and filter contextual information. These models are grounded in HiPPO theory (Gu et al., 2020),
which demonstrates that a first-order Ordinary Differential Equation (ODE) can encapsulate long-term
memory through a designated state matrix known as the HiPPO matrix. Subsequent research (Gu
et al., 2021b;a; Gupta et al., 2022; Gu et al., 2022a) has simplified this state matrix to a diagonal
form, significantly improving computational efficiency while retaining the ability to model longrange dependencies. Mamba (Gu & Dao, 2023) introduced a selection mechanism that selectively
aggregates pertinent information from the context into the state. Concurrently, linear attention models


1


Published as a conference paper at ICLR 2025


have been derived from streamlined attention mechanisms (Katharopoulos et al., 2020; Sun et al.,
2023; Peng et al., 2023; Yang et al., 2023). Collectively, these advances can be interpreted through a
unified lens as more structured SSMs (Dao & Gu, 2024).


Despite their initial empirical successes, recent findings indicate that SSMs may not match transformers in their ability to recall information from long contexts (Arora et al., 2023; Poli et al., 2024) or in
handling more complex retrieval patterns (Park et al., 2024). Furthermore, it has been observed that
Mamba continues to underperform compared to transformers on larger scales (Waleffe et al., 2024).
These shortcomings have not yet been systematically elucidated.


In this paper, we identify two fundamental limitations of SSMs in their ability to model complex longrange dependencies. First, we argue that the long-term memory capabilities of modern SSMs may be
misinterpreted. Our analysis reveals that an SSM layer exhibits a strong recency bias, limiting tokens
to primarily interact with the nearby context. This bias is intrinsic to SSMs and many linear attention
models, regardless of the content-informing techniques employed, such as the selection mechanism
introduced by Mamba (Gu & Dao, 2023). We further posit that the loss of long-range capabilities may
stem from the oversimplification of HiPPO-induced SSMs, which trades efficiency off performance.
To substantiate this claim, we perform a long-range retrieval task on an industrial-scale language
model (Jiang et al., 2023) based on Mamba. Our test results indicate that Mamba catastrophically
forgets distant content once the context length exceeds its memory capacity. Furthermore, we raise
a novel robustness concern regarding SSMs with recency bias: our empirical outcomes show that
Mamba is more susceptible to perturbations on local tokens, making it vulnerable to adversarial
attack, as these local tokens can be easily manipulated to serve as backdoors.


We then conduct a series of scaling experiments with varying context lengths during the pre-training of
SSMs. Our results indicate that increasing the model depth is crucial for enhancing its ability to utilize
long contexts by expanding the receptive field. However, we observe that depth scaling encounters
another bottleneck as performance begins to saturate as depth continues to increase. We analyze the
feature dynamics across the SSM layers and theoretically revealed that SSMs inherently function as
smoothing operators, leading to over-smoothing in deep architectures (NT & Maehara, 2021; Oono &
Suzuki, 2019; Cai & Wang, 2020). As a result, token representations become increasingly uniform
and indistinguishable with each additional layer.


Recognizing such fundamental dilemma between recency and over-smoothing, we introduce a unified
approach called the _polarization_, which simultaneously addresses recency bias and over-smoothing.
Specifically, we reserve two dedicated channels in the state transition matrices of SSMs and reset
them to zero and one, respectively. The all-one channel helps preserve historical information and
prevents catastrophic forgetting caused by locality, while the zero channel inhibits excessive fusion of
information from past tokens, effectively slowing the smoothing rate. Our experiments on associative
recall (Arora et al., 2023) demonstrate that the polarization technique significantly improves recall
accuracy by mitigating locality artifacts and can gain more performance when combined with deeper
architectures by effectively alleviating over-smoothing.


2 PRELIMINARIES


In SSMs (and alike), we represent the a discrete-time sequence of _T_ tokens as _**x**_ =

[ _**x**_ 1 _· · ·_ _**x**_ _T_ ] _[⊤]_ _∈_ R _[T]_ . For vector-valued input sequences, SSMs process each channel independently. To simplify notations, we focus on scalar-valued sequences without loss of generality. The
impact of multi-channel inputs will be addressed in the relevant context. SSMs learn to represent
and forecast the next token by integrating past information. Formally, SSMs can be viewed as a
sequence-to-sequence transformation from inputs _**x**_ _∈_ R _[T]_ to outputs _**y**_ _∈_ R _[T]_ through a _memory state_
_**h**_ _t ∈_ R _[N]_, which is iteratively updated with a linear recurrence. A general form can be written as:


_**h**_ _t_ = _**A**_ _t_ _**h**_ _t−_ 1 + ∆ _t_ _**b**_ _t_ ( _**x**_ _t_ ) _,_ _**y**_ _t_ = _ct_ ( _**h**_ _t_ ) _,_ _**h**_ 0 = **0** _,_ _∀t ∈_ [ _T_ ] _,_ (1)


where _t ∈_ [ _T_ ] denotes the time step. Intuitively, _**A**_ _t ∈_ R _[N]_ _[×][N]_ extracts information from the previous
state _**h**_ _t−_ 1 [1], _**b**_ _t_ : R _→_ R _[N]_ projects every input token to the hidden space, ∆ _t ∈_ R controls how
much information of the new token will be fused into the hidden memory, and _ct_ : R _[N]_ _→_ R decodes


1In most scenarios discussed in this paper, we assume real parameterization by default, as it is the standard
approach in the cases of our primary interest, such as language modeling (Gu & Dao, 2023)


2


Published as a conference paper at ICLR 2025


the hidden state at time _t_ to the final prediction. In all SSMs considered in this paper, it is necessary
to assume ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _only depends on the inputs at the time t_ . SSMs are trained from end to end
to optimize for the parameters _{_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ], for which the different SSMs adopt various
types of instantialization. Below, we list some representative examples.


**S4, DSS, and S4D.** The seminal works (Gu et al., 2020; 2021b; 2022b) demonstrate that discretizing
time-invariant ODE _**h**_ _[′]_ ( _t_ ) = _**Ah**_ ( _t_ ) + _**b**_ _x_ ( _t_ ) with some special realization of matrix _**A**_ can yield
an efficient recurrent network for long-sequence modeling. The follow-up works Gu et al. (2021a)
together with Gupta et al. (2022); Gu et al. (2022a) simplifies _**A**_ to be a diagonal matrix. Applying
the zero-order hold rule for discretization, as suggested by Gupta et al. (2022), we can summarize
this series of models in the form of Eq. 1:


(S4) _**A**_ _t_ = exp(∆ _**A**_ ) _,_ _**b**_ _t_ ( _**x**_ _t_ ) = _**bx**_ _t,_ _c_ ( _**h**_ _t_ ) = _**c**_ _[⊤]_ _**h**_ _t,_ ∆ _t_ = ∆ _,_ [2] (2)


where ( _**A**_ _,_ _**b**_ _,_ _**c**_ _,_ ∆) are learnable parameters. In particular, _**A**_ is restricted to be a diagonal matrix and
can be complex valued. However, _**A**_ must have negative real part (Gu et al., 2022a). ∆ _∈_ (0 _,_ 1] is
often interpreted as the time interval for discretization. We call this family of SSMs _S4_ following the
naming convention in Gu & Dao (2023).


**Mamba.** A recent breakthrough Mamba (Gu & Dao, 2023) introduces the _selection_ mechanism
to extend S4. Instead of learning ( _**A**_ _,_ _**b**_ _,_ _**c**_ _,_ ∆) in Eq. 2 as free parameters, Mamba conditions
( _**A**_ _,_ _**b**_ _,_ _**c**_ _,_ ∆) on the inputs, which enables each iterative step in Eq. 1 to filter useful token information
during the recurrence. Specifically, Mamba computes ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) as follows:


(Mamba) _**A**_ _t_ = exp(∆ _t_ _**A**_ ) _,_ _**b**_ _t_ ( _**x**_ _t_ ) = ( _**W**_ _B_ _**x**_ _t_ ) _**x**_ _t, ct_ ( _**h**_ ) = ( _**W**_ _C_ _**x**_ _t_ ) _[⊤]_ _**h**_ _t,_ ∆ _t_ = _σ_ ( _**W**_ ∆ _**x**_ _t_ ) _,_ (3)


where _**W**_ ∆ _∈_ R _,_ _**W**_ _B ∈_ R _[N]_ _,_ _**W**_ _C ∈_ R _[N]_ are learnable weights in addition to _**A**_, and _σ_ ( _·_ ) denotes
softplus activation. When handling multi-dimensional token embeddings, _**W**_ ∆ _,_ _**W**_ _B,_ _**W**_ _C_ are extended on the input dimension, different ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) are assigned to each token channel and Eq.
3 is running channel-wisely. ∆ _t_ is varying across different channels, while _**b**_ _t,_ _**c**_ _t_ are shared across
token channels. That is, if the token embedding dimension is _D_, then each token channel will be
allocated with a distinct _**W**_ ∆ _∈_ R _[D]_, and shared _**W**_ _B ∈_ R _[N]_ _[×][D]_, and _**W**_ _C ∈_ R _[N]_ _[×][D]_ . In language
modeling, _**A**_ has strictly negative real-valued diagonal, which ensures _**A**_ _t ∈_ (0 _,_ 1) _[N]_ _[×][N]_ . Additionally,
Mamba is integrated with the H3 architecture (Fu et al., 2022), wherein the selective SSMs is working
with a local convolution and sandwiched by two gated connections.


**Linear Attention.** Concurrent with SSMs, there is another line of work streamlining attention
to linear time complexity. With slight abuse of terminology, we name all of them collectively as
Linear Attention Models (LAMs). We observe that many of them can be written in the form of Eq. 1
such that in the remainder of this paper, we extend the definition of SSMs to include LAMs without
introducing ambiguity, as LAMs and SSMs are dual to each other (Dao & Gu, 2024). We leave a full
summary to Appendix B.


3 CAN SSM EFFECTIVELY REPRESENT LONG-RANGE DEPENDENCIES?


3.1 SSMS ARE LOCALLY BIASED


In this section, we investigate the ability of SSMs to learn long-range dependencies. Recent studies
find that SSMs seem more effective than transformers on this task (Gu et al., 2020; Tay et al., 2020;
Li et al., 2022; Gu & Dao, 2023). However, in Sec. 3.1 we theoretically show a negative result that
an SSM layer is inherent to _local bias_ and loses long-term memory exponentially. In Sec. 3.2, we
empirically justify our claim by showing SSMs struggle to retrieve from distant context. We also
demonstrate that the local bias may lead to robustness issues in Sec. 3.3.


To understand how information is propagated and long-range dependencies are modeled in SSMs, we
aim to uncover the relationship between the output at time _t ∈_ [ _T_ ] and the input token at time _s ≤_ _t_ .
We define the derivatives _|∂_ _**y**_ _t/∂_ _**x**_ _s|_ as the _influential score_ to represent the importance of the _s_ -th


2More rigorously, by zero-order hold, _**b**_ should be parameterized as _**b**_ = (∆ _**A**_ ) _−_ 1(exp( _−_ ∆ _**A**_ ) _−_ _**I**_ ) _**b**_ .
However, the presented form is more commonly used in practice as in Gu & Dao (2023).


3


Published as a conference paper at ICLR 2025


input token to the _t_ -th output token. Note that _|∂_ _**y**_ _t/∂_ _**x**_ _s|_ is well-defined for every _s, t ∈_ [ _T_ ] as long
as ( _**A**_ _t._ _**b**_ _t, ct,_ ∆ _t_ ) are all differentiable in terms of _**x**_ . Intuitively, if _|∂_ _**y**_ _t/∂_ _**x**_ _s|_ is larger, then the _s_ -th
input token is more influential on the _t_ -th output token, and vice versa.


Below we present a formal result regarding the influential score.
**Theorem 3.1** (Recency of SSMs) **.** _Consider an SSM defined in Eq. 1 with {_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _._
_Assume that (i) the input space X ⊂_ R _[T]_ _is compact, (ii) {_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _are continuous and_
_have continuous derivatives, and (iii)_ _**A**_ _t ∈_ (0 _,_ 1) _[N]_ _[×][N]_ _are diagonal matrices for all t ∈_ [ _T_ ] _. Let_
_Amax_ = max _t∈_ [ _T_ ] _,n∈_ [ _N_ ]( _**A**_ _t_ ) _n,n. Then for arbitrary_ _**x**_ _∈X and every s, t ∈_ [ _T_ ] _such that s < t,_
_|∂_ _**y**_ _t/∂_ _**x**_ _s|_ = _O_ (exp( _−κ_ ( _t −_ _s_ ))) _for some κ_ = Θ(log( _A_ _[−]_ _max_ [1] [))] _[.]_





The proof can be found in Appendix D.1. The first two
assumptions are standard and always satisfied. The third
assumption also holds for most of SSMs discussed in
Sec. 2. Therefore, Theorem 3.1 applies to numerous
SSMs including S4 (Gu et al., 2021a; 2022a), Mamba
(Gu & Dao, 2023), and many LAMs (Sun et al., 2023;
Peng et al., 2023; Yang et al., 2023; Qin et al., 2024; De
et al., 2024). Theorem 3.1 states that influential scores
between two tokens modeled by SSMs are _exponentially_
diminishing with respect to their relative distance. The
decay rate is determined by the maximal values among all
_**A**_ _t_ ’s elements. The closer _Amax_ is to zero, the faster the
influential scores decay. The practical implication is that
SSMs are factually recency-biased models. Tokens farther
away are under-reaching and forgotten rapidly while the
information of closer tokens dominates the final output.
This can significantly limit their ability of fitting complex
long-range relationships.


|20|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||||
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||||
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||~~130M pr~~|~~ trained~~|
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||<br>1.4B pret<br>130M w/<br>1.4B w/o|<br> rained<br> o training<br>  training|
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||||
|0<br>100<br>200<br>300<br>400<br>50<br>Relative token position<br>10<br>5<br>0<br>5<br>10<br>15<br><br>log Score<br>~~130M pretrained~~<br>1.4B pretrained<br>130M w/o training<br>1.4B w/o training|||||



Figure 1: Visualization of log influential
scores log _|∂_ _**y**_ _t/∂_ _**x**_ _s|_ versus distance ( _t−s_ ).





**Empirical Validation.** In Fig. 1, we plot the logarithmic influence scores against relative distances.
Across various model sizes, Mamba consistently exhibits a linear decay rate of influence scores,
both at initialization and throughout training. This observation suggests that recency arises from an
inherent model bias, as described in Theorem 3.1, rather than solely from capturing data statistics.
Moreover, the adopted initialization scheme further amplifies the locality bias.


**Is the design of decay necessary or desirable?** One key observation from our theory is that the
parameterization of _**A**_ _t_ within the interval (0 _,_ 1) leads to strictly decaying dependencies among tokens
based on their relative distances. This design choice appears to be a standard practice, and perhaps
intentional, in several recently proposed SSMs (Gu & Dao, 2023; Dao & Gu, 2024; Beck et al., 2024;
Yang et al., 2023; Peng et al., 2024; De et al., 2024; Ma et al., 2024; Liu et al., 2024; Yang et al., 2024).
Interestingly, it has been demonstrated that this “intentional” decay of long-range dependencies not
only avoids degrading the perplexity of language models but also improves generalization for length
extrapolation. This observation aligns with the empirical success of soft gating mechanisms adopted
in traditional RNNs (Cho, 2014; Cho et al., 2014; Gu et al., 2021b) and the decaying patterns imposed
on transformers (Raffel et al., 2020; Press et al., 2021; Sun et al., 2022). We find the constraint
_**A**_ _t ∈_ (0 _,_ 1) _[N]_ _[×][N]_ is likely inherent to SSMs as it plays a critical role in ensuring the numerical
stability for length generalization during long-context recurrence (Gu et al., 2022a; Yang et al., 2023).
Promoting the importance of local tokens could also lead to a nearly correct bias, as natural language
generation mostly utilizes recent contexts. However, as we will detail in subsequent sections, this
design comes with a significant drawbacks: it result in substantial loss of long-distance information
(Sec. 3.2) and may raise potential security concerns (Sec. 3.3). This observation underscores that
the validation perplexity may not fully capture all aspects of a model’s capabilities and can be an
inadequate metric for assessing long-range dependencies.


3.2 LOST IN THE DISTANCE: LONG-CONTEXT RETRIEVAL TEST


To assess the ability of large language models (LLMs) to effectively utilize long-context data, we
evaluate open-source SSM using the “Needle in a Haystack” benchmark and compare its performance


4


Published as a conference paper at ICLR 2025













Figure 2: Comparison between SSM and Transformer on the “Needle in a Haystack" benchmark. The left figure
shows the retrieval accuracy of the Mamba-Codestral-7B model, while the right figure presents the retrieval
accuracy of the Mistral-7B model. We present a heatmap where "full context length" refers to the total length
of the document, and "needle position" denotes the relative position of the statement to be retrieved within the
context. See more fine-grained visualization in Appendix E.2.


with that of Transformer. In this benchmark, a randomly generated statement is embedded within the
middle of a long document, and the models are tasked with retrieving the statement. By varying the
insertion position of the statement, we measure the retrieval accuracy at each location, which reflects
the model’s positional bias. To enforce LLMs using the data within context, instead of recalling
information memorized by its model weights, we carefully design the statement with factual error.
See detailed examples in Appendix E.2.


We compare the retrieval accuracy of the Mamba-Codestral-7B model, a representative SSM capable
of handling long-context inputs of up to 256k tokens, with Mistral-7B (Jiang et al., 2023), which
utilizes a transformer architecture. As illustrated in Figure 2, the retrieval accuracy of the Transformer
remains stable regardless of the needle position. In contrast, the SSM achieves higher accuracy when
the needle is placed closer to the end of the context (i.e., larger needle position values), while its
accuracy drops when the needle is located near the beginning of the document. This indicates a
positional bias towards local tokens in the SSM.


3.3 POTENTIAL RISK ON MODEL ROBUSTNESS


We conduct quantitative experiments to show the recency-biased nature of SSMs will lead to potential
hazards. The downstream task in this study is image classification on sequences of pixels (Tay et al.,
2020), where _W × H_ images are flattened to sequences of pixel tokens and fed to sequence models
for classification. We test a family of SSMs, including H3 (Fu et al., 2022), RWKV (Peng et al.,
2023), and Mamba (Gu & Dao, 2023), and compare them against a transformer baseline (Vaswani
et al., 2017) on CIFAR-10 dataset. To adapt SSMs for this task, we append a learnable class token
after the last token of the input sequence. The output state of this class token is then mapped to logits
using a classifier head. Experiment details are given in Appendix E.3. In the following, two attack
patterns on the input data are introduced, which degrade the robustness of SSMs in this task.


**Adversarial Attack.** To assess the bias of SSMs towards corrupted data, we perturb the leading
and trailing tokens of input sequences with random noise. In unbiased models, perturbations in both
leading and trailing tokens cause similar performance drops. However, in locally biased models,
where the class token is appended after the last input token, the trailing tokens are supposed to have
greater impacts on classification outcomes than leading tokens. Table 1 presents our experimental
results on the CIFAR-10 dataset under two corruption ratios. For each ratio, the same number of


**Corrupted region** (seq. length = 1024)
Models (no corrupt) [992:1024] [0:32] [928:1024] [0:96]


H3 0.654 0.569 ( _↓_ 13.04%) 0.654 ( _↓_ 0.03%) 0.477 ( _↓_ 27.07%) 0.650 ( _↓_ 0.72%)
Transformer 0.580 0.535 ( _↓_ 7.81%) 0.447 ( _↓_ 22.95%) 0.431 ( _↓_ 25.76%) 0.370 ( _↓_ 36.32%)
RWKV 0.474 0.150 ( _↓_ 68.35%) 0.466 ( _↓_ 1.58%) 0.138 ( _↓_ 70.88%) 0.460 ( _↓_ 2.91%)
Mamba 0.674 **0.126 (** _↓_ **81.24%)** 0.658 ( _↓_ 2.30%) **0.098 (** _↓_ **85.46%)** 0.647 ( _↓_ 3.98%)


Table 1: Results of adversarial attack experiments on the CIFAR-10 dataset, evaluated using classification
accuracy. Each input sequence contains 1,024 tokens. Two corruption ratios (32 _/_ 1024 and 96 _/_ 1024) are applied
to perturb the leading and trailing tokens, respectively.


5


Published as a conference paper at ICLR 2025











|Attack region<br>[0:256]<br>[768:1024]|Attack region<br>[0:256]<br>[768:1024]|Col3|Col4|
|---|---|---|---|
|Attack region<br>[0:256]<br>[768:1024]||||
|Attack region<br>[0:256]<br>[768:1024]||||
|Attack region<br>[0:256]<br>[768:1024]||||


|Col1|Col2|
|---|---|
|||


|Col1|Col2|
|---|---|
|||


(a) Attack ratio = 256 _/_ 1024 (25 _._ 00%)



|Col1|Attack region<br>[0:480]<br>[544:1024]|Col3|Col4|Col5|
|---|---|---|---|---|
||Attack region<br>[0:480]<br>[544:1024]|Attack region<br>[0:480]<br>[544:1024]|Attack region<br>[0:480]<br>[544:1024]||
||||||
||||||
||||||
||||||
|||sfo|sfo|sfo|


|Col1|Col2|
|---|---|
|||


(b) Attack ratio = 480 _/_ 1024 (46 _._ 875%)



Figure 3: Results of target attack experiments on CIFAR-10, where “horse” is the target class. (a) and (b)
present target attack success rates under two attack ratios. Lower success rates suggest higher robustness in the
corresponding attack regions.


leading and trailing tokens are corrupted with Gaussian noise. Among all the SSM family methods
compared, the performance drops caused by trailing token corruption are significantly larger than
those caused by leading token corruption. Notably, for Mamba, perturbing the last 32 out of 1024
tokens results in an 81 _._ 24% drop in classification accuracy, whereas corrupting the first 32 tokens only
reduces accuracy by 2 _._ 30%. In contrast, the transformer baseline shows relatively smaller impacts
from trailing token corruption. Instead, our experiments indicate that more informative features from
transformers tend to sink in the leading tokens, aligning with the observations in Xiao et al. (2023).


**Target Attack.** Beyond degrading the performance of SSMs by attacking trailing tokens, we also
demonstrate that local bias creates a backdoor for target attacks. In this scenario, a target class is
selected, and pixel tokens from that class are used to replace those in images from other classes. The
attack succeeds when models mis-classify images from other classes as belonging to the target class.
Due to the local bias, trailing tokens are expected to be a more effective attack region for SSMs,
leading to a significantly higher attack success rate compared to leading tokens. Fig. 3 shows the
success rate comparisons across different attack regions and ratios. When trailing regions are replaced
with pixels from the target class, SSMs achieve much higher success rates than when leading regions
are attacked. This phenomenon is observed at both 25% and 47% attack ratios. By comparison, the
transformer model possesses greater robustness, maintaining similar success rates between attacks on
leading and trailing tokens.


**Implications for Language Models.** While our adversarial attack experiments are conducted on
image datasets, the findings have broader implications for language models. System prompts, which
are typically a group of confidential tokens prepended to user inputs, play a critical role in controlling
the behavior of language models and preventing undesirable outputs. However, our targeted attack
experiments reveal that SSM-based language models are particularly vulnerable to jailbreak attacks
(Perez & Ribeiro, 2022; Zou et al., 2023). This is because SSMs prioritize recent information over
past tokens, making it easier to bypass system prompts by appending jailbreak instructions at the end
of the input. Moreover, our theoretical analysis suggests that fine-tuning LLMs with instructional
datasets or human feedback to enforce adherence to system prompts may not resolve this vulnerability,
as the recency bias remains inherent to SSM models regardless of weight configurations.


4 UNDERSTANDING SCALABILITY BOTTLENECK OF SSMS


4.1 NECESSITY AND LIMITS OF DEPTH SCALING


In Sec. 3.1, we have seen that the dependencies between tokens are exponentially decaying with
their relative distances in an SSM layer. Consequently, SSMs resemble localized kernels, similar
to those employed in various neural architectures such as Convolutional Neural Networks (CNNs)
(LeCun et al., 1998) and Graph Neural Networks (GNNs) (Kipf & Welling, 2016). It is a reasonable
postulation that increasing the number of layers can extend the model’s receptive field (Goodfellow
et al., 2016). We justify this hypothesis via a scaling-up experiment with various context lengths and
model architectures.


We pretrain Mamba using causal language modeling with two context lengths, {2048, 8192}. Besides,
we fix the number of layers at {16, 24, 32, 48, 64, 72} and vary the hidden dimension. We defer


6


Published as a conference paper at ICLR 2025










|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||


|Col1|Col2|Col3|Col4|# Laye<br># Laye|r=|Col7|
|---|---|---|---|---|---|---|
||||||||
|||||<br># Laye<br>~~# Laye~~|<br> r=32<br>~~ =48~~||
||||||||
|||||<br># Laye<br>~~# Laye~~|<br> r=64<br>~~ =72~~||
||||||||
||||||||
||||||||
||||||||
||||||||



Figure 4: We empirically observe that deeper models become increasingly advantageous as the context length
grows. However, beyond a certain depth, the performance of SSMs begins to plateau and eventually declines.


more experiment details in Appendix E.4. The validation loss versus the number of parameters is
plotted in Fig. 4. Under the 2048 context length, models of different configurations exhibit similar
performance, consistent with the findings of Kaplan et al. (2020). However, as the context length
increases, the scaling behavior across depth-width configurations begins to diverge. Notably, deeper
models outperform shallower ones, likely because deeper architectures can more effectively utilize
the extended context to meet the training objectives. Nevertheless, we observe that the performance
gain starts to saturate when we keep increasing the depth (cf. the 32-layer and 48-layer models).
When the depth of the model continues to increase, the validation perplexity starts to rise, indicating
a decline in performance (cf. the 64-layer and 72-layer models). In Mamba with a 2048 context
length, models with more than 48 layers perform worse than 16-layer models. Longer-context models
appear to be more tolerant of increased depth, whereas shorter-context models experience a rapid
performance degradation once the depth exceeds a certain threshold.


4.2 UNVEILING OVER-SMOOTHING IN SSMS


To explain the depth scaling bottleneck revealed in the previous section, we conduct a theoretical
and empirical investigation of the feature and state dynamics in SSMs. Our key finding is that token
embeddings, after being processed by SSM layers, tend to become increasingly similar, which leads
to a phenomenon commonly referred to as _over-smoothing_ (NT & Maehara, 2021; Cai & Wang, 2020;
Oono & Suzuki, 2019). Over-smoothing occurs when token representations become indistinguishable,
rendering the state uninformative.


First of all, we warm up by studying continuous-time S4 with constant ( _**A**_ _,_ _**b**_ _,_ _**c**_ ). Recall that a
continuous-time S4 layer can be described by a group of ODEs: _**h**_ _[′]_ ( _t_ ) = _**Ah**_ ( _t_ ) + _**b**_ _x_ ( _t_ ) _,_ _**y**_ ( _t_ ) =
_**c**_ _[⊤]_ _**h**_ ( _t_ ). Our analysis starts with the equivalence between convolution and S4 (Gu et al., 2021b).
This is, the analytic solution to the time-invariant ODE can be expressed as _**y**_ ( _t_ ) = - _**c**_ _[⊤]_ exp( _**A**_ ( _t −_
_s_ )) _**b**_ _x_ ( _s_ ) _ds_ . Now we analyze the filtering property of this convolution operator from the Fourier
domain perspective. We define a convolutional operator as a low-pass filter if it suppresses highfrequency components (see Definition D.3). We summarize the main finding in the following
proposition, whose formal version and proof are provided in Appendix D.2.1:


**Proposition 4.1** (Low-pass filtering of continuous S4) **.** _Consider a continuous-time S4 with pa-_
_rameters_ ( _**A**_ _,_ _**b**_ _,_ _**c**_ ) _. Assume_ _**A**_ _is diagonal with all values negative. Then_ _**y**_ ( _t_ ) = - _**c**_ _[⊤]_ exp( _**A**_ ( _t −_
_s_ )) _**b**_ _x_ ( _s_ ) _ds defines a low-pass filter._


Proposition 4.1 states that S4 is inherently a low-pass filter regardless of how ( _**A**_ _,_ _**b**_ _,_ _**c**_ ) are trained.
Therefore, the high-frequency components of input signals are being constantly removed at each
layer. Presumably, stacking many S4 layers might cause over-smoothing when all high-frequency
components are suppressed to zero.


Now we consider a more general scenario when SSMs work on discrete-time regime and
( _**A**_ _t,_ _**b**_ _t,_ _**c**_ _t,_ ∆ _t_ ) are time-varying or even data-dependent. Formally, we prove the following result
showing the sharpness of input signals will be reduced as well:


7


Published as a conference paper at ICLR 2025







(a) _**b**_ _t_ and _**h**_ _t_ .



(c) Block output.











**Theorem 4.2** (Over-smoothing of SSMs) **.** _Consider an SSM specified in Eq. 1 with_
_{_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _._ _Assume an input space X_ _⊂_ R _[T]_ _such that for every_ _**x**_ _∈X_ _, (i)_
( _**A**_ _t_ ) _n,n_ +∆ _t ≤_ 1 _for every n ∈_ [ _N_ ] _and t ∈_ [ _T_ ] _, (ii)_ min _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n ≤_ 0 _and_ max _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n ≥_
0 _for every n ∈_ [ _N_ ] _. Let Amin_ = min _t∈_ [ _T_ ] _,n∈_ [ _N_ ]( _**A**_ _t_ ) _n,n. Then for any_ _**x**_ _∈X and the memory_
_states {_ _**h**_ _t_ : _t ∈_ [ _T_ ] _} generated by the SSM, we have:_


max �1 _−_ _A_ _[T]_ _min_ _[ −]_ [1]       - max (4)
_t,s∈_ [ _T_ ] _[∥]_ _**[h]**_ _[t][ −]_ _**[h]**_ _[s][∥][∞]_ _[≤]_ _t,s∈_ [ _T_ ] _[∥]_ _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)] _[ −]_ _**[b]**_ _[s]_ [(] _**[x]**_ _[s]_ [)] _[∥][∞]_ _[,]_


Proof can be found in Appendix D.2.2. We first justify our assumptions here. ( _**A**_ _t_ ) _n,n_ + ∆ _t ≤_ 1 is a
generic condition to ensure the recurrence of SSMs is non-expansive, which is crucial to guarantee
memory states stay numerically stable. The second assumption requires the data to be well-distributed
and centered around the origin, which can be easily satisfied by normalization techniques. We find
that prevalent SSM models such as (Gu & Dao, 2023; Peng et al., 2023; De et al., 2024; Qin et al.,
2024) can easily achieve these two assumptions.


Moreover, if ( _**A**_ _t_ ) _n,n_ + ∆ _t_ = 1 is always true letting each recurrent update be conservative (Peng
et al., 2023; Ma et al., 2022), then we can remove the second assumption as well (see Theorem D.5).
Theorem 4.2 examines the relationship between the pairwise distances of memory states and encoded
tokens within the sequence. This result indicates that the pairwise discrepancies among memory
states are diminished by a factor less than one, suggesting that the memories undergo smoothing
following the application of an SSM in Eq. 1. We deem that if the memory is losing its discriminative
capacity, the intermediate hidden feature space will similarly collapse.


Delving deeper, the decay rate is intricately linked to both the context length and the minimal value
within _{_ _**A**_ _t, t ∈_ [ _T_ ] _}_ . As the context length increases, it requires more time to effectively mix all
tokens. This can be understood from the message-passing perspective (Gilmer et al., 2017): the
message of the first token needs to go through the whole sequence to be mixed with the last token.
When _Amin_ approaches one, the decay rate is maximized, as the entire SSM essentially performs a
uniform pooling over the entire sequence, which smoothens the signal via a box-like filter. It is worth
noting that the smoothing nature of SSMs is intuitive; one can conceptualize the recurrent operation
of SSMs as performing a running average of the encoded token signals.


**Empirical Validation.** We adopt a pairwise distance between tokens to quantify the sharpness
of a signal: _E_ ( _**x**_ ) = 2( _N_ 1 _−_ 1) �� _i_ = _j_ _[∥]_ _**[x]**_ _[i][ −]_ _**[x]**_ _[j][∥]_ 2 [2] - _/_ �� _i_ _[∥]_ _**[x]**_ _[i][∥]_ 2 [2] �. _E_ ( _**x**_ ) being small means the token
representations are close to each other and become less discriminative. We plot the feature smoothness
of a 1.4B Mamba in Fig. 5. In Fig. 5a, _**b**_ _t_ is above _**h**_ _t_ among all Mamba blocks. This suggests
the sharpness of input signals is consistently higher than the sharpness of the memory state output
from Mamba, verifying our Theorem 4.2. In addition, Fig. 5b and 5c show the sharpness of Mamba
mixer and Mamba block output, which tends to decrease rapidly in deeper layers. We also provide a
comparison with a transformer (of the same size). Although transformers suffer from over-smoothing
in theory (Dong et al., 2021; Shi et al., 2022; Wang et al., 2022), we observe that transformers have a
slower decay of feature sharpness. See a theoretical comparison in Appendix C.


8


Published as a conference paper at ICLR 2025


5 DISCUSSIONS


In this section, we provide further discussions based on our theory while deferring the remaining
parts to Appendix C due to the page limit. We also introduce more related work in Appendix A.


**Revisiting HiPPO theory.** HiPPO, introduced in (Gu et al., 2020) and extended by Gu et al. (2021b;
2022b), forms the theoretical basis of SSMs. It optimally reconstructs a signal _x_ up to time _t_ by
minimizing _∥x≤t −_ _y_ [(] _[t]_ [)] _∥L_ 2( _ω_ ( _t_ )) with respect to a measure _ω_ [(] _[t]_ [)] supported on ( _−∞, t_ ]. The solution
projects _x≤t_ onto _N_ basis functions, producing a coefficient vector _**h**_ ( _t_ ), which synthesizes _y_ [(] _[t]_ [)] via
linear combinations. Gu et al. (2020) showed _**h**_ ( _t_ ) evolves as _**h**_ _[′]_ ( _t_ ) = _**A**_ ( _t_ ) _**h**_ ( _t_ ) + _**b**_ ( _t_ ) _x_ ( _t_ ), where
_ω_ [(] _[t]_ [)] = I[0 _, t_ ] _/t_ yields closed form _**A**_ ( _t_ ) = _−_ _**A**_ _hippo/t_ . Subsequent works like S4 (Gu et al., 2021a)
and Mamba (Gu & Dao, 2023) utilize this matrix as initialization but omitted 1 _/t_, resulting in a
warped measure _ω_ [(] _[t]_ [)] ( _s_ ) _∝_ exp( _s −_ _t_ )I[ _s < t_ ] (Gu et al., 2022b). This measure emphasizes recent
history when approximating _x≤t_ . Hence, findings of Gu et al. (2022b) do not contradict our results
but also align with Theorem 3.1. The practical implementations often disconnect from HiPPO theory
by neglecting the unitary matrices associated with _**A**_ _hippo_ . Whereas, we focus on discrete-domain
SSMs, adhering to practical parameterizations. Additionally, Gu et al. (2021b) demonstrates that
SSMs’ expressiveness spans all convolutions and RNNs. We contend that the low-pass filtering
property also arises from the parameter simplifications of _**A**_ _t_ (see Proposition 4.1).


**The effect of selection mechanism.** Traditional S4 architectures operate as linear time-invariant
systems. To introduce more non-linearity, Mamba (Gu & Dao, 2023) proposes modeling ( _**b**_ _t, ct,_ ∆ _t_ )
as a function of inputs, a mechanism known as _selection_ . This is motivated by the selective copying
synthetic task, wherein _**A**_ _t_ and _**b**_ _t_ need to adapt based on content to filter relevant information
for memory updates. Despite this adaptation, Theorem 3.1 still holds in scenarios involving the
selection mechanism, meaning selective SSMs like Mamba may continue to suffer from recency bias.
In the meantime, Theorem 4.2 also applies to Mamba, suggesting that the selective SSMs do not
demonstrate higher expressiveness in filtering signals and perform similarly to linear S4 as low-pass
filters (Proposition 4.1). Nevertheless, we note that selection can alleviate these issues by adaptively
controlling the values in _**A**_ _t_ . According to our theory, the selection mechanism can potentially make
the upper bound _Amax_ closer to one and the lower bound _Amin_ closer to zero. However, parameter
_**A**_ in Eq. 3 is initialized with negative integers (Gu et al., 2022a), which exacerbates the bound in
Theorem 3.1 by accelerating the decay rate of the influence score.


**Complex parameterization.** Our analysis in the main text primarily focuses on the case where
( _**A**_ _t,_ _**b**_ _t, ct_ ) are both real-valued. While most modern SSMs adopt real parameterizations, complex
parameterizations – particularly complex-valued _**A**_ _t_ - have been explored in prior works such as
Gupta et al. (2022); Goel et al. (2022); Gu et al. (2022a). Notably, we show that both our locality result
(Theorem 3.1) and over-smoothing result (Proposition 4.1) remain valid for complex-valued SSMs.
We formalize these extensions in Theorems D.2 and D.4. These findings collectively demonstrate that
complex parameterization does not eliminate the locality or over-smoothing bias inherent in SSMs.


**Importance of context scaling.** Theorem 4.2 also highlights the importance of context-length
scaling to mitigate the over-smoothing issue. As the smoothing rate decreases with increased context
length, lengthening training sequences not only relieves over-smoothing but also maximizes the
utility of hardware efficiency of SSMs. This is also evidenced by Fig. 4, where models with longer
training contexts have better tolerance of deeper architectures. To enable SSMs to fully utilize the
context, increasing the model depth is essential. Consequently, a synchronized scaling of both model
depth and context length is required. Investigating the scaling laws governing these two dimensions
presents an intriguing direction for future research.


6 MITIGATING RECENCY AND OVER-SMOOTHING VIA POLARIZATION


In this section, we propose a simple solution to mitigate recency and over-smoothing simultaneously.
First of all, results in Theorem 3.1 and Theorem 4.2 can be interpreted in conjunction. To relieve the
smoothening rate, one might aim to minimize the values in _**A**_ _t_ . However, this could inadvertently
enhance the locality of SSMs, as a decrease in _Amax_ may occur. A practical implication of this


9


Published as a conference paper at ICLR 2025


relationship is that the values in _**A**_ _t_ should be as diverse as possible to simultaneously mitigate the
artifacts of recency and over-smoothing.



Although _Amin ≈_ 0 and _Amax ≈_ 1 could theoretically occur simultaneously, our empirical findings show that these values are largely concentrated
within a narrow range. To illustrate this, we visualize the distribution of ( _Amax −_ _Amin_ ) across different channels in Fig. 6. Each bin represents the
proportion of channels whose memory state satisfies
( _Amax_ _−Amin_ ) being smaller than the corresponding
threshold on the x-axis. Notably, over 60% of channels have ( _Amax −_ _Amin_ ) values smaller than 0.5,
indicating that most channels cannot simultaneously
achieve _Amax ≈_ 1 and _Amin ≈_ 0. Memory representations will inevitably undergo either exponential
diminishing or over-smoothing.


|Cumula|Col2|Col3|umula|ative Di|Col6|Col7|istrib|bution|of (Ama|Col11|ax Am|Col13|min)|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||Am|Am|ax<br>Am|ax<br>Am|in|in|
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||
|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Cumulative Percentage (%)<br>Cumul|||||||||||||||



Figure 6: Cumulative histogram of ( _Amax −_
_Amin_ ). The height of each bin represents the cumulative proportion of ( _Amax −_ _Amin_ ) less than
or equal to the corresponding value on the x-axis.







To this end, we propose to maintain one component _max_ _min_
in _**A**_ _t_ as a constant 1, another as a constant 0, and or equal to the corresponding value on the x-axis.
others freely learnable. We term this approach as _polarization_ . Polarization ensures that the dimension
polarized to zero in the state memory consistently focuses on the current token, counteracting oversmoothing by preventing mixing with previous tokens. Simultaneously, another dimension polarized
to one exclusively retains information from past tokens, avoiding locality issues by preserving the
complete history. In our implementation (see Appendix E.5), we polarize the first state channel to


We empirically validate our polariza
_recall_ tasks, where Mamba models

call information with higher accuracy

|channel to zero (i.e., (At)N,N = 0).|Col2|
|---|---|
|Confgurations<br># Layers<br># KV Pairs<br>Avg.<br>64<br>128<br>256|Confgurations<br># Layers<br># KV Pairs<br>Avg.<br>64<br>128<br>256|
|Default**_ A_**_t_<br>Default**_ A_**_t_|2<br>98.38<br>81.81<br>36.00<br>72.06<br>4<br>99.23<br>82.08<br>33.52<br>71.61|
|(**_A_**_t_)1_,_1 = 1<br>(**_A_**_t_)_N,N_ = 0<br>(**_A_**_t_)_N,N_ = 0|2<br>99.81<br>94.70<br>56.39<br>83.63<br>2<br>98.41<br>81.35<br>36.55<br>72.10<br>4<br>99.74<br>92.20<br>52.21<br>81.38|
|(**_A_**_t_)1_,_1 = 1_,_ (**_A_**_t_)_N,N_ = 0<br>(**_A_**_t_)1_,_1 = 1_,_ (**_A_**_t_)_N,N_ = 0|2<br>99.23<br>95.54<br>54.74<br>83.17<br>4<br>99.94<br>98.80<br>81.56<br>93.43|

from a larger number of key-value Table 2: Results of polarization. Rows 1-2 have no polarization,
pairs, then it has better long-context rows 3-5 only polarize one channel to either one or zero, and rows
capability. We test the performance of 6-7 polarize both channels.
Mamba with neither, one of, or both zero- and one-polarized channels, across different numbers
of layers. The empirical results are reported in Tab. 2 (an extended version in Appendix E.5).
The key observation is that the default parameterization of _**A**_ _t_ suffers from information retrieval
from long context, and deepening the architecture even harms the performance (potentially due to
over-smoothing). However, once we introduce a channel polarized to one, even shallow Mamba could
perform high-accuracy associative recall (row 3). We can also gain the performance by deepening
Mamba if the over-smoothing issue can be bypassed by adopting the zero-polarized channel (row 5).
Furthermore, if we could apply one- and zero-polarized channels, and in the meanwhile deepening
the architecture, we find it achieves the best performance over all settings (row 7).





Table 2: Results of polarization. Rows 1-2 have no polarization,
rows 3-5 only polarize one channel to either one or zero, and rows
6-7 polarize both channels.



7 CONCLUSION


In this study, we identify two critical limitations of SSMs. First, we reveal that SSMs exhibit a
pronounced recency bias, which undermines their ability to model long-range dependencies, recall
distant information, and maintain robustness. Second, we find that increasing the depth of SSMs
induces over-smoothing, rendering token representations indistinguishable and hindering further
performance improvements. To address these challenges, we propose a polarization technique that reserves two channels in the state transition matrices, assigning one a value of zero and the other a value
of one. Both theoretical analysis and empirical evaluations demonstrate that polarization significantly
enhances the long-range modeling capabilities of SSMs while alleviating over-smoothing.


10


Published as a conference paper at ICLR 2025


ACKNOWLEDGMENTS


PW thanks Bo Liu and Songlin Yang for the insightful discussions when preparing this manuscript.
Work was done while JZ and PS were interning at UT Austin. PL is supported by NSF awards
IIS-2239565 and IIS-2428777 for this project.


REFERENCES


Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. _arXiv preprint_
_arXiv:2403.01590_, 2024.


Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
_arXiv preprint arXiv:2006.05205_, 2020.


Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra,
and Christopher Ré. Zoology: Measuring and improving recall in efficient language models. _arXiv_
_preprint arXiv:2312.04927_, 2023.


Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova,
Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended
long short-term memory. _arXiv preprint arXiv:2405.04517_, 2024.


Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf,
and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. _arXiv_
_preprint arXiv:2406.14528_, 2024.


Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. _IEEE transactions on neural networks_, 5(2):157–166, 1994.


Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.
Pythia: A suite for analyzing large language models across training and scaling. In _International_
_Conference on Machine Learning_, pp. 2397–2430. PMLR, 2023.


Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. _Advances in neural information processing systems_, 33:1877–1901, 2020.


Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In _International_
_Conference on Machine Learning Workshop (ICMLW)_, 2020.


Kyunghyun Cho. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. _arXiv preprint arXiv:1406.1078_, 2014.


Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. _arXiv preprint arXiv:1409.1259_, 2014.


Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. _arXiv preprint arXiv:2009.14794_, 2020.


Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. _arXiv preprint_
_arXiv:1901.02860_, 2019.


Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality. _arXiv preprint arXiv:2405.21060_, 2024.


Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert
Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. _arXiv preprint_
_arXiv:2402.19427_, 2024.


11


Published as a conference paper at ICLR 2025


Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In _Proceedings of NAACL_, 2019.


Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M
Bronstein. On over-squashing in message passing neural networks: The impact of width, depth,
and topology. In _International Conference on Machine Learning_, pp. 7865–7885. PMLR, 2023.


Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure
attention loses rank doubly exponentially with depth. In _International Conference on Machine_
_Learning (ICML)_, 2021.


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint_
_arXiv:2010.11929_, 2020.


Leo Feng, Frederick Tung, Mohamed Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh.
Were rnns all we needed? _arXiv preprint arXiv:2410.01201_, 2024.


Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungry hungry hippos: Towards language modeling with state space models. _arXiv preprint_
_arXiv:2212.14052_, 2022.


Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspective on transformers. _arXiv preprint arXiv:2312.10794_, 2023.


Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In _International conference on machine learning_, pp.
1263–1272. PMLR, 2017.


Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It’s raw! audio generation with
state-space models. In _International Conference on Machine Learning_, pp. 7616–7633. PMLR,
2022.


Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. _Deep learning_, volume 1.
MIT Press, 2016.


Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv_
_preprint arXiv:2312.00752_, 2023.


Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory
with optimal polynomial projections. _Advances in neural information processing systems_, 33:
1474–1487, 2020.


Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured
state spaces. _arXiv preprint arXiv:2111.00396_, 2021a.


Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré.
Combining recurrent, convolutional, and continuous-time models with linear state space layers.
_Advances in neural information processing systems_, 34:572–585, 2021b.


Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization
of diagonal state space models. _Advances in Neural Information Processing Systems_, 35:35971–
35983, 2022a.


Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. How to train your hippo:
State space models with generalized orthogonal basis projections. _arXiv preprint arXiv:2206.12037_,
2022b.


Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. _Advances in Neural Information Processing Systems_, 35:22982–22994, 2022.


Michael Hahn. Theoretical limitations of self-attention in neural sequence models. _Transactions of_
_the Association for Computational Linguistics_, 8:156–171, 2020.


12


Published as a conference paper at ICLR 2025


Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):
1735–1780, 1997.


Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.


Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. _arXiv preprint arXiv:2402.01032_, 2024.


Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.


Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
_arXiv preprint arXiv:2001.08361_, 2020.


Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In _International conference on machine_
_learning_, pp. 5156–5165. PMLR, 2020.


Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
_arXiv preprint arXiv:1609.02907_, 2016.


Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. _Proceedings of the IEEE_, 86(11):2278–2324, 1998.


Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash
Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training
sets for language models. _arXiv preprint arXiv:2406.11794_, 2024.


Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In _Proceedings of the AAAI conference on artificial intelligence_,
volume 32, 2018.


Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional
models great on long sequence modeling? _arXiv preprint arXiv:2210.09298_, 2022.


Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,
Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformermamba language model. _arXiv preprint arXiv:2403.19887_, 2024.


Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space
models are amortized online learners. _arXiv preprint arXiv:2407.14207_, 2024.


Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. _arXiv preprint_
_arXiv:2209.10655_, 2022.


Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke
Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference
with unlimited context length. _arXiv preprint arXiv:2404.08801_, 2024.


William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models.
_arXiv preprint arXiv:2404.08819_, 2024.


Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
In _International Conference on Pattern Recognition (ICPR)_, 2021.


Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.
_arXiv preprint arXiv:2209.11895_, 2022.


13


Published as a conference paper at ICLR 2025


Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In _International Conference on Learning Representations (ICLR)_, 2019.


Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on
in-context learning tasks. _arXiv preprint arXiv:2402.04248_, 2024.


Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin
Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the
transformer era. _arXiv preprint arXiv:2305.13048_, 2023.


Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene
Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with
matrix-valued states and dynamic recurrence. _arXiv preprint arXiv:2404.05892_, 2024.


Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. _arXiv_
_preprint arXiv:2211.09527_, 2022.


Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional
language models. In _International Conference on Machine Learning_, pp. 28043–28078. PMLR,
2023.


Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian
Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and
scaling of hybrid architectures. _arXiv preprint arXiv:2403.17844_, 2024.


Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. _arXiv preprint arXiv:2108.12409_, 2021.


Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong.
Hgrn2: Gated linear rnns with state expansion. _arXiv preprint arXiv:2404.07904_, 2024.


Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.


Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.


Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text
transformer. _J. Mach. Learn. Res._, 21(140):1–67, 2020.


Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and
James T Kwok. Revisiting over-smoothing in bert from the perspective of graph. _arXiv preprint_
_arXiv:2202.08625_, 2022.


Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.


Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei
Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive
hidden states. _arXiv preprint arXiv:2407.04620_, 2024.


Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary,
Xia Song, and Furu Wei. A length-extrapolatable transformer. _arXiv preprint arXiv:2212.10554_,
2022.


Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and
Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv preprint_
_arXiv:2307.08621_, 2023.


Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
_Advances in neural information processing systems_, 27, 2014.


14


Published as a conference paper at ICLR 2025


Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. _arXiv preprint arXiv:2011.04006_, 2020.


Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M
Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. _arXiv preprint_
_arXiv:2111.14522_, 2021.


Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unified understanding of transformer’s attention via the lens of
kernel. _arXiv preprint arXiv:1908.11775_, 2019.


Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. _arXiv_
_preprint arXiv:1804.04849_, 2018.


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is All You Need. In _Proceedings of NeurIPS_, 2017.


Aaron Voelker, Ivana Kaji´c, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. _Advances in neural information processing systems_, 32,
2019.


Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert
Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. _arXiv preprint arXiv:2406.07887_, 2024.


Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep
vision transformers via the fourier domain analysis: From theory to practice. _arXiv preprint_
_arXiv:2203.05962_, 2022.


Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal
approximators with exponential decaying memory. _Advances in Neural Information Processing_
_Systems_, 36, 2024.


Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. A non-asymptotic analysis of
oversmoothing in graph neural networks. _arXiv preprint arXiv:2212.10701_, 2022.


Xinyi Wu, Amir Ajorlou, Yifei Wang, Stefanie Jegelka, and Ali Jadbabaie. On the role of attention
masks and layernorm in transformers. _arXiv preprint arXiv:2405.18781_, 2024a.


Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Demystifying oversmoothing in attentionbased graph neural networks. _Advances in Neural Information Processing Systems_, 36, 2024b.


Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
language models with attention sinks. _arXiv preprint arXiv:2309.17453_, 2023.


Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention
transformers with hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.


Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers
with the delta rule over sequence length. _arXiv preprint arXiv:2406.06484_, 2024.


Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Ré. The hedgehog & the
porcupine: Expressive linear attentions with softmax mimicry. _arXiv preprint arXiv:2402.04347_,
2024a.


Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, and Shuicheng Yan. Point could mamba: Point
cloud learning via state space model. _arXiv preprint arXiv:2403.00762_, 2024b.


Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision
mamba: Efficient visual representation learning with bidirectional state space model. _arXiv preprint_
_arXiv:2401.09417_, 2024.


Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial
attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.


15


Published as a conference paper at ICLR 2025


A OTHER RELATED WORK


Despite the empirical success of SSMs in various long-range applications (Lieber et al., 2024; Zhu
et al., 2024; Zhang et al., 2024b), their theoretical properties and limitations remain underexplored.
Arora et al. (2023) leverages associative recall tasks to theoretically analyze the expressiveness of
convolutional SSMs, while advocating for input-dependent kernels. Jelassi et al. (2024) separates the
representation capacity of transformers and SSMs via coping tasks. Recent work by Merrill et al.
(2024) identifies failure modes of SSMs in state-tracking problems through circuit complexity theory.
Hahn (2020) characterize the expressivity of SSMs using linear controlled differential equations,
and Ali et al. (2024) reveal structural similarities between selective SSMs and attention mechanisms.
While these works provide valuable insights consistent with our findings, none directly examine the
long-range modeling capability of SSMs. Ben-Kish et al. (2024) points out that the product of gating
matrices (formalized in our Lemma D.1) exhibits locality issues, potentially hindering their ability to
model long-range dependencies. However, their analysis lacks a formal justification.


The most relevant prior work to our study on recency bias is perhaps Wang & Xue (2024), in which
Their Theorem 3.13 reveals exponentially decaying memory for SSMs. Distinct from their findings,
our primary contribution lies in analyzing nonlinearity and input-dependent mechanisms widely
adopted in modern SSMs (Gu & Dao, 2023; Yang et al., 2023; Arora et al., 2023). To the best of
our knowledge, our work presents the first counterargument showing that even with input-dependent
SSMs, effective context filtering may not be achieved. Instead, these mechanisms impose a strict
recency bias, inheriting the limitations of linear SSMs as previously highlighted by Wang & Xue
(2024). In particular, Wang & Xue (2024) only addresses non-linear activations after performing
linear SSMs ( _i.e._, S4), with the state transition matrix _**A**_ _t_ being independent of inputs. Our analysis,
however, considers both _**A**_ _t_ and _**b**_ _t_ as input-dependent, aligning with settings in Mamba and other
follow-up works. The approach in Wang & Xue (2024) does not naturally extend to this case, as
they assume the linearity of the sequence mixer (cf. Eq. 9 in Wang & Xue (2024)). Furthermore, we
conduct a finer-grained analysis, quantitatively relating the decay rate to the specific values within the
input-dependent gating matrix _**A**_ _t_ . To our best knowledge, none prior work revealed the smoothening
nature of SSM operators.


B UNIFIED FORMULATION OF SSMS


Linear Attention Models (LAMs) represent a vast family of architectures evolving rapidly. We present
a reformulation of several representative LAMs using the SSM framework described in Eq. 1. This
reformulation is expected to extend to models not explicitly covered below, including but not limited
to Megalodon (Ma et al., 2022; 2024), Hyena (Poli et al., 2023), HGRN2 (Qin et al., 2024), TTT
(Sun et al., 2024), Longhorn (Liu et al., 2024), xLSTM (Beck et al., 2024), and minLSTMs/minGRUs
(Feng et al., 2024).


**Linear Attention.** Attention notoriously suffers from quadratic computation and memory complexities. To address this limitation, Tsai et al. (2019); Katharopoulos et al. (2020) finds that if we express
self-attention as a linear dot-product of kernel feature maps, then we can interchange the order of
matrix products to achieve a linear time complexity. This finding leads to a series of LAMs. Ignoring
the denominator therein, we can formalize linear attention via the recurrence in Eq. 1:


(LA) _**A**_ _t_ = _**I**_ _,_ _**b**_ _t_ ( _**x**_ _t_ ) = _**k**_ ( _**x**_ _t_ ) _,_ _ct_ ( _**h**_ _t_ ) = _**q**_ ( _**x**_ _t_ ) _[⊤]_ _**h**_ _t,_ ∆ _t_ = _v_ ( _**x**_ _t_ ) _,_


where _**k**_ _,_ _**q**_ : R _→_ R _[N]_ are the kernel feature maps and _v_ : R _→_ R transforms the input features.
Considering multi-channel inputs, _**k**_, _**q**_, and _v_ become functions processing vectors, and for each
channel, the _**k**_ and _**q**_ are shared, while ∆ _t_ being specified individually. The roles of _**k**_ _,_ _**q**_ _, v_ are similar
to the key, query, and value transformations in standard transformer (Vaswani et al., 2017). While
_**k**_ _,_ _**q**_ _, v_ are often chosen as linear mappings, many other options exist. Katharopoulos et al. (2020)
adds 1+ELU after linear mapping, Choromanski et al. (2020) leverages orthogonal random Fourier
features with hyperbolic cosine as activations, and Zhang et al. (2024a) finds MLP with exponential
activation effective.


**Retentive Networks (RetNet).** RetNet (Sun et al., 2023) is another variant of LAMs, proposed as a
successor to transformers given its remarkable performance. Each layer of RetNet consists of a key,


16


Published as a conference paper at ICLR 2025


query, and value transformation, akin to linear attention. In addition, it imposes a new decaying term
over the past states. We find RetNet can be summarized via our formulation in Eq. 1:


(RetNet) _**A**_ _t_ = _γ_ _**I**_ _,_ _**b**_ _t_ ( _**x**_ _t_ ) = _**k**_ ( _**x**_ _t_ ) _,_ _ct_ ( _**h**_ _t_ ) = _**q**_ ( _**x**_ _t_ ) _[⊤]_ _**h**_ _t,_ ∆ _t_ = _v_ ( _**x**_ _t_ ) _,_


where _γ ∈_ (0 _,_ 1) is a (learnable) scalar, _**k**_ _,_ _**q**_ : R _→_ R _[N]_, _v_ : R _→_ R are linear functions. Similar to
Mamba (Gu & Dao, 2023), RetNet shares _**b**_ _t_ and _ct_ across channels while assigning distinct ∆ _t_ for
each channel when handling multi-channel inputs.


**Gated Linear Attention (GLA).** GLA (Yang et al., 2023) introduces gating mechanism, originally
from RNNs (Van Der Westhuizen & Lasenby, 2018), to LAMs. Its computational mechanism can be
encompassed by our formulation in Eq. 1:


(GLA) _**A**_ _t_ = diag( _**α**_ ( _**x**_ _t_ )) _,_ _**b**_ _t_ ( _**x**_ _t_ ) = _**k**_ ( _**x**_ _t_ ) _,_ _ct_ ( _**h**_ _t_ ) = _**q**_ ( _**x**_ _t_ ) _[⊤]_ _**h**_ _t,_ ∆ _t_ = _v_ ( _**x**_ _t_ ) _,_


where _**α**_ : R _→_ (0 _,_ 1) _[N]_ converts input to gating logits, _**k**_ _,_ _**q**_ : R _→_ R _[N]_, _v_ : R _→_ R are linear
key, query, value mappings. When the inputs are multi-dimensional, we share _**α**_ across channels
while assigning each channel with a separate _**k**_ _,_ _**q**_ _, v_ and extending their input dimension accordingly.
Linear attention (Katharopoulos et al., 2020) can be regarded as GLA with constant _**A**_ _t_, while RetNet
(Sun et al., 2023) can be formulated as GLA with input-independent _**A**_ _t_ (Liu et al., 2024).


**RWKV.** RWKV is a series of models that linearize attention computation (Peng et al., 2023; 2024).
We focus on RWKV-4 (Peng et al., 2023) and demonstrate it can also be reformulated into the
structure of SSMs:


exp( _−w_ ) _**I**_
_**A**_ _t_ = _**b**_ _t_ ( _**x**_ _t_ ) = _**v**_ ( _**x**_ _t_ ) _,_
exp( _−w_ ) + exp( _k_ ( _**x**_ _t_ )) _[,]_
(RWKV)

exp( _k_ ( _**x**_ _t_ ))
_ct_ ( _**h**_ _t_ ) = _**q**_ ( _**x**_ _t_ ) _[⊤]_ _**h**_ _t,_ ∆ _t_ =
exp( _−w_ ) + exp( _k_ ( _**x**_ _t_ )) _[,]_


where _w ∈_ R+ is a learnable coefficient, _k_ : R _→_ R and _**q**_ _,_ _**v**_ : R _→_ R _[N]_ are linear mappings. If the
inputs are vector-valued, _w_ becomes a vector while _k,_ _**q**_ _,_ _**v**_ turns into functions that take vectors as
inputs. One important property of RWKV is that ( _**A**_ _t_ ) _n,n_ + ∆ _t_ = 1. Later in RWKV-5 and RWKV-6
(Peng et al., 2024), the normalizer is removed for numerical stability.


**Griffin.** The recurrent unit in Griffin (De et al., 2024) can be re-formulated as a kind of SSMs:



��
(Griffin) _**A**_ _t_ = diag ( _**α**_ ( _**x**_ _t_ )) _,_ _**b**_ _t_ ( _**x**_ _t_ ) = diag ( _**i**_ ( _**x**_ _t_ )) _,_ _ct_ ( _**h**_ _t_ ) = _**h**_ _t,_ ∆ _t_ = diag




    1 _−_ _**α**_ ( _**x**_ _t_ ) [2] _,_



where _**i**_ ( _**x**_ _t_ ) = sigmoid( _**W**_ _x_ _**x**_ _t_ + _**b**_ _x_ ) is an input gate, _**α**_ is computed in log-space: log _**α**_ ( _**x**_ _t_ ) =
_−ξ_ softplus( **Γ** ) _⊙_ sigmoid( _**W**_ _a_ _**x**_ _t_ + _**b**_ _a_ ), _⊙_ is Hadamard product, _ξ_ is a constant, and **Γ**, _**W**_ _a_,
_**b**_ _a_, _**W**_ _x_, _**b**_ _x_ are learnable parameters. In particular, the dimension of _**h**_ _t_ in Griffin is equal to the
dimension of _**x**_ _t_ . If we consider single-channel _**x**_ _t_, then _**A**_ _t_, _**b**_ _t_, and ∆ _t_ are all scalar-valued.


C DEFERRED DISCUSSIONS


**Extended discussion with HiPPO theory.** HiPPO established in (Gu et al., 2020), extended
by Gu et al. (2021b; 2022b) is the theoretical foundation of SSMs. Consider a signal _x_ and its
reconstruction _y_ [(] _[t]_ [)] up to time _t_ . To optimally memorizes the history of _x_ using _y_ [(] _[t]_ [)], HiPPO minimizes
_∥x≤t −_ _y_ [(] _[t]_ [)] _∥L_ 2( _ω_ ( _t_ )) w.r.t. a measure _ω_ [(] _[t]_ [)] supported on ( _−∞, t_ ]. The solution is to project the history
of _x_ before time _t_ onto _N_ basis functions (e.g. Legendre polynomials), which yields a time-continuous
coefficient vector _**h**_ ( _t_ ), and _y_ [(] _[t]_ [)] can be synthesized by linearly combining the _N_ basis using _**h**_ ( _t_ ).
Gu et al. (2020) shows that the evolution of _**h**_ ( _t_ ) follows an ODE _**h**_ _[′]_ ( _t_ ) = _**A**_ ( _t_ ) _**h**_ ( _t_ ) + _**b**_ ( _t_ ) _x_ ( _t_ ). In
particular, Gu et al. (2020) chooses a uniform measure over the past history _ω_ [(] _[t]_ [)] = I[0 _, t_ ] _/t_, which
places no approximation bias over the time horizon in contrast to an earlier work (Voelker et al., 2019).
As a result, _**A**_ ( _t_ ) in Eq. 1 can be written in a closed form: _**A**_ ( _t_ ) = _−_ _**A**_ _hippo/t_, where _**A**_ _hippo_ is a
time-independent constant called the HiPPO matrix. Its various forms have been used as initialization
in subsequent works including S4 (Gu et al., 2021a) and Mamba (Gu & Dao, 2023). While HiPPO
theory seems to guarantee the long-rangeness for SSMs, the actual form of _**A**_ ( _t_ ) employed in S4 and


17


Published as a conference paper at ICLR 2025


Mamba drops the normalizer 1 _/t_ . Gu et al. (2022b) shows that this change causes a warp of measure
from uniform to _ω_ [(] _[t]_ [)] ( _s_ ) _∝_ exp( _s −_ _t_ )I( _∞, t_ ]. We note that this warped measure assigns more
importance to recent history, and thus, our Theorem 3.1 does not contradict HiPPO theory but also
matches the findings in Gu et al. (2022b). We also point out that when adopting the diagonalized form
of _**A**_ _hippo_ (Gu et al., 2021a; Gupta et al., 2022; Gu et al., 2022a), the unitary matrices decomposed
from _**A**_ _hippo_ is sometimes not applied to _**b**_ _t_ and _**c**_ _t_, which introduces a disconnect between HiPPO
theory and its practical implementation. Our paper directly studies the discrete-domain SSMs and
aligns with the parameterization used in practice. Another less-discussed property of SSMs is their
approximation power for a broad family of operators. In Gu et al. (2021b), SSMs are shown to possess
expressiveness that encompasses both convolutions and RNNs. It is worth noting that the original
HiPPO-based SSM is not necessarily a low-pass filter. Rather, it is the successive simplifications in
the parameterization of _**A**_ _t_ that impart the smoothing characteristics of SSMs (see Proposition 4.1).


**Does hungry hungry hippos help?** The key innovation of Hungry Hungry Hippos (H3) (Fu et al.,
2022) lies in the introduction of self-gating connections and locally shifting convolutions to improve
in-context recall for state space models (SSMs). This design has quickly become a standard backbone
for various SSMs (Gu & Dao, 2023; Beck et al., 2024). However, we question its effectiveness in
addressing the local rangeness issue in SSMs. The gating mechanism operates at the token level,
which impacts the bound in Theorem 3.1 only by a constant factor. Additionally, the introduced
convolutions typically use small kernels, which are insufficient to mitigate the exponentially decaying
relevance between tokens. As we empirically show in Fig. 2, while Mamba with H3 performs
adequately in associative recall tasks when the state size is sufficiently large, a locality bias begins to
emerge as the number of key-value pairs exceeds the model’s memory capacity. This highlights the
limitations of the architecture in handling long-range dependencies under constrained memory.


**Does gradient vanish in SSMs?** Vanishing gradients refer to a challenge in RNNs, where
backpropagation-based learning is impeded due to gradient magnitudes decaying exponentially
over time. The diminishing dependencies among distant tokens are a fundamental cause of this
issue (Bengio et al., 1994). SSMs were initially proposed to address this limitation by explicitly
modeling long-range dependencies, as highlighted in (Gu et al., 2020; 2021b). Subsequent work,
such as Mamba, extends this approach by adopting their initialization alongside newly proposed
selection mechanisms, which are widely believed to enhance these capabilities. Our Theorem 3.1
lies in theoretically challenging this assumption. We demonstrate that modern SSMs still suffer from
the recency bias, which not only undermines their ability to capture long-term dependencies but also
potentially exacerbates the vanishing gradient problem.


**Connection with over-squashing theory in GNNs.** We can compare recency of SSMs to oversquashing in GNNs. The influential score defined in Sec. 3.1 is also used for _over-squashing_
analysis in Graph Neural Networks (GNNs) to identify information bottleneck (Topping et al.,
2021; Di Giovanni et al., 2023). The sensitivity analysis in Topping et al. (2021) demonstrates
similar exponentially decaying dependencies among graph nodes, dependent on the underlying graph
topology. We postulate that propagating information from long distances remains challenging for
SSMs because the model needs to encapsulate all history information into a fixed-dimension hidden
vector, which is also observed as one major problem with RNNs (Bengio et al., 1994; Alon & Yahav,
2020; Sutskever et al., 2014; Cho et al., 2014; Cho, 2014).


**Connection with over-smoothing theory in GNNs and transformers.** Over-smoothing issues
were first identified in GNNs (Li et al., 2018; NT & Maehara, 2021; Oono & Suzuki, 2019; Cai
& Wang, 2020; Wu et al., 2022; 2024b) and later explored in transformers (Dong et al., 2021;
Wang et al., 2022; Shi et al., 2022; Wu et al., 2024a; Geshkovski et al., 2023). In both cases,
over-smoothing manifests as feature representations becoming increasingly uniform with greater
model depth. To the best of our knowledge, our work is the first to uncover this phenomenon in
SSMs. In GNNs, the over-smoothing effect typically follows a linear decay rate governed by the
second-largest eigenvalue of the adjacency matrix: _O_ ( _λ_ _[L]_ 2 [)][, where] _[ L]_ [ denotes the number of layers]
(Oono & Suzuki, 2019; Cai & Wang, 2020). More closely related to our setting, Wu et al. (2024a)
analyzes the convergence rate of feature smoothness in causal attention, demonstrating a rate of
_O_ ((1 _−_ _A_ [ˆ] _[T]_ _min_ [)] _[L/T]_ [ )][, where][ ˆ] _[A][min]_ [ is the minimal value in attention maps. In comparison, our result]
in Theorem 4.2 shows a rate of _O_ ((1 _−_ _A_ _[T]_ _min_ [)] _[L]_ [)][, indicating that the smoothening speed of SSMs is]


18


Published as a conference paper at ICLR 2025


faster than that of transformers. Our Fig. 5 supports this claim, as the transformer-based architecture
exhibits a mild decrease of sharpness at deeper layers, whereas SSMs show a constantly steeper
decay slope. Moreover, due to the inherent locality of SSMs (Theorem 3.1), achieving effective
long-range interactions necessitates deeper architectures. In contrast, transformers, which allow
arbitrary long-range interactions among tokens, do not have the same requirement. This distinction is
partially supported by the observation that modern SSMs often adopt architectures that are roughly
twice as deep as transformers. Consequently, depth-scaling limitations are more critical for SSMs
than for transformers.


D PROOFS


D.1 EXPONENTIALLY DECAYING DEPENDENCY WITH RELATIVE DISTANCE


In this section, we extend and prove Theorem 3.1 in a more general case where inputs and parameters
are all complex-valued. Below we by default assume that _**A**_ _t ∈_ C _[N]_ _[×][N]_, _**b**_ _t_ : C _→_ C _[N]_, _ct_ : C _[N]_ _→_ C,
and ∆ _t ∈_ R. First of all, we present the following auxiliary lemma, reformulating SSM recurrence in
an explicit parallel form.

**Lemma D.1** (Parallel form) **.** _For any {_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _and_ _**x**_ _∈_ C _[T]_ _,_ _**y**_ _∈_ C _[T]_ _computed via an_
_SSM defined in Eq. 1 is equal to:_




- _t_

 - _**A**_ _r_


_r_ = _s_ +1







∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) + ∆ _t_ _**b**_ _t_ ( _**x**_ _t_ ) _,_ _**y**_ _t_ = _ct_ ( _**h**_ _t_ ) _,_ _∀t ∈_ [ _T_ ] _._ (5)



_**h**_ _t_ =



_t−_ 1



_s_ =1




- _t_

 


_Proof._ Proof by induction. First let us examine the base case when _t_ = 1: _**h**_ 1 = ∆1 _**b**_ 1( _**x**_ 1),
indicating that Eq. 5 holds trivially. Now we make inductive hypothesis that Eq. 5 holds for some
_t ∈_ [ _T −_ 1]. Then for time step _t_ + 1 _∈_ [ _T_ ]:


_**h**_ _t_ +1 = _**A**_ _t_ +1 _**h**_ + ∆ _t_ +1 _**b**_ _t_ +1( _**x**_ _t_ +1)




- _t_

 - _**A**_ _r_


_r_ = _s_ +1








      
∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) + ∆ _t_ _**b**_ _t_ ( _**x**_ _t_ )



+ ∆ _t_ +1 _**b**_ _t_ +1( _**x**_ _t_ +1)



= _**A**_ _t_ +1




- _t−_ 1

 

_s_ =1




- _t_

 



- _t_ +1

 - _**A**_ _r_


_r_ = _s_ +1


- _t_ +1

 - _**A**_ _r_


_r_ = _s_ +1



_t_

- _**A**_ _r_


_r_ = _s_ +1







∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) + _**A**_ _t_ +1∆ _t_ _**b**_ _t_ ( _**x**_ _t_ ) + ∆ _t_ +1 _**b**_ _t_ +1( _**x**_ _t_ +1)



=


=


=



_t−_ 1



_s_ =1


_t−_ 1



_s_ =1


_t_



_s_ =1





_**A**_ _t_ +1 _·_



_t_




∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) + ∆ _t_ +1 _**b**_ _t_ +1( _**x**_ _t_ +1) _,_













∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) +




- _t_ +1

 - _**A**_ _r_


_r_ = _t_ +1



∆ _t_ _**b**_ _t_ ( _**x**_ _t_ ) + ∆ _t_ +1 _**b**_ _t_ +1( _**x**_ _t_ +1)



which also satisfies Eq. 5. Then we conclude the proof by induction.


**A remark of Lemma D.1.** Lemma D.1 provides an alternative perspective on how SSMs compute
the outputs. The predicted value for the _t_ -th token is obtained via decoding a weighted aggregation
over representations of all past tokens. The encoding and decoding stage is element-wise independent
of the context. Whereas, the “weight” associated with each past token in the summation reflects the
pairwise relationship, playing a similar role to attention weights in transformers (Dao & Gu, 2024;
Ali et al., 2024). The weight corresponding to one past token is calculated as the cumulative product


_r_ _**[A]**_ _[r]_ [, where] _[ r][ ∈]_ [[] _[s]_ [ + 1] _[, t]_ []][ traverses from the past token (at time] _[ s]_ [) to the target token (at time] _[ t]_ [).]
Assume _**A**_ _t ∈_ (0 _,_ 1) _[N]_ _[×][N]_, which is satisfied by most of SSMs discussed in Sec. 2, we can show that
( [�] _[t]_ _r_ = _s_ +1 _**[A]**_ _[r]_ [)] _[n,n][ <]_ [ (][�] _r_ _[s]_ = _s_ _[′]_ +1 _**[A]**_ _[r]_ [)] _[n,n]_ [ for any] _[ s < s][′][ < t]_ [ and] _[ n][ ∈]_ [[] _[N]_ []][. By this interpretation, SSMs]
assign strictly higher “attention” to the nearer tokens than the further tokens.


Below we formally state and prove the complex version of Theorem 3.1. Theorem 3.1 is a straightforward corollary of Theorem D.2.


19


Published as a conference paper at ICLR 2025


**Theorem D.2** (Recency of SSMs) **.** _Consider an SSM defined in Eq. 1 with {_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _._
_Assume that:_


_(i) The input space X ⊂_ C _[T]_ _is compact._



_(ii) {_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _and_ �� _∂∂_ _**Ax**_ _t_




_[∂]_ _**[b]**_ _[t]_ _[∂c][t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_



_∂_ _**x**_ _t_



_∂_ _**A**_ _t_ _[∂]_ _**[b]**_ _[t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_ _t_




_[∂c][t]_ _[∂]_ [∆] _[t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_ _t_



��

_t∈_ [ _T_ ] _[are continuous.]_



��



_(iii)_ _**A**_ _t is diagonal and_ 0 _< |_ ( _**A**_ _t_ ) _n,n| <_ 1 _for all t ∈_ [ _T_ ] _, n ∈_ [ _N_ ] _._


_Let Amax_ = max _t∈_ [ _T_ ] _,n∈_ [ _N_ ] _|_ ( _**A**_ _t_ ) _n,n|. Then for arbitrary_ _**x**_ _∈X and every s, t ∈_ [ _T_ ] _such that_
_s < t,_

_∂_ _**y**_ _t_ _≤_ _C_ exp ( _−κ_ ( _t −_ _s_ )) _,_
���� _∂_ _**x**_ _s_ ����


_for some constant C >_ 0 _independent of t and s, and κ_ = log( _A_ _[−]_ _max_ [1] [)] _[.]_



_Proof of Theorem 3.1._ First of all, we note that by compactness of input space (Assumption (i)) and
continuity (Assumption (ii)), ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) and - _∂_ _**A**_ _t_ _[∂]_ _**[b]**_ _[t]_ _[∂c][t]_ _[∂]_ [∆] _[t]_ - are all bounded by some

_[,]_ _[,]_ _[,]_



_∂_ _**A**_ _t_ _[∂]_ _**[b]**_ _[t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_ _t_




_[∂]_ _**[b]**_ _[t]_ _[∂c][t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_




_[∂c][t]_ _[∂]_ [∆] _[t]_

_∂_ _**x**_ _t_ _[,]_ _∂_ _**x**_ _t_



_∂_ _**x**_ _t_




are all bounded by some



constant for every _t ∈_ [ _T_ ].


By Lemma D.1, Eq. 1 can be expressed in the closed form as follows:




- _t_

 - _**A**_ _r_


_r_ = _i_ +1







∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ) + ∆ _t_ _**b**_ _t_ ( _**x**_ _t_ )


  


_**h**_ _t_ =


=


=



_t−_ 1



_i_ =1



_t−_ 1

- exp


_i_ =1


_s−_ 1

- exp


_i_ =1




- _t_

 


∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )




- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 


log _**A**_ _r_

_r_ = _i_ +1



∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ) + ∆ _t_ _**b**_ _t_ ( _**x**_ _t_ )







∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )








- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 


+∆ _t_ _**b**_ _t_ ( _**x**_ _t_ ) _,_



+



_t−_ 1

- exp


_i_ = _s_




~~�~~ ~~��~~ ~~�~~
_**u**_ _t_




~~�~~ - ~~�~~ ~~�~~
_**v**_ _t_



where the second equality is by simply rewriting the cumulative product as an exponential of
cumulative summation of logarithms. The logarithmic terms are well defined and constantly negative
due to Assumption (iii). We decompose _**h**_ _t_ into three components, where the first term, denoted
as _**u**_ _t_, depends on _**x**_ _s_ only through _{_ _**A**_ _r}_ _[t]_ _r_ = _s_ [, the second term, denoted as] _**[ v]**_ _[t]_ [, only relies on] _**[ x]**_ _[s]_ [via]
_{_ ∆ _i_ _**b**_ _i}_ _[t]_ _i_ = _s_ [, while the remaining part is independent of] _**[ x]**_ _[s]_ [.]


Now we tackle each component separately. First, we simplify _∂_ _**u**_ _t/∂_ _**x**_ _s_ as:








    
∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )



_∂_ _**u**_ _t_ _∂_
=
_∂_ _**x**_ _s_ _∂_ _**x**_ _s_




- _s−_ 1

- exp


_i_ =1




- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 



- _t_

 - log _**A**_ _r_


_r_ = _i_ +1



��

∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )



=


=


=



_s−_ 1

- _∂_

_∂_ _**x**_ _s_

_i_ =1



_s−_ 1

- exp


_i_ =1


_s−_ 1

- exp


_i_ =1





exp




- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 


�� _t_

 

_r_ = _i_ +1



_∂_ log _**A**_ _r_

_∂_ _**x**_ _s_







∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )




- _t_

 


��
_∂_ _**A**_ _s_
_**A**_ _[−]_ _s_ [1] _∂_ _**x**_ _s_



��
_∂_ _**A**_ _s_
_**A**_ _[−]_ _s_ [1]




∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ) _,_



20


Published as a conference paper at ICLR 2025



where the last steps follow from the basic chain rule. Then we can bound its _ℓ_ 1 norm by:
_∂_ _**u**_ _t_ _≤_      - _N_      - _s−_ 1 exp      -      - _t_ log _**A**_ _r_      - _·_      - 1 _∂_ _**A**_ _r_      - ∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ) _n_
���� _∂_ _**x**_ _s_ ���� ������ ������ ����� _**A**_ _r_ _∂_ _**x**_ _s_ �����




- _t_

 







- 1



_s−_ 1




_≤_
����1



_N_




_n_ =1







∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ) _n_
_n,n_



(6)
�����



_n,n_



_∂_ _**A**_ _r_

_∂_ _**x**_ _s_



_i_ =1



exp
������



log _**A**_ _r_

_r_ = _i_ +1




_·_
������



�����



_**A**_ _r_




- _t_

 


log _|_ ( _**A**_ _r_ ) _n,n|_

_r_ = _i_ +1







(7)



_≤_ _C_ 1



_N_



_n_ =1



_s−_ 1

- exp


_i_ =1



_s−_ 1




_≤_ _C_ 1 _N_



_s−_ 1

- exp (log _Amax_ ( _t −_ _i_ )) (8)


_i_ =1



= _C_ 1 _N_ [1] _[ −]_ [exp (] _[−]_ [log] _[ A][max]_ [(] _[s][ −]_ [1))] exp (log _Amax_ ( _t −_ 1)) (9)

1 _−_ _A_ _[−]_ _max_ [1]



= _C_ 1 _N_ [exp (log] _[ A][max]_ [(] _[t][ −]_ _[s]_ [))] _[ −]_ [exp (log] _[ A][max]_ [(] _[t][ −]_ [1))]

_A_ _[−]_ _max_ [1] _[−]_ [1]



_≤_ _C_ 1 _N_      - _−_ log _A_ _[−]_ _max_ [1] [(] _[t][ −]_ _[s]_ [)]      - _._ (10)
_A_ _[−]_ _max_ [1] _[−]_ [1 exp]

We elaborate on the derivation step by step. Eq. 6 is due to triangular inequality. To obtain Eq. 7, we
note that _**A**_ _[−]_ _r_ [1] _[, ∂]_ _**[A]**_ _[r][/∂]_ _**[x]**_ _[s][,]_ [ ∆] _[i][,]_ _**[ b]**_ _[i]_ [are all element-wisely bounded, thus, we can extract their uniform]
upper bound _C_ 1 _>_ 0 out of the summation Eq. 8 can be derived by applying the supremum _Amax_
over all _{|_ ( _**A**_ _t_ ) _n,n|}t∈_ [ _T_ ] _,n∈_ [ _N_ ]. Eq. 9 follows from the summation of geometric series. And finally,
inequality in Eq. 10 holds by dropping a negative term.


Similarly, we rewrite _∂_ _**v**_ _t/∂_ _**x**_ _s_ as below:








    
∆ _i_ _**b**_ _i_ ( _**x**_ _i_ )



_∂_ _**v**_ _t_ _∂_
=
_∂_ _**x**_ _s_ _∂_ _**x**_ _s_




- _t−_ 1

- exp


_i_ = _s_




- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




- _t_

 



- _t_

 - log _**A**_ _r_


_r_ = _i_ +1




_∂_ (∆ _i_ _**b**_ _i_ ( _**x**_ _i_ ))

_∂_ _**x**_ _s_



=



_t−_ 1

- exp


_i_ = _s_




- _t_

 



- _t_

 - log _**A**_ _r_


_r_ = _s_ +1




- _t_

 



_∂_ (∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ))

_,_
_∂_ _**x**_ _s_




_∂_ (∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ))



= exp



by which we can yield the following upper bound on its _ℓ_ 1 norm:


_N_           - _t_           
_∂_ _**v**_ _t_ _≤_         - exp         - log _**A**_ _r_ _·_
���� _∂_ _**x**_ _s_ ���� ����

������ ������



_∂_ _**x**_ _s_








_·_ _∂_ (∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) _n_ )
���� _∂_ _**x**_ _s_
������



_N_





- _t_

 


_≤_
����1



_n,n_



����



_n_ =1



exp
������



log _**A**_ _r_

_r_ = _s_ +1



(11)




- _t_

 


log _|_ ( _**A**_ _r_ ) _n,n|_

_r_ = _s_ +1







_≤_ _C_ 2



_N_

- exp


_n_ =1



_N_




_≤_ _NC_ 2 exp        - _−_ log _A_ _[−]_ _max_ [1] [(] _[t][ −]_ _[s]_ [)]        - _,_ (12)

where Eq. 11 is obtained by applying uniform upper bound _C_ 2 _>_ 0 over _|∂_ (∆ _s_ _**b**_ _s_ ( _**x**_ _s_ ) _n_ ) _/∂_ _**x**_ _s|_ . Eq.
12 is induced by leveraging the supremum _Amax_ over all _{|_ ( _**A**_ _t_ ) _n,n|}t∈_ [ _T_ ] _,n∈_ [ _N_ ].



Combining Eqs. 10 and 12, we have:
_∂_ _**h**_ _t_ _≤_         - _C_ 1 _N_         - exp         - _−_ log _A_ _[−]_ _max_ [1] [(] _[t][ −]_ _[s]_ [)]         - _._ (13)
���� _∂_ _**x**_ _s_ ����1 _A_ _[−]_ _max_ [1] _[−]_ [1 +] _[ NC]_ [2]



Finally, we conclude the proof based on Eq. 13:


_⊤_

_∂_ _**y**_ _t_ _∂ct_ ( _**h**_ _t_ ) _∂_ _**h**_ _t_

= _≤_

���� _∂_ _**x**_ _s_ ���� ����� _∂_ _**h**_ _t_ _∂_ _**x**_ _s_ �����



_∂_ _**h**_ _t_



=
����



_∂ct_ ( _**h**_ _t_ )
����� _∂_ _**h**_ _t_



_∂_ _**h**_ _t_



_⊤_
_∂_ _**h**_ _t_



_∂_ _**h**_ _t_
���� _∂_ _**x**_ _s_



����1



_∂_ _**x**_ _s_



_≤_ _∂ct_ ( _**h**_ _t_ )
����� ���� _∂_ _**h**_ _t_



���� _∞_



_≤_ _C_ 3 - _C_ 1 _N_

_A_ _[−]_ _max_ [1] _[−]_ [1] [+] _[ NC]_ [2]

= _C_ exp ( _−κ_ ( _t −_ _s_ ))


21




exp  - _−_ log _A_ _[−]_ _max_ [1] [(] _[t][ −]_ _[s]_ [)]  - _,_


Published as a conference paper at ICLR 2025


where we use Hölder inequality in the first equation and upper bound _∥∂ct_ ( _**h**_ _t_ ) _/∂_ _**h**_ _t∥∞_ again via
constant _C_ 3 _>_ 0 due to continuity ( _ct_ ( _**h**_ _t_ ) is a composition of a series of continuous functions as in
Assumption (ii)). This is as desired after letting _C >_ 0 absorb all constants and _κ_ = log _A_ _[−]_ _max_ [1] [.]


D.2 OVER-SMOOTHING IN SSMS


D.2.1 LOW-PASS FILTERING PROPERTIES


In this section, we formally state and prove Proposition 4.1. To begin with, we define low-pass filters
as below:

**Definition D.3** (Low-pass Filter) **.** _Suppose z_ ( _t_ ) : C _→_ C _is absolutely integrable. Then the Fourier_
_transform Z_ ( _ω_ ) : C _→_ C _exists and Z_ ( _ω_ ) ≜ - _z_ ( _t_ ) exp( _−_ i _ωt_ ) _dt. We say z_ ( _t_ ) _is an_ ( _ϵ,_ Ω) _-low-pass_
_filter for some_ 0 _< ϵ <_ 1 _and_ Ω _>_ 0 _if |Z_ ( _ω_ ) _| ≤_ _ϵ for every |ω| ≥_ Ω _. Furthermore, we say z_ ( _t_ ) _is a_
_low-pass filter if for every_ 0 _< ϵ <_ 1 _, there exists_ Ω _>_ 0 _such that z_ ( _t_ ) _is an_ ( _ϵ,_ Ω) _-low-pass filter._


Note that our definition of low-pass filters focuses on its effect on removing high-frequency components. By Definition D.3, it is equivalent to say a system performs low-pass filtering if and only if
the responses converge to zero as the frequency increases. Fixing _ϵ >_ 0 small enough, we can say a
( _ϵ,_ Ω)-low-pass filter has cut-off frequency at _|ω|_ = Ω.


Now we present a formal version of Proposition 4.1 as below, which quantifies the cut-off frequency
of a filter induced by continuous-time S4. Note that Proposition D.4 holds for complex-valued
( _**A**_ _,_ _**b**_ _,_ _**c**_ ). Below we assume _**A**_ _∈_ C _[N]_ _[×][N]_, _**b**_ _∈_ C _[N]_, _**c**_ _∈_ C _[N]_ by default.


**Proposition D.4** (Formal version of Proposition 4.1) **.** _Consider a continuous-time S4 with parameters_
( _**A**_ _,_ _**b**_ _,_ _**c**_ ) _. Assume_ _**A**_ _∈_ C _[N]_ _[×][N]_ _is diagonal and its diagonal values have negative real parts. Then_
_**y**_ ( _t_ ) = - _**c**_ _[⊤]_ exp( _**A**_ ( _t −_ _s_ )) _**b**_ _x_ ( _s_ ) _ds is an_ ( _ϵ, O_ (1 _/ϵ_ )) _-low-pass filter for any ϵ >_ 0 _sufficiently small._


_Proof._ First of all, let us rewrite the filter induced by S4 as below:



_z_ ( _t_ ) =


Then we apply Fourier transform on _z_ ( _t_ ):



_N_

- _**c**_ _n_ _**b**_ _n_ exp( _**A**_ _n,nt_ ) _._


_n_ =1



_N_

   -    _Z_ ( _ω_ ) = _z_ ( _t_ ) _e_ _[−]_ [i] _[ωt]_ _dt_ = - _**c**_ _n_ _**b**_ _n_ exp( _**A**_ _n,nt_ ) exp( _−_ i _ωt_ ) _dt_


_n_ =1



=


=


=



_N_

- _**c**_ _n_ _**b**_ _n_


_n_ =1



_N_



_n_ =1



_**c**_ _n_ _**b**_ _n_
_,_
i _ω −_ _**A**_ _n,n_



_N_

- _**c**_ _n_ _**b**_ _n_


_n_ =1




exp( _**A**_ _n,nt_ ) exp( _−_ i _ωt_ ) _dt_


exp(( _**A**_ _n,n −_ i _ω_ ) _t_ ) _dt_



where the last integral converges due to _ℜ_ ( _**A**_ _n,n_ ) _<_ 0. Then we can upper bound the magnitude of
_Z_ ( _ω_ ) as:



_N_



_n_ =1



_N_

- _**c**_ _n_ _**b**_ _n_

���� i _ω −_ _**A**_ _n,n_

_n_ =1



_N_



_n_ =1



_|_ _**c**_ _n_ _**b**_ _n|_
_||ω| −|_ _**A**_ _n,n||_ _[.]_



_≤_
�����



_≤_
����



_|Z_ ( _ω_ ) _|_ =



�����



_**c**_ _n_ _**b**_ _n_
i _ω −_ _**A**_ _n,n_



We consider _ϵ >_ 0 small enough, thus, it is sufficient to consider the scenario when _|ω| > Amax_ ≜
max _n∈_ [ _N_ ] _|_ _**A**_ _n,n|_ . In this regime, we have:




1 _≤_ _∥_ _**b**_ _∥_ 2 _∥_ _**c**_ _∥_ 2 _,_
_|ω| −_ _Amax_ _|ω| −_ _Amax_



_|Z_ ( _ω_ ) _| ≤_




- _N_

 


_|_ _**c**_ _n_ _**b**_ _n|_

_n_ =1



22


Published as a conference paper at ICLR 2025


where the last two inequalities are due to Hölder’s inequality. It is sufficient to set:

Ω= _[∥]_ _**[b]**_ _[∥]_ [2] _[∥]_ _**[c]**_ _[∥]_ [2] + _Amax,_

_ϵ_

to let _|Z_ ( _ω_ ) _| ≤_ _ϵ_ for every _|ω| ≥_ Ω hold.


D.2.2 GENERALIZED SMOOTHENING PROPERTIES


In this section, we define generalized smoothening operators as those that narrow distances between
token features. We provide the following results that formalize this point. Theorem D.5 below extends
the result of Theorem 4.2 to include the case when ( _**A**_ _t_ ) _n,n_ + ∆ _t_ = 1 for every _n ∈_ [ _N_ ] _, t ∈_ [ _T_ ],
which is more aligned with Ma et al. (2022; 2024); Peng et al. (2023) in practice.

**Theorem D.5** (Complete version of Theorem 4.2) **.** _Consider an SSM specified in Eq. 1 with_
_{_ ( _**A**_ _t,_ _**b**_ _t, ct,_ ∆ _t_ ) _}t∈_ [ _T_ ] _. Assume an input space X ⊂_ R _[T]_ _such that for every_ _**x**_ _∈X_ _, either of the_
_following conditions is satisfied:_


_(i)_ ( _**A**_ _t_ ) _n,n_ + ∆ _t_ = 1 _for every n ∈_ [ _N_ ] _and t ∈_ [ _T_ ] _, or_


_(ii)_ ( _**A**_ _t_ ) _n,n_ + ∆ _t ≤_ 1 _for every n ∈_ [ _N_ ] _, t ∈_ [ _T_ ] _, and_ min _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n ≤_ 0 _and_
max _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n ≥_ 0 _for every n ∈_ [ _N_ ] _._


_Let Amin_ = min _t∈_ [ _T_ ] _,n∈_ [ _N_ ]( _**A**_ _t_ ) _n,n. Then for any_ _**x**_ _∈X and the memory states {_ _**h**_ _t_ : _t ∈_ [ _T_ ] _}_
_generated by the SSM, we have:_

max �1 _−_ _A_ _[T]_ _min_ _[ −]_ [1]       - max
_t,s∈_ [ _T_ ] _[∥]_ _**[h]**_ _[t][ −]_ _**[h]**_ _[s][∥][∞]_ _[≤]_ _t,s∈_ [ _T_ ] _[∥]_ _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)] _[ −]_ _**[b]**_ _[s]_ [(] _**[x]**_ _[s]_ [)] _[∥][∞]_ _[,]_


_Proof._ To simplify the proof, we first only consider the dynamics of one channel in the memory state.
We denote _αt_ = ( _**A**_ _t_ ) _n,n_, _zt_ = _**b**_ _t_ ( _**x**_ _t_ ) _n_, and _st_ = _**h**_ _t,n_ for some _n ∈_ [ _N_ ]. Additionally, we define
the following two quantities:


_m_ = min _M_ = max
_t∈_ [ _T_ ] _[z][t][,]_ _t∈_ [ _T_ ] _[z][t]_


When Assumption (ii) holds, we know that _m ≤_ 0 and _M ≥_ 0. Suppose _z_ 1 = _pm_ + (1 _−_ _p_ ) _M_ for
some _p ∈_ [0 _,_ 1]. Furthermore, let _q_ = 1 _−_ _p_ . Now we consider the following dynamics with _s_ 0 = 0:

_st_ = _αtst−_ 1 + ∆ _tzt,_ _t ∈_ [ _T_ ] _,_ (14)


Next, we can show the result below (proved later):


**Lemma D.6.** _We have the following inequality for Eq. 14:_

(1 _−_ _A_ _[t]_ _min_ _[−]_ [1] _[q]_ [)] _[m]_ [ +] _[ A][t]_ _min_ _[−]_ [1] _[qM][ ≤]_ _[s][t][ ≤]_ _[A][t]_ _min_ _[−]_ [1] _[pm]_ [ + (1] _[ −]_ _[A][t]_ _min_ _[−]_ [1] _[p]_ [)] _[M,]_ _∀t ∈_ [ _T_ ] _,_


_if either of the following conditions holds:_


_(i) αt_ + ∆ _t_ = 1 _;_


_(ii) αt_ + ∆ _t <_ 1 _and m ≤_ 0 _≤_ _M_ _._


Note that the two conditions in Lemma D.6 corresponds to Assumptions (i) and (ii) in Theorem D.5,
respectively. Thus we can upper and lower bound the minimum and maximum of memory states
_{st_ : _t ∈_ [ _T_ ] _}_ through:

_m_ _[′]_ ≜ min _min_ _[q]_ [)] _[m]_ [ +] _[ A][t]_ _min_ _[−]_ [1] _[qM]_
_t∈_ [ _T_ ] _[s][t][ ≥]_ _t_ [min] _∈_ [ _T_ ] [(1] _[ −]_ _[A][t][−]_ [1]

= (1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] _[q]_ [)] _[m]_ [ +] _[ A][T]_ _min_ _[ −]_ [1] _[qM,]_

as we are moving the convex combination towards the smaller end (i.e., 1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] _[q >]_ [ 1] _[ −]_ _[A]_ _min_ _[t][−]_ [1] _[q]_ [),]
and similarly,

_M_ _[′]_ ≜ max _min_ _[pm]_ [ + (1] _[ −]_ _[A][t]_ _min_ _[−]_ [1] _[p]_ [)] _[M]_
_t∈_ [ _T_ ] _[s][t][ ≤]_ _t_ [max] _∈_ [ _T_ ] _[A][t][−]_ [1]

_≤_ _A_ _[T]_ _min_ _[ −]_ [1] _[pm]_ [ + (1] _[ −]_ _[A]_ _min_ _[T][ −]_ [1] _[p]_ [)] _[M.]_


23


Published as a conference paper at ICLR 2025


as we are now relaxing the convex combination towards the larger end (i.e., 1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] _[p >]_ [ 1] _[−]_ _[A]_ _min_ _[t][−]_ [1] _[p]_ [)]
Henceforth, we can upper bound:

_M_ _[′]_ _−_ _m_ _[′]_ _≤_ _A_ _[T]_ _min_ _[ −]_ [1] _[pm]_ [ + (1] _[ −]_ _[A][T]_ _min_ _[ −]_ [1] _[p]_ [)] _[M][ −]_ [(1] _[ −]_ _[A]_ _min_ _[n][−]_ [1] _[q]_ [)] _[m][ −]_ _[A]_ _min_ _[n][−]_ [1] _[qM]_

= (1 _−_ _A_ _[T]_ _min_ _[ −]_ [1][)(] _[M][ −]_ _[m]_ [)] _[,]_


using the fact that _p_ + _q_ = 1.


Now we can apply the above result to all memory channels. Assigning each channel with _mn_ =
min _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n, Mn_ = max _t∈_ [ _T_ ] _**b**_ _t_ ( _**x**_ _t_ ) _n_ and _Mn_ _[′]_ _[, m]_ _n_ _[′]_ [accordingly, where] _[ n][ ∈]_ [[] _[N]_ []][. We can yield:]

max _n_ _[−]_ _[m]_ _n_ _[′]_ [)]
_t,s∈_ [ _T_ ] _[∥]_ _**[h]**_ _[t][ −]_ _**[h]**_ _[s][∥][∞]_ _[≤]_ _n_ [max] _∈_ [ _N_ ] [(] _[M][ ′]_



_≤_ max
_n∈_ [ _N_ ]



�1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] - ( _Mn −_ _mn_ )



_≤_ �1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] - max
_n∈_ [ _N_ ] [(] _[M][n][ −]_ _[m][n]_ [)]



= �1 _−_ _A_ _[T]_ _min_ _[ −]_ [1] - max
_n∈_ [ _N_ ]




- max
_t∈_ [ _T_ ] _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)] _[ −]_ _t_ [min] _∈_ [ _T_ ] _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)]



= �1 _−_ _Amin_ _[T][ −]_ [1]         - max
_n∈_ [ _N_ ] _t,s_ [max] _∈_ [ _T_ ] [(] _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)] _[ −]_ _**[b]**_ _[s]_ [(] _**[x]**_ _[s]_ [))]

_≤_ �1 _−_ _A_ _[T]_ _min_ _[ −]_ [1]         - max
_t,s∈_ [ _T_ ] _[∥]_ _**[b]**_ _[t]_ [(] _**[x]**_ _[t]_ [)] _[ −]_ _**[b]**_ _[s]_ [(] _**[x]**_ _[s]_ [)] _[∥][∞]_ _[,]_


where we exchange the two maximum in the last step.


Finally, we prove an auxiliary result we used before. This result generalizes Lemma 3 in Wu et al.
(2024a) in the scenario when _αt_ + ∆ _t <_ 1 and _m ≤_ 0 _≤_ _M_ .


_Proof of Lemma D.6._ We first prove the side that is “less than”. Note that the desired inequality
trivially holds for the base case _s_ 1. Then we conduct the following inductive steps. Suppose the
desired inequality holds for some _r ∈_ [ _T −_ 1], namely:


_sr ≤_ _A_ _[r]_ _min_ _[−]_ [1] _[pm]_ [ + (1] _[ −]_ _[A]_ _min_ _[r][−]_ [1] _[p]_ [)] _[M.]_


Then we show that for time step _r_ + 1 _∈_ [ _T_ ],


_sr_ +1 = _Ar_ +1 _sr_ + ∆ _r_ +1 _zr_ +1
_≤_ _Ar_ +1       - _A_ _[r]_ _min_ _[−]_ [1] _[pm]_ [ + (1] _[ −]_ _[A]_ _min_ _[r][−]_ [1] _[p]_       - _M_ ) + ∆ _r_ +1 _M_ (15)

_≤_ _Ar_ +1       - _A_ _[r]_ _min_ _[−]_ [1] _[pm]_ [ + (1] _[ −]_ _[A]_ _min_ _[r][−]_ [1] _[p]_       - _M_ ) + (1 _−_ _Ar_ +1) _M_ (16)

_≤_ _Amin_       - _A_ _[r]_ _min_ _[−]_ [1] _[pm]_ [ + (1] _[ −]_ _[A][r]_ _min_ _[−]_ [1] _[p]_       - _M_ ) + (1 _−_ _Amin_ ) _M_ (17)

= _A_ _[r]_ _min_ _[pm]_ [ + (1] _[ −]_ _[A][r]_ _min_ _[p]_ [)] _[M,]_


where we substitute the upper bounds of _st_ (by the inductive hypothesis) and _zt_ in Eq. 15, Eq. 16
holds because either ∆ _r_ +1 = 1 _−_ _Ar_ +1 (by Assumption (i)) or ∆ _r_ +1 _≤_ 1 _−_ _Ar_ +1 and _M ≥_ 0 (by
Assumption (ii)), Eq. 17 is satisfied as we are lowering down the coefficient for the smaller end in the
convex combination. This concludes the proof by induction.


The proof for the “greater than” side follows from a symmetric argument. We provide a brief proof
for completeness. The base case _s_ 1 trivially satisfies the desired inequality. Consider an inductive
step where _sr_ for some _r ∈_ [ _T −_ 1] also satisfies the desired inequality. Then we have:


_sr_ +1 = _Ar_ +1 _sr_ + ∆ _r_ +1 _zr_ +1
_≥_ _Ar_ +1 �(1 _−_ _Amin_ _[r][−]_ [1] [)] _[qm]_ [ +] _[ A][r]_ _min_ _[−]_ [1] _[qM]_ [)]       - + ∆ _r_ +1 _m_

_≥_ _Ar_ +1 �(1 _−_ _Amin_ _[r][−]_ [1] [)] _[qm]_ [ +] _[ A][r]_ _min_ _[−]_ [1] _[qM]_ [)]       - + (1 _−_ _Ar_ +1) _m_

_≥_ _Amin_ �(1 _−_ _A_ _[r]_ _min_ _[−]_ [1] [)] _[qm]_ [ +] _[ A]_ _min_ _[r][−]_ [1] _[qM]_ [)]       - + (1 _−_ _Amin_ ) _m_

= (1 _−_ _A_ _[r]_ _min_ _[q]_ [)] _[m]_ [ +] _[ A][r]_ _min_ _[qM,]_


which concludes the proof by induction.


24


Published as a conference paper at ICLR 2025





(b) Influential score ratios.





(a) Influential scores.







Figure 7: Visualization of influential scores for Mamba and Transformer models under _∼_ 150M and
_∼_ 1.4B parameters. (a) illustrates the influence score on the last output token by the preceding input
tokens. (b) shows Mamba-to-transformer ratios of influential scores within the nearest 128 tokens.


E EXTENDED EXPERIMENTS


E.1 INFLUENTIAL SCORES: TRANSFORMERS VS. MAMBA


We further compare how influence scores vary with the relative distance between Transformers and
Mamba. For our study, we select Pythia (Biderman et al., 2023), which integrates Rotary Position
Encoding (RoPE) (Su et al., 2024) into the attention computation. Fig. 7a illustrates the influence
scores of each token contributing to the last token. Pythia demonstrates the well-known “lost in the
middle” pattern, where tokens at the beginning and end have high influence scores, while those in the
middle exhibit lower scores. In contrast, Mamba’s highly influential tokens are more concentrated in
the nearest positions.


We also compare the decay rate of influence scores for these two models at local tokens. Each curve
as shown in Figure 7b represents the ratio of Mamba’s influence scores to those of Pythia of various
model sizes. The dotted curves are fitted with an exponential function. Our findings reveal that
Mamba exhibits an exponentially faster decay speed compared to Pythia. This suggests that Mamba
places greater emphasis on local tokens than transformers.


E.2 NEEDLE IN A HAYSTACK TEST


Our experiments of testing positional bias for Mamba in Sec. 3.2 are based on an open-source
project LLMTest_NeedleInAHaystack [3] . To validate the retrieval capability of the models
while preventing them from relying on memorized information stored in their model weights, we
carefully design the inserted statements to contain factual errors. Several examples of such statements
are provided in Figure 8. For instance, we insert the statement, “The capital of France is Madrid.” and
then test the model’s retrieval ability by asking the question, “What is the capital of France?” While
the correct answer, Paris, is likely memorized by the LLM, if the model “correctly” outputs Madrid
based on the context provided, it demonstrates that the model is successfully using the contextual
information rather than relying on pre-existing knowledge. This approach ensures that the evaluation
focuses on the model’s ability to retrieve and process information from the input context. We also
add an instruction “ignore the fact, only use information in the context to answer the questions” to
facilitate this behavior. We provide the fine-grained visualization result in Figure 9.


E.3 CIFAR-10 IMAGE CLASSIFICATION


Here we present experiment details in Sec. 3.3, where we conduct image classification on the
CIFAR-10 dataset to study locality bias in SSMs. Specifically, 32 _×_ 32 RGB images in the dataset are


[3https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)


25


Published as a conference paper at ICLR 2025


**Inserted Statement** : The capital of France is Madrid.

**Inserted Statement** : The telephone was invented by Elon Musk.

**Inserted Statement** : The largest planet in our solar system is Earth.

**Inserted Statement** : The country known for the Great Wall is Brazil.









Figure 8: An illustration of our synthetic data.













Figure 9: Detailed comparison between SSM and Transformer on the “Needle in a Haystack"
benchmark. The upper figure shows the retrieval accuracy of the Mamba-Codestral-7B model, while
the lower figure presents the retrieval accuracy of the Mistral-7B model. We present a heatmap where
"full context length" refers to the total length of the document, and "needle position" denotes the
relative position of the statement to be retrieved within the context.


flattened into sequences with a shape of (1024 _,_ 3), where 1024 represents the sequence length and 3
corresponds to the RGB channels of the pixel tokens. These pixel tokens are then linearly projected
into _H_ -dimensional features, which are then input into SSM or transformer mixers. In addition to
pixel tokens, we insert a class token at the last position of the input sequence. The output state of the
class token will be processed by a one-layer classifier head to generate the final logits.


Note that while the ViT architecture (Dosovitskiy et al., 2020) places the class token at the first
position of the input sequence, this design is incompatible with SSMs, which rely on causal sequence
modeling. In SSMs, the class token must be positioned last to aggregate features from the entire
sequence. We position the class token as the last token to establish long-range dependencies between


26


Published as a conference paper at ICLR 2025


Table 3: Extended results of adversarial attack experiments on the CIFAR-10 dataset. Classification
accuracy is used as the metric.

|Models|Corrupted region (seq. length = 1024)<br>(no corrupt) [1014:1024] [0:10] [768:1024] [0:256] [512:544] [480:576]|Col3|Col4|Col5|
|---|---|---|---|---|
|H3<br>Transformer<br>RWKV<br>Mamba|0.654<br>0.580<br>0.474<br>0.674|0.629<br>0.654<br>0.571<br>0.500<br>**0.194**<br>0.470<br>0.348<br>0.664|0.394<br>0.639<br>0.249<br>0.263<br>**0.107**<br>0.448<br>**0.099**<br>0.597|0.603<br>0.543<br>0.498<br>0.347<br>0.405<br>0.392<br>0.515<br>0.446|



global image features and the leading pixel tokens. Alternative methods for aggregating features
across the entire sequence, such as mean pooling (Gu et al., 2021a; Tay et al., 2020) or placing the
class token in the middle of the sequence (Zhu et al., 2024), work more robustly in general but do not
fit the needs for our arguments on locality.


In addition, our image classification setup differs from Tay et al. (2020), where an 8-bit pixel intensity
lookup table is used as the token embedder. Instead, we employ a linear projection to map RGB pixel
colors into _H_ -dimensional features.


For a fair comparison, the same hyperparameters are used across all models: learning rate = 0.001,
weight decay = 0.1, number of layers = 3, feature dimension _H_ = 32, and number of states = 64.
Each model is trained for 100 epochs. The models and training pipelines are built on Arora et al.
(2023). No perturbations are imposed on the input sequences in the training stage.


**Adversarial Attack.** To introduce perturbations to test data for adversarial attack, we first define
a corruption length _K_, which is small relative to the entire sequence length. We then replace the
leading and trailing _K_ tokens with random Gaussian noise. In our experiments, _K_ is set to 32 and 96,
corresponding to one row and three rows of pixels, respectively. Table 3 shows more results under
other corruption regions.


**Target Attack.** For the target attack experiments, a target class is first selected. For each image
from the other classes, an image from the target class is randomly sampled, and its leading and trailing
pixels are used to replace the corresponding pixels in the original image. We test two attack ratios:
256/1024 and 480/1024. Replacing fewer than 256 pixels generally does not result in considerable
success rates based on our trials. In our main text, we show success rates when “horse” is the target
class. Similar patterns are also observed across other classes. Fig. 10 shows the average success rates
obtained by setting each class as the target.







Figure 10: Overall success rate of our target attack experiments on CIFAR-10, calculated by averaging
the attack success rates obtained when each class is individually set as the target class.


E.4 SCALING LAW WITH VARYING DEPTH


In this section, we elaborate on experiment details for Sec. 4.1. We perform casual language modeling
(Radford et al., 2018) with Mamba and report the validation perplexity. For each curve in Fig. 4,
we fix the depth of the model and vary the number of parameters from 140M to 550M by adjusting
the hidden dimension accordingly. The depth is chosen from {16, 24, 32, 48, 64, 72}. All other
model configurations keep the default values: the memory state size _N_ = 16, the intermediate SSM


27


Published as a conference paper at ICLR 2025


dimension is two times the hidden dimension, and the intermediate rank for predicting time step ∆ _t_
is the hidden dimension divided by 16. We adopt DataComp-LM (Li et al., 2024) as our training
corpus for its high data quality. The evaluation set is created by holding out a subset of 10M tokens
from the training data. We follow the training recipe provided in Appendix E.2.1 of Gu & Dao (2023),
which roughly follows the Chinchilla scaling laws (Hoffmann et al., 2022) to allocate the number
of tokens for each model size. We test two block sizes {2048, 8192}. During training, we fix the
number of tokens in one training batch (# sequences _×_ sequence length) as 0.5M. The number of
training steps is computed by the total number of tokens divided by batch size (# tokens / # tokens
in one batch). We use AdamW as the optimizer with gradient clip value 1.0, weight decay 0.1, and
linear learning warmup with cosine decay to 1e-5, following Gu & Dao (2023). The peak learning
rate is 5 _×_ the GPT3 values (Brown et al., 2020) for different model sizes. We summarize the training
hyperparameters for varying-sized models in Tab. 4.


# Params Training steps Peak LR Batch Size (in tokens) # Tokens


100-250M 4800 3e-3 0.5M 2.5B
250-400M 13500 1.5e-3 0.5M 7B
400-550M 20000 1.25e-3 0.5M 10B


Table 4: Summary of training settings for varying-sized Mamba. The settings are following Chinchilla
law (Hoffmann et al., 2022) and consistent with Gu & Dao (2023).


E.5 SSM POLARIZATION AND ASSOCIATIVE RECALL EXPERIMENTS


In this section, we provide implementation details for our proposed _polarization_ technique. Also, we
introduce experiment details for the associative recall tasks we used to validate our techniques.


E.5.1 POLARIZED MAMBA


Recall that we polarize a channel in _**A**_ _t_ with constant zero and another with constant one. Note that
in the implementation of Mamba, _**A**_ _t_ is parameterized by _**A**_ _t_ = exp(∆ _t_ _**A**_ ), where _**A**_ is a learnable
parameters. We change the forward pass of Mamba by prepending a zero and appending a number
negatively large enough (i.e., -1000). So the pre- and post-exponential _**A**_ and _**A**_ _t_ become:







�1 exp(∆ _t_ _**A**_ )
0



_**A**_ _←_



�0
_**A**_

_−_ 1000



_,_ _**A**_ _t ≈_



We also study two variants: 0-polarized and 1-polarized Mamba by only pretending or appending
one or zero to _**A**_ _t_ to single out the effects of one and zero gating, respectively.


The gradient flow also complies with the original one, meaning polarization does not change the
optimization dynamics of Mamba. Below we show that the polarized _**A**_ _t_ does not influence the
gradient in terms of the time interval ∆ _t_ . First of all, let _**G**_ _t_ = _∂_ ∆ _t∂ℓ_ _**b**_ _t_ ( _**x**_ _t_ ) _⊤_ _∂_ ∆ _∂t_ _**b**_ ∆ _t_ ( _t_ _**x**_ _t_ ), then



_∂_ ( _**A**_ _t_ )1 _,_ 1 _∂ℓ_

+
_∂_ ∆ _t_ _∂_ ( _**A**_ _t_ ) _N,N_



_∂_ ( _**A**_ _t_ ) _N,N_

_∂_ ∆ _t_



_∂ℓ_
= _**G**_ _t_ +
_∂_ ∆ _t_


= _**G**_ _t_ +



_N_ _−_ 1



_n_ =2


_N_ _−_ 1



_n_ =2



_∂ℓ_
_∂_ ( _**A**_ _t_ ) _n,n_


_∂ℓ_
_∂_ ( _**A**_ _t_ ) _n,n_



_∂_ ( _**A**_ _t_ ) _n,n_ _∂ℓ_

+
_∂_ ∆ _t_ _∂_ ( _**A**_ _t_ )1 _,_ 1



_∂_ ( _**A**_ _t_ ) _n,n_ _∂ℓ_

+ exp(∆ _t_ ( _**A**_ _t_ )1 _,_ 1) ( _**A**_ _t_ )1 _,_ 1
_∂_ ∆ _t_ _∂_ ( _**A**_ _t_ )1 _,_ 1

~~�~~                   - ~~�~~                   =0



_∂ℓ_
_∂_ ∆ _t_



_∂ℓ_
+ exp(∆ _t_ ( _**A**_ _t_ ) _N,N_ )( _**A**_ _t_ ) _N,N_
_∂_ ( _**A**_ _t_ ) _N,N_

~~�~~       - ~~�~~ ~~�~~
_≈_ 0



_∂_ ( _**A**_ _t_ ) _n,n_

_,_
_∂_ ∆ _t_



_≈_ _**G**_ _t_ +



_N_ _−_ 1



_n_ =2



_∂ℓ_
_∂_ ( _**A**_ _t_ ) _n,n_



where the term exp(∆ _t_ ( _**A**_ _t_ ) _N,N_ )( _**A**_ _t_ ) _N,N →_ 0 is achieved by taking ( _**A**_ _t_ ) _N,N →−∞_ . The
component directly contributed by _**A**_ _t_ is not affected regardless of the polarization.


28


Published as a conference paper at ICLR 2025

|# KV Pairs<br>Configurations # Layers Recency Over-smoothing Avg.<br>64 128 256|Col2|Col3|
|---|---|---|
|Default**_ A_**_t_<br>2<br>Default**_ A_**_t_<br>4||98.38<br>81.81<br>36.00<br>72.06<br>99.23<br>82.08<br>33.52<br>71.61|
|(**_A_**_t_)1_,_1 = 1<br>2<br>(**_A_**_t_)_N,N_ = 0<br>2<br>(**_A_**_t_)_N,N_ = 0<br>4||99.81<br>94.70<br>56.39<br>83.63<br>98.41<br>81.35<br>36.55<br>72.10<br>99.74<br>92.20<br>52.21<br>81.38|
|(**_A_**_t_)1_,_1 = 1_,_ (**_A_**_t_)_N,N_ = 0<br>2<br>(**_A_**_t_)1_,_1 = 1_,_ (**_A_**_t_)_N,N_ = 0<br>4||99.23<br>95.54<br>54.74<br>83.17<br>99.94<br>98.80<br>81.56<br>93.43|



Table 5: Extension to Tab. 5. We note the extent of locality and over-smoothing for each configuration. We
consider 1-polarization mitigates locality most significantly, while deepening architecture only relieves recency
mildly but deteriorates over-smoothing. 0-polarization alleviates over-smoothening and unleash the benefits by
depth scaling.


E.5.2 ASSOCIATIVE RECALL EXPERIMENTS


Associative Recall (AR) is the task of retrieving a specific value tied to a given key. This ability
is particularly important for in-context learning, where a model leverages previously presented
information to adapt to new inputs (Olsson et al., 2022; Arora et al., 2023).


In our experiments, we use Mamba as the representative SSM, following the setup described in Arora
et al. (2023). The input sequences to the model consist of two parts. The first part is a set of key-value
pairs, randomly sampled from a vocabulary, with keys and values drawn independently. The second
part contains random keys, selected from those present in the first part (without repetition), and placed
according to a power-law distribution at random. Any remaining slots in the sequence are filled with
a zero token. The target sequence contains the values corresponding to the positions of the respective
key occurrences, while all other tokens are excluded from loss supervision and accuracy evaluation.


We employ a training strategy with mixed sequence lengths and varying numbers of key-value
pairs. Specifically, the total sequence length varies across {64, 128, 256, 512, 1024}, with {12.5%,
25%, 50%} of the sequence allocated to key-value pairs and the remainder to queried keys. Each
configuration includes 20,000 examples, resulting in a total of 300,000 training examples. For
evaluation, the sequence length is fixed at 1024, while the number of key-value pairs varies among
64, 128, 256. We use a batch size of 128 and a learning rate of 1e-3 throughout the experiments. We
provide an extended table for the results in Tab. 5.


29


