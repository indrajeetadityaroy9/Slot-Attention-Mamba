# Primary results: Polar-Mem-Mamba on MQAR
# Training: torchrun --nproc_per_node=N -m align_mamba.train --config configs/main.yaml
# Eval:     align-eval --config configs/main.yaml

run:
  seed: 42
  output_dir: outputs

model:
  d_model: 256
  encoder_layers: 6
  decoder_layers: 6
  d_state: 64
  n_heads: 4
  cross_attn_layers: [0, 2]
  mem_pool_size: 50
  mem_summary_dim: 64
  mem_update_freq: 4
  dropout: 0.1

data:
  num_pairs: 128
  num_queries: 16
  num_samples: 100000

training:
  batch_size: 256
  max_steps: 100000
  learning_rate: 3.0e-4
  weight_decay: 0.01
  label_smoothing: 0.1
  grad_clip: 1.0
  warmup_ratio: 0.01

eval:
  checkpoint: outputs/best
  mode: standard
  num_samples: 1000
  batch_size: 32
