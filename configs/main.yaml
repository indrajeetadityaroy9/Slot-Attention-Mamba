model:
  d_model: 256
  encoder_layers: 6
  decoder_layers: 6
  d_state: 64
  n_heads: 4
  vocab_size: 8192
  dropout: 0.1
  block_size: 4
  n_householder_steps: 2
  kronecker_partitions: 5
  kronecker_subdim: 4
  top_k_slots: 8
  mamba_expand: 2
  mamba_d_conv: 4

data:
  num_pairs: 128
  num_queries: 16
  num_samples: 100000
  val_ratio: 0.1

training:
  batch_size: 256
  max_steps: 100000
  seed: 42
  output_dir: results
  resume_from: ""

evaluation:
  eval_mode: standard
  eval_num_samples: 1000
  eval_batch_size: 32
  eval_checkpoint: results/best
