# @package _global_
#
# Zamba-Style Shared Attention Ablation
#
# PURPOSE: Test Zamba-style shared GSA with concatenated residuals
# - Standard: Per-layer cross-attention modules
# - Zamba: Single shared GSA, concat [current, initial] features
#
# Reference: arXiv:2411.15242 (Zamba2)
#
# USAGE:
#   python -m align_mamba.train experiment=05_shared_attention model.concat_initial_residual=false
#   python -m align_mamba.train experiment=05_shared_attention model.concat_initial_residual=true
#
#   # Compare parameter efficiency
#   python -m align_mamba.train -m experiment=05_shared_attention \
#       model.concat_initial_residual=false,true project.seed=42,1337

defaults:
  - override /data: mqar
  - override /model: hybrid_small
  - override /training: experiment_fast

experiment_name: shared_attention_ablation

project:
  name: shared-attention-ablation
  output_dir: outputs/05_shared_attention

model:
  vocab_size: 8192
  d_model: 256
  encoder_layers: 2
  decoder_layers: 8
  d_state: 64
  n_heads: 8
  hybrid_positions: [0, 2, 4, 6]
  concat_initial_residual: true

data:
  num_pairs: 64
  num_queries: 16
